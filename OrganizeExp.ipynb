{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBv3sgvbq6DW"
      },
      "source": [
        "Experiment,Variable to Change,Fixed Variables,Purpose\n",
        "1. Sensitivity,\"Threshold % (0, 1, 5, 10, 20)\",Model: 1.5B  Method: NF4,Find the optimal trade-off (Sweet Spot).\n",
        "2. Methods,\"Method (NF4, AWQ, GPTQ)\",Model: 1.5B  Threshold: 5% (or Sweet Spot),Compare KLD impact on different quantizers.\n",
        "3. Scaling,\"Model Size (1.5B, 7B)\",Method: (Best of Exp 2)  Threshold: 5%,Test if larger models behave differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFNRiQb4R3WP"
      },
      "source": [
        "# **Setup & Dependencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BY_eeCnsWg1"
      },
      "source": [
        "**We use L4 GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BsBQ9VbffXlD",
        "outputId": "0af596c9-bd1f-4c4e-d3bf-2a987e87f34d"
      },
      "outputs": [],
      "source": [
        "!pip uninstall transformers torch torchaudio torchvision wandb -y\n",
        "!pip install llmcompressor\n",
        "!pip install -q accelerate bitsandbytes datasets scipy matplotlib wandb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from datasets import Dataset\n",
        "import copy\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import wandb\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8tJpydOUJBM"
      },
      "outputs": [],
      "source": [
        "# Set for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import set_seed\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "set_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZfycqbNSRtx"
      },
      "source": [
        "## **Experiment 2 Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhwP5QjrlJCZ"
      },
      "outputs": [],
      "source": [
        "# --- Experiment Settings ---\n",
        "MODELS_TO_TEST = [\"Qwen/Qwen2.5-1.5B-Instruct\"]\n",
        "SENSITIVITY_THRESHOLDS = [0.0, 0.05, 0.1, 0.2, 0.3]\n",
        "CALIBRATION_SAMPLES = 128\n",
        "EVAL_SAMPLES = 5000\n",
        "WANDB_PROJECT_NAME = \"KLD_Quantization_Exp2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import os\n",
        "os.environ[\"WANDB_QUIET\"] = \"true\"\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "if 'results_table' not in globals():\n",
        "    results_table = []\n",
        "\n",
        "print(\"Loading MMLU Dataset...\")\n",
        "try:\n",
        "    mmlu_dataset = concatenate_datasets([\n",
        "        load_dataset(\"cais/mmlu\", \"all\", split='test')\n",
        "    ])\n",
        "    print(f\"MMLU Dataset Loaded. Size: {len(mmlu_dataset)} samples.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading MMLU: {e}\")\n",
        "    from datasets import Dataset\n",
        "    mmlu_dataset = Dataset.from_dict({\n",
        "        \"question\": [\"1+1=?\"], \"choices\": [[\"1\", \"2\", \"3\", \"4\"]], \"answer\": [1]\n",
        "    })\n",
        "\n",
        "print(\"Global setup complete. Ready for Step 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Metrics & Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recursive_getattr(obj, attr):\n",
        "    for part in attr.split('.'):\n",
        "        obj = getattr(obj, part)\n",
        "    return obj\n",
        "\n",
        "def recursive_setattr(obj, attr, val):\n",
        "    pre, _, post = attr.rpartition('.')\n",
        "    parent = recursive_getattr(obj, pre) if pre else obj\n",
        "    setattr(parent, post, val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- MMLU Logic ---\n",
        "def format_mmlu_prompt(example):\n",
        "    options = [f\"{label}. {example['choices'][i]}\" for i, label in enumerate(['A', 'B', 'C', 'D'])]\n",
        "    prompt_text = f\"Question: {example['question']}\\nOptions:\\n\" + \"\\n\".join(options) + \"\\nAnswer:\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Output only the single letter (A, B, C, or D) corresponding to the correct answer.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_text}\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "def get_mmlu_predictions(model, dataset, num_samples):\n",
        "    predictions, ground_truths = [], []\n",
        "    choices = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    choice_ids = [tokenizer.encode(c)[0] for c in choices]\n",
        "\n",
        "    for i in tqdm(range(min(num_samples, len(dataset))), desc=\"MMLU Eval\"):\n",
        "        ex = dataset[i]\n",
        "        inputs = tokenizer(format_mmlu_prompt(ex), return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits[0, -1, choice_ids]\n",
        "            pred = choices[torch.argmax(logits).item()]\n",
        "        predictions.append(pred)\n",
        "        ground_truths.append(choices[ex['answer']])\n",
        "    return predictions, ground_truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Metrics Helpers ---\n",
        "def compute_kld(logits_p, logits_q):\n",
        "    p_probs = F.softmax(logits_p, dim=-1)\n",
        "    q_log_probs = F.log_softmax(logits_q, dim=-1)\n",
        "    return nn.KLDivLoss(reduction='batchmean')(q_log_probs, p_probs).item()\n",
        "\n",
        "def calculate_flip_rate(base_preds, new_preds):\n",
        "    \"\"\"Calculates % of answers that changed from the baseline.\"\"\"\n",
        "    if not base_preds or not new_preds: return 0.0\n",
        "    flips = sum([1 for b, n in zip(base_preds, new_preds) if b != n])\n",
        "    return flips / len(base_preds)\n",
        "\n",
        "def compute_perplexity(model, tokenizer):\n",
        "    \"\"\"Computes perplexity on a subset of WikiText-2\"\"\"\n",
        "    encodings = tokenizer(\"\\n\\n\".join(load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:20]), return_tensors=\"pt\")\n",
        "    max_length = model.config.max_position_embeddings\n",
        "    stride = 512\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "\n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Computing PPL\"):\n",
        "        end_loc = min(begin_loc + max_length, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc\n",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "            nlls.append(outputs.loss)\n",
        "\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len: break\n",
        "\n",
        "    return torch.exp(torch.stack(nlls).mean()).item()\n",
        "\n",
        "def measure_efficiency(model, tokenizer, input_text=\"Hello world\"):\n",
        "    # 1. Cleanup\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # 2. Measure Static Memory (Model Weights Only)\n",
        "    # This shows the pure effect of quantization storage\n",
        "    static_mem_bytes = torch.cuda.memory_allocated()\n",
        "    static_mem_gb = static_mem_bytes / 1024**3\n",
        "\n",
        "    # 3. Run Inference\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model.generate(**input_ids, max_new_tokens=50, min_new_tokens=50)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 4. Measure Peak Memory (Weights + KV Cache + Temp Buffers)\n",
        "    # This shows the \"True Cost\" to run the model\n",
        "    peak_mem_bytes = torch.cuda.max_memory_allocated()\n",
        "    peak_mem_gb = peak_mem_bytes / 1024**3\n",
        "\n",
        "    latency = end_time - start_time\n",
        "\n",
        "    return latency, static_mem_gb, peak_mem_gb\n",
        "\n",
        "def evaluate_full_suite(model, tokenizer, dataset, metric_name):\n",
        "    \"\"\"Runs all metrics and returns them.\"\"\"\n",
        "    print(f\"--- Evaluating: {metric_name} ---\")\n",
        "\n",
        "    # 1. Accuracy\n",
        "    preds, truths = get_mmlu_predictions(model, dataset, EVAL_SAMPLES)\n",
        "    acc = sum([1 for p, g in zip(preds, truths) if p == g]) / len(truths)\n",
        "\n",
        "    # 2. Perplexity\n",
        "    ppl = compute_perplexity(model, tokenizer)\n",
        "\n",
        "    # 3. Efficiency (Unpack 3 values now)\n",
        "    lat, static_mem, peak_mem = measure_efficiency(model, tokenizer)\n",
        "\n",
        "    print(f\"Results -> Acc: {acc:.2%}, PPL: {ppl:.2f}, Latency: {lat:.2f}s, Static Mem: {static_mem:.2f}GB, Peak Mem: {peak_mem:.2f}GB\")\n",
        "\n",
        "    # Return separate memory metrics\n",
        "    return acc, ppl, lat, static_mem, peak_mem, preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Advanced Sensitivity Profiling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def profile_restoration_sensitivity(model_q, model_ref, calib_input, granularity='layer'):\n",
        "    \"\"\"\n",
        "    Profiles sensitivity by measuring the KLD improvement when restoring\n",
        "    individual parts of the quantized model (model_q) back to FP16 (model_ref).\n",
        "\n",
        "    Returns:\n",
        "        sensitivity_scores: Dict mapping name -> KLD improvement (Higher is more sensitive).\n",
        "    \"\"\"\n",
        "    print(f\"Profiling Restoration Sensitivity (Granularity: {granularity})...\")\n",
        "\n",
        "    # Compute Baseline\n",
        "    model_ref.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_device = next(model_ref.parameters()).device\n",
        "        base_logits = model_ref(calib_input.to(ref_device)).logits.to(device)\n",
        "        current_logits = model_q(calib_input.to(device)).logits\n",
        "        initial_kld = compute_kld(base_logits, current_logits)\n",
        "\n",
        "    print(f\"Initial Quantized KLD: {initial_kld:.6f}\")\n",
        "\n",
        "    sensitivity_scores = {}\n",
        "\n",
        "    def get_module_by_name(module, access_string):\n",
        "        names = access_string.split(sep='.')\n",
        "        return reduce(getattr, names, module)\n",
        "\n",
        "    from functools import reduce\n",
        "\n",
        "    # Block-wise or Layer-wise\n",
        "    if granularity == 'block':\n",
        "        if hasattr(model_q, 'model') and hasattr(model_q.model, 'layers'):\n",
        "            iterable_items = list(enumerate(model_q.model.layers))\n",
        "            prefix = \"model.model.layers\"\n",
        "        else:\n",
        "            raise ValueError(\"Could not detect transformer blocks structure.\")\n",
        "        iterator = tqdm(iterable_items, desc=\"Profiling Blocks\")\n",
        "    elif granularity == 'layer':\n",
        "        # # We limit this to just the linear layers to save time\n",
        "        # iterable_items = [(n, m) for n, m in model_q.named_modules() if isinstance(m, (nn.Linear,  import_bnb_linear_type_if_needed()))]\n",
        "        iterable_items = [(n, m) for n, m in model_q.named_modules()\n",
        "                          if \"mlp\" in n or \"self_attn\" in n]\n",
        "        iterator = tqdm(iterable_items, desc=\"Profiling Layers\")\n",
        "\n",
        "    # Restoration Loop\n",
        "    for name_or_idx, module_q in iterator:\n",
        "        target_name = f\"{prefix}.{name_or_idx}\" if granularity == 'block' else name_or_idx\n",
        "        try:\n",
        "            module_ref = recursive_getattr(model_ref, target_name)\n",
        "            backup_quant_module = recursive_getattr(model_q, target_name)\n",
        "            module_fp16_gpu = copy.deepcopy(module_ref).to(device)\n",
        "            recursive_setattr(model_q, target_name, module_fp16_gpu)\n",
        "\n",
        "            # Measure New KLD\n",
        "            with torch.no_grad():\n",
        "                new_logits = model_q(calib_input.to(device)).logits\n",
        "                new_kld = compute_kld(base_logits, new_logits)\n",
        "\n",
        "            improvement = initial_kld - new_kld\n",
        "            sensitivity_scores[target_name] = improvement\n",
        "            recursive_setattr(model_q, target_name, backup_quant_module)\n",
        "\n",
        "            # Cleanup VRAM\n",
        "            del module_fp16_gpu\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {target_name}: {e}\")\n",
        "\n",
        "    return sensitivity_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **The \"Surgery\" Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_surgery(model, sensitive_names, fp16_model_cpu):\n",
        "    \"\"\"\n",
        "    Replaces the sensitive quantized layers in 'model' (GPU)\n",
        "    with the original FP16 layers from 'fp16_model_cpu' (CPU).\n",
        "\n",
        "    This Generic Version uses deepcopy, so it works for:\n",
        "    - Individual Linear layers (gate_proj, q_proj)\n",
        "    - Entire Blocks (Qwen2MLP, Qwen2Attention)\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    print(f\"Surgery: Replacing {len(sensitive_names)} Sensitive Layers with FP16...\")\n",
        "\n",
        "    for name in sensitive_names:\n",
        "        try:\n",
        "            # 1. Get original FP16 module from CPU backup\n",
        "            #    (This handles Linear, Qwen2MLP, Qwen2Attention, etc.)\n",
        "            original_module = recursive_getattr(fp16_model_cpu, name)\n",
        "\n",
        "            # 2. Create a deep copy and move to GPU\n",
        "            #    We use deepcopy instead of manually instantiating nn.Linear.\n",
        "            #    This preserves the exact class type and configuration.\n",
        "            module_fp16_gpu = copy.deepcopy(original_module).to(model.device)\n",
        "\n",
        "            # 3. Swap into the quantized model\n",
        "            recursive_setattr(model, name, module_fp16_gpu)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping layer {name}: {e}\")\n",
        "\n",
        "    print(f\"Surgery Complete: {count} layers restored.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Selection & Baseline Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select model\n",
        "CURRENT_MODEL_ID = MODELS_TO_TEST[0]\n",
        "\n",
        "print(f\"{'='*40}\\nSelected Model: {CURRENT_MODEL_ID}\\n{'='*40}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CURRENT_MODEL_ID)\n",
        "print(\"Loading FP16 Baseline (This may take a minute)...\")\n",
        "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "    CURRENT_MODEL_ID,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Evaluate Baseline\n",
        "base_acc, base_ppl, base_lat, base_static_mem, base_peak_mem, base_preds = evaluate_full_suite(\n",
        "    model_fp16, tokenizer, mmlu_dataset, \"FP16 Baseline\"\n",
        ")\n",
        "\n",
        "# Log Baseline to WandB\n",
        "run = wandb.init(project=WANDB_PROJECT_NAME, name=f\"{CURRENT_MODEL_ID.split('/')[-1]}-Baseline\", reinit=True)\n",
        "wandb.log({\n",
        "    \"Accuracy\": base_acc,\n",
        "    \"Perplexity\": base_ppl,\n",
        "    \"Latency\": base_lat,\n",
        "    \"Static_Memory\": base_static_mem,\n",
        "    \"Peak_Memory\": base_peak_mem,\n",
        "    \"Threshold\": 0,\n",
        "    \"Flip_Rate\": 0.0,\n",
        "    \"Method\": \"Baseline\"\n",
        "})\n",
        "run.finish()\n",
        "\n",
        "# Store in Results Table\n",
        "results_table.append({\n",
        "    \"Model\": CURRENT_MODEL_ID,\n",
        "    \"Method\": \"FP16 Baseline\",\n",
        "    \"Threshold\": 0,\n",
        "    \"Acc\": base_acc,\n",
        "    \"Flip\": 0.0,\n",
        "    \"PPL\": base_ppl,\n",
        "    \"Latency\": base_lat,\n",
        "    \"Static Mem\": base_static_mem,\n",
        "    \"Peak Mem\": base_peak_mem\n",
        "})\n",
        "\n",
        "print(\"Baseline Loaded & Evaluated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Profiling & Offloading\n",
        "print(\"Preparing Calibration Data...\")\n",
        "calib_data = tokenizer(\n",
        "    \"\\n\\n\".join(load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10]),\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids.to(device)\n",
        "\n",
        "granularity_mode = 'layer'\n",
        "\n",
        "# Offload FP16 Model to CPU to save memory\n",
        "print(\"Moving FP16 model to CPU to free up VRAM...\")\n",
        "model_fp16.cpu()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"VRAM Cleared. Ready for Experiments.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3KtTkPVrU_o"
      },
      "source": [
        "# Preliminary Check:\n",
        "Justifying the \"Base\" Precision\n",
        "Variable: Floating Point Type (FP8 vs. FP4/NF4)\n",
        "\n",
        "Purpose: Before running complex KLD experiments, you must decide what your \"base\" low-precision format is. If FP4 destroys the model completely and FP8 is perfect, then KLD is needed for FP4 but not FP8. If both are good, FP4 is better for efficiency.\n",
        "\n",
        "Design:\n",
        "\n",
        "Run: 1.5B Model (FP16 Baseline) vs. 1.5B (FP8) vs. 1.5B (NF4).\n",
        "\n",
        "Metric: Perplexity & MMLU Accuracy.\n",
        "\n",
        "Hypothesis: NF4 offers higher compression but higher degradation than FP8. This justifies using NF4 (or INT4 methods) as the primary candidate for your KLD restoration because it needs the help more than FP8 does.\n",
        "\n",
        "Decision: If verified, fix 4-bit (NF4/INT4) as the standard base for the rest of the experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_vSknhGrb5_"
      },
      "source": [
        "**Refer to the FP8 vs FP4 notebook**\n",
        "\n",
        "**Results: NF4 offers higher compression but higher degradation than FP8. We will use NF4 (or INT4 methods) as the primary candidate for the following experiments because it needs the help more than FP8 does.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S49TWxQ0OLeO"
      },
      "source": [
        "# Experiment 1: Sensitivity Analysis\n",
        "Research Question: How much of the model actually needs to be kept in high precision to recover performance? Is there a point of diminishing returns?\n",
        "\n",
        "Fixed Variables:\n",
        "\n",
        "Model: Qwen2.5-1.5B (Small enough to run fast, big enough to show signal).\n",
        "\n",
        "Method: NF4 (The simplest 4-bit baseline).\n",
        "\n",
        "Independent Variable (Change this):\n",
        "\n",
        "KLD Threshold / % Restored: 0% (Baseline), 1%, 5%, 10%, 20%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUZ8LdxhjgT4"
      },
      "source": [
        "**Refer to the FP8 vs FP4 notebook**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imX3DJ0JrUsU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyT4R4Orjq1u"
      },
      "source": [
        "# Experiment 2: Algorithm Comparison\n",
        "Research Question: Does KLD guidance work better on top of simple quantization (NF4) or advanced quantization (AWQ/GPTQ)?\n",
        "\n",
        "Fixed Variables:\n",
        "\n",
        "Model: Qwen2.5-1.5B (Consistent with Exp 1).\n",
        "\n",
        "Threshold: Fix this to the \"winner\" from Exp 1 (e.g., if 5% was the sweet spot, use 5% for all).\n",
        "\n",
        "Independent Variable (Change this):\n",
        "\n",
        "Method: NF4 vs. AWQ vs. GPTQ.\n",
        "\n",
        "Why: AWQ and GPTQ already do some optimization. You want to see if your KLD method adds value on top of them, or if it's only useful for naive methods like NF4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AWQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llmcompressor.modifiers.awq import AWQModifier\n",
        "from llmcompressor.modifiers.quantization import GPTQModifier\n",
        "from llmcompressor import oneshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc0qTTETq3aq"
      },
      "outputs": [],
      "source": [
        "# AWQ\n",
        "print(f\"\\n--- Starting Experiment: AWQ ({CURRENT_MODEL_ID}) ---\")\n",
        "\n",
        "print(\"Running AWQ Oneshot Quantization...\")\n",
        "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "calib_data_obj = Dataset.from_dict({\"text\": [text for text in ds[\"text\"] if len(text) > 0][:128]})\n",
        "\n",
        "recipe = [AWQModifier(targets=\"Linear\", scheme=\"W4A16\")]\n",
        "oneshot(\n",
        "    model=CURRENT_MODEL_ID,\n",
        "    dataset=calib_data_obj,\n",
        "    recipe=recipe,\n",
        "    output_dir=\"./awq_temp\",\n",
        "    num_calibration_samples=128,\n",
        "    max_seq_length=512,\n",
        "    save_compressed=True\n",
        ")\n",
        "\n",
        "model_awq = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./awq_temp\", device_map=\"auto\", trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Profiling AWQ Sensitivity...\")\n",
        "awq_sensitivity = profile_restoration_sensitivity(\n",
        "    model_q=model_awq,\n",
        "    model_ref=model_fp16,\n",
        "    calib_input=calib_data,\n",
        "    granularity='block'\n",
        ")\n",
        "\n",
        "sorted_awq = sorted(awq_sensitivity.items(), key=lambda x: x[1], reverse=True)\n",
        "all_layer_names = [n for n, s in sorted_awq]\n",
        "\n",
        "# Experiment loop\n",
        "sorted_thresholds = sorted(SENSITIVITY_THRESHOLDS)\n",
        "current_restored_count = 0\n",
        "\n",
        "for threshold in sorted_thresholds:\n",
        "    print(f\"\\nTargeting Threshold: {threshold:.0%} kept in FP16\")\n",
        "\n",
        "    target_count = int(len(all_layer_names) * threshold)\n",
        "    layers_to_fix_now = all_layer_names[current_restored_count : target_count]\n",
        "\n",
        "    if layers_to_fix_now:\n",
        "        print(f\"Restoring {len(layers_to_fix_now)} additional layers...\")\n",
        "        perform_surgery(model_awq, layers_to_fix_now, model_fp16)\n",
        "        current_restored_count = target_count\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=WANDB_PROJECT_NAME,\n",
        "        name=f\"{CURRENT_MODEL_ID.split('/')[-1]}-AWQ-{threshold}\",\n",
        "        config={\"model\": CURRENT_MODEL_ID, \"method\": \"KLD-AWQ\", \"threshold\": threshold},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    acc, ppl, lat, static_mem, peak_mem, preds = evaluate_full_suite(\n",
        "        model_awq, tokenizer, mmlu_dataset, f\"KLD-AWQ-{threshold:.0%}\"\n",
        "    )\n",
        "\n",
        "    flip = calculate_flip_rate(base_preds, preds)\n",
        "\n",
        "    wandb.log({\n",
        "        \"Accuracy\": acc, \"Perplexity\": ppl, \"Latency\": lat, \"Static_Memory\": static_mem,\n",
        "        \"Peak_Memory\": peak_mem, \"Flip_Rate\": flip, \"Threshold\": threshold\n",
        "    })\n",
        "\n",
        "    results_table.append({\n",
        "        \"Model\": CURRENT_MODEL_ID,\n",
        "        \"Method\": \"KLD-AWQ\",\n",
        "        \"Threshold\": threshold,\n",
        "        \"Acc\": acc,\n",
        "        \"Flip\": flip,\n",
        "        \"PPL\": ppl,\n",
        "        \"Latency\": lat,\n",
        "        \"Static Mem\": static_mem,\n",
        "        \"Peak Mem\": peak_mem\n",
        "    })\n",
        "    run.finish()\n",
        "\n",
        "shutil.rmtree(\"./awq_temp\")\n",
        "del model_awq\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### GPTQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPTQ\n",
        "\n",
        "print(f\"\\n--- Starting Experiment: GPTQ ({CURRENT_MODEL_ID}) ---\")\n",
        "\n",
        "print(\"Running GPTQ Optimization...\")\n",
        "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "calib_data_obj = Dataset.from_dict({\"text\": [text for text in ds[\"text\"] if len(text) > 0][:128]})\n",
        "\n",
        "recipe = [\n",
        "    GPTQModifier(\n",
        "        targets=\"Linear\",\n",
        "        scheme=\"W4A16\",\n",
        "        ignore=[\"lm_head\"],\n",
        "        dampening_frac=0.01\n",
        "    )\n",
        "]\n",
        "\n",
        "oneshot(\n",
        "    model=CURRENT_MODEL_ID,\n",
        "    dataset=calib_data_obj,\n",
        "    recipe=recipe,\n",
        "    output_dir=\"./gptq_temp\",\n",
        "    num_calibration_samples=128,\n",
        "    max_seq_length=512,\n",
        "    save_compressed=True\n",
        ")\n",
        "\n",
        "model_gptq = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./gptq_temp\", device_map=\"auto\", trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Profiling GPTQ Sensitivity...\")\n",
        "gptq_sensitivity = profile_restoration_sensitivity(\n",
        "    model_q=model_gptq,\n",
        "    model_ref=model_fp16,\n",
        "    calib_input=calib_data,\n",
        "    granularity= 'layer'\n",
        ")\n",
        "\n",
        "sorted_gptq = sorted(gptq_sensitivity.items(), key=lambda x: x[1], reverse=True)\n",
        "all_layer_names = [n for n, s in sorted_gptq]\n",
        "\n",
        "sorted_thresholds = sorted(SENSITIVITY_THRESHOLDS)\n",
        "current_restored_count = 0\n",
        "\n",
        "for threshold in sorted_thresholds:\n",
        "    print(f\"\\nTargeting Threshold: {threshold:.0%} kept in FP16\")\n",
        "\n",
        "    target_count = int(len(all_layer_names) * threshold)\n",
        "    layers_to_fix_now = all_layer_names[current_restored_count : target_count]\n",
        "\n",
        "    if layers_to_fix_now:\n",
        "        print(f\"Restoring {len(layers_to_fix_now)} additional layers...\")\n",
        "        perform_surgery(model_gptq, layers_to_fix_now, model_fp16)\n",
        "        current_restored_count = target_count\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=WANDB_PROJECT_NAME,\n",
        "        name=f\"{CURRENT_MODEL_ID.split('/')[-1]}-GPTQ-{threshold}\",\n",
        "        config={\"model\": CURRENT_MODEL_ID, \"method\": \"KLD-GPTQ\", \"threshold\": threshold},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    acc, ppl, lat, mem, preds = evaluate_full_suite(\n",
        "        model_gptq, tokenizer, mmlu_dataset, f\"KLD-GPTQ-{threshold:.0%}\"\n",
        "    )\n",
        "\n",
        "    flip = calculate_flip_rate(base_preds, preds)\n",
        "\n",
        "    wandb.log({\n",
        "        \"Accuracy\": acc, \"Perplexity\": ppl, \"Latency\": lat, \"Static_Memory\": static_mem,\n",
        "        \"Peak_Memory\": peak_mem, \"Flip_Rate\": flip, \"Threshold\": threshold\n",
        "    })\n",
        "\n",
        "    results_table.append({\n",
        "        \"Model\": CURRENT_MODEL_ID,\n",
        "        \"Method\": \"KLD-GPTQ\",\n",
        "        \"Threshold\": threshold,\n",
        "        \"Acc\": acc,\n",
        "        \"Flip\": flip,\n",
        "        \"PPL\": ppl,\n",
        "        \"Latency\": lat,\n",
        "        \"Static Mem\": static_mem,\n",
        "        \"Peak Mem\": peak_mem\n",
        "    })\n",
        "    run.finish()\n",
        "\n",
        "shutil.rmtree(\"./gptq_temp\")\n",
        "del model_gptq\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Create DataFrame\n",
        "df = pd.DataFrame(results_table)\n",
        "\n",
        "# Display the data\n",
        "print(df)\n",
        "\n",
        "# 2. Set up the figure with 2 subplots (Static vs Peak)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# --- Plot A: Static Memory (Model Size) vs Flip Rate ---\n",
        "sns.scatterplot(\n",
        "    data=df, x='Static Mem', y='Flip', hue='Method', style='Method',\n",
        "    s=200, palette='viridis', ax=axes[0]\n",
        ")\n",
        "axes[0].set_title(\"Compression Efficiency: Static Memory vs Flip Rate\")\n",
        "axes[0].set_xlabel(\"Static Memory (GB) - [Weights Only]\")\n",
        "axes[0].set_ylabel(\"Flip Rate (Lower is Better)\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add labels for Plot A\n",
        "for i in range(df.shape[0]):\n",
        "    row = df.iloc[i]\n",
        "    # Check if 'Static_Mem' exists to avoid errors\n",
        "    if 'Static_Mem' in row:\n",
        "        axes[0].text(row.Static_Mem + 0.01, row.Flip + 0.001, f\"{row.Threshold:.0%}\", fontsize=9)\n",
        "\n",
        "# --- Plot B: Peak Memory (Runtime Cost) vs Flip Rate ---\n",
        "sns.scatterplot(\n",
        "    data=df, x='Peak Mem', y='Flip', hue='Method', style='Method',\n",
        "    s=200, palette='magma', ax=axes[1]\n",
        ")\n",
        "axes[1].set_title(\"Runtime Efficiency: Peak Memory vs Flip Rate\")\n",
        "axes[1].set_xlabel(\"Peak Memory (GB) - [Weights + Activations + Overhead]\")\n",
        "axes[1].set_ylabel(\"Flip Rate (Lower is Better)\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Add labels for Plot B\n",
        "for i in range(df.shape[0]):\n",
        "    row = df.iloc[i]\n",
        "    if 'Peak_Mem' in row:\n",
        "        axes[1].text(row.Peak_Mem + 0.05, row.Flip + 0.001, f\"{row.Threshold:.0%}\", fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"efficiency_frontier_comparison_Experiment2.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure DataFrame is ready\n",
        "df = pd.DataFrame(results_table)\n",
        "\n",
        "# Set up the figure with 3 subplots side-by-side\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
        "\n",
        "# --- Plot 1: Perplexity (Lower is Better) ---\n",
        "sns.lineplot(\n",
        "    data=df, x='Threshold', y='PPL', hue='Method', style='Method',\n",
        "    markers=True, markersize=10, linewidth=2.5, ax=axes[0], palette='viridis'\n",
        ")\n",
        "axes[0].set_title(\"Perplexity vs Restoration (Lower is Better)\")\n",
        "axes[0].set_xlabel(\"% Layers Restored to BF16\")\n",
        "axes[0].set_ylabel(\"Perplexity\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].invert_yaxis() # Optional: If you want 'better' (lower) to be higher up, remove if standard view preferred.\n",
        "\n",
        "# --- Plot 2: Accuracy (Higher is Better) ---\n",
        "sns.lineplot(\n",
        "    data=df, x='Threshold', y='Acc', hue='Method', style='Method',\n",
        "    markers=True, markersize=10, linewidth=2.5, ax=axes[1], palette='magma'\n",
        ")\n",
        "axes[1].set_title(\"MMLU Accuracy vs Restoration (Higher is Better)\")\n",
        "axes[1].set_xlabel(\"% Layers Restored to BF16\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# --- Plot 3: Latency (Lower is Better) ---\n",
        "sns.lineplot(\n",
        "    data=df, x='Threshold', y='Latency', hue='Method', style='Method',\n",
        "    markers=True, markersize=10, linewidth=2.5, ax=axes[2], palette='coolwarm'\n",
        ")\n",
        "axes[2].set_title(\"Inference Latency vs Restoration\")\n",
        "axes[2].set_xlabel(\"% Layers Restored to BF16\")\n",
        "axes[2].set_ylabel(\"Latency (Seconds)\")\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"metrics_comparison_Experiment2.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil # Import shutil for copying files\n",
        "\n",
        "# Define the directory to save files in Google Drive\n",
        "save_dir = '/content/drive/MyDrive/Columbia-LLMSeminar/SLLM project/Jiayi/exp2'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save results_table as CSV\n",
        "df_results = pd.DataFrame(results_table)\n",
        "df_results.to_csv(os.path.join(save_dir, 'exp2_results.csv'), index=False)\n",
        "print(f\"Results table saved to {os.path.join(save_dir, 'exp2_results.csv')}\") # Corrected print statement\n",
        "\n",
        "# Copy figures to Google Drive\n",
        "figures_to_copy = [\n",
        "    'efficiency_frontier_comparison_Experiment2.png',\n",
        "    'metrics_comparison_Experiment2.png'\n",
        "]\n",
        "\n",
        "for fig_name in figures_to_copy:\n",
        "    fig_path = os.path.join('/content', fig_name)\n",
        "    if os.path.exists(fig_path):\n",
        "        # Use shutil.copy2 to copy the file, then os.remove to delete the original\n",
        "        shutil.copy2(fig_path, os.path.join(save_dir, fig_name))\n",
        "        # os.remove(fig_path) # Remove original after copying\n",
        "        print(f\"Copied {fig_name} to {os.path.join(save_dir, fig_name)} and removed original.\")\n",
        "    else:\n",
        "        print(f\"Figure {fig_name} not found in current directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz51AE_4kNIt"
      },
      "source": [
        "# Experiment 3: Model Size\n",
        "Research Question: Does this technique generalize to larger models? (Larger models are usually more robust to quantization; do they need less restoration?)\n",
        "\n",
        "Fixed Variables:\n",
        "\n",
        "Method: The \"Winner\" from Exp 2 (likely NF4 for speed or AWQ for accuracy).\n",
        "\n",
        "Threshold: Fix to the \"sweet spot\" (e.g., 5%).\n",
        "\n",
        "Independent Variable (Change this):\n",
        "\n",
        "Model Size: 1.5B vs. 7B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rMVJqUJkjib"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
