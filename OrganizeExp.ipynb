{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBv3sgvbq6DW"
      },
      "source": [
        "Experiment,Variable to Change,Fixed Variables,Purpose\n",
        "1. Sensitivity,\"Threshold % (0, 1, 5, 10, 20)\",Model: 1.5B  Method: NF4,Find the optimal trade-off (Sweet Spot).\n",
        "2. Methods,\"Method (NF4, AWQ, GPTQ)\",Model: 1.5B  Threshold: 5% (or Sweet Spot),Compare KLD impact on different quantizers.\n",
        "3. Scaling,\"Model Size (1.5B, 7B)\",Method: (Best of Exp 2)  Threshold: 5%,Test if larger models behave differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFNRiQb4R3WP"
      },
      "source": [
        "# **Setup & Dependencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BY_eeCnsWg1"
      },
      "source": [
        "**We use L4 GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BsBQ9VbffXlD",
        "outputId": "15a81420-4960-450c-e440-f48f54b0cd73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.57.3\n",
            "Uninstalling transformers-4.57.3:\n",
            "  Successfully uninstalled transformers-4.57.3\n",
            "Found existing installation: torch 2.9.0+cu126\n",
            "Uninstalling torch-2.9.0+cu126:\n",
            "  Successfully uninstalled torch-2.9.0+cu126\n",
            "Found existing installation: torchaudio 2.9.0+cu126\n",
            "Uninstalling torchaudio-2.9.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "Found existing installation: torchvision 0.24.0+cu126\n",
            "Uninstalling torchvision-0.24.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.24.0+cu126\n",
            "Found existing installation: wandb 0.23.1\n",
            "Uninstalling wandb-0.23.1:\n",
            "  Successfully uninstalled wandb-0.23.1\n",
            "Collecting llmcompressor\n",
            "  Downloading llmcompressor-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting loguru<=0.7.3,>=0.7.2 (from llmcompressor)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (6.0.3)\n",
            "Requirement already satisfied: numpy<=2.3.3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.0.2)\n",
            "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.32.4)\n",
            "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.67.1)\n",
            "Collecting torch<=2.8.0,>=2.7.0 (from llmcompressor)\n",
            "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting transformers<=4.56.2,>=4.53.0 (from llmcompressor)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets<=4.1.1,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.0.0)\n",
            "Collecting accelerate<=1.10.1,>=1.6.0 (from llmcompressor)\n",
            "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting nvidia-ml-py<=13.580.82,>=12.560.30 (from llmcompressor)\n",
            "  Downloading nvidia_ml_py-13.580.82-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: pillow<=11.3.0,>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (11.3.0)\n",
            "Collecting compressed-tensors==0.12.2 (from llmcompressor)\n",
            "  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.12.2->llmcompressor) (2.12.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (5.9.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (0.7.1)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch<=2.8.0,>=2.7.0->llmcompressor)\n",
            "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.10.1,>=1.6.0->llmcompressor) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.8.0,>=2.7.0->llmcompressor) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.8.0,>=2.7.0->llmcompressor) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.17.0)\n",
            "Downloading llmcompressor-0.8.1-py3-none-any.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-13.580.82-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m135.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-ml-py, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, loguru, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, transformers, torch, compressed-tensors, accelerate, llmcompressor\n",
            "  Attempting uninstall: nvidia-ml-py\n",
            "    Found existing installation: nvidia-ml-py 13.590.44\n",
            "    Uninstalling nvidia-ml-py-13.590.44:\n",
            "      Successfully uninstalled nvidia-ml-py-13.590.44\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "timm 1.0.22 requires torchvision, which is not installed.\n",
            "fastai 2.8.5 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.10.1 compressed-tensors-0.12.2 llmcompressor-0.8.1 loguru-0.7.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-ml-py-13.580.82 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.8.0 transformers-4.56.2 triton-3.4.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall transformers torch torchaudio torchvision wandb -y\n",
        "!pip install llmcompressor\n",
        "!pip install -q accelerate bitsandbytes datasets scipy matplotlib wandb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from datasets import Dataset\n",
        "import copy\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import wandb\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E8tJpydOUJBM"
      },
      "outputs": [],
      "source": [
        "# Set for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import set_seed\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "set_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZfycqbNSRtx"
      },
      "source": [
        "## **Experiment 2 Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LhwP5QjrlJCZ"
      },
      "outputs": [],
      "source": [
        "# --- Experiment Settings ---\n",
        "MODELS_TO_TEST = [\"Qwen/Qwen2.5-1.5B-Instruct\"]\n",
        "SENSITIVITY_THRESHOLDS = [0.0, 0.05, 0.1, 0.2, 0.3]\n",
        "CALIBRATION_SAMPLES = 128\n",
        "EVAL_SAMPLES = 5000\n",
        "WANDB_PROJECT_NAME = \"KLD_Quantization_Exp2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739,
          "referenced_widgets": [
            "3588dccaff12499f9accd29558988651",
            "00aa65e3489e4543bc38ceccbb19a120",
            "44bd6f0ff8944824bcf2b540bad6d40a",
            "d65a08b0a057495db6532fdb122f8558",
            "fd41eab1035941658cbe38a1d93d8ffb",
            "53445d7cc2404c398a4bf12d07323e99",
            "8c295fcbe6004a8eb0a47e7eb840a7fd",
            "95560e5e95a84a81b62cca7baea22016",
            "9d76c8a693204c94a7971c121e2cc6e6",
            "f7e47e2d13e342be937e88185e7cffdc",
            "d2145c5aab8146f394c6c9a6f49c6979",
            "847ccad130d8410d84b3e34b384073fb",
            "94d7feb31bbb4b9eb17a4c540f17eded",
            "7a396006415c4e4888dceca9f3c9bbc1",
            "1f3c1068a8984fae96830b56ef135402",
            "b92e52a7d73a4cc9aea69e193dd70a47",
            "7c15fc046ce84687ac3ecc1439a8f481",
            "73452d30f9344d1cbeddf0af2c2e0af9",
            "1f31c227f53e45e2880b98612fd316b2",
            "1643f11fcdfd4b6599471dcac17ef9b2",
            "3e12fc680353442ea32c8b7b5b2246b5",
            "8dea8d8377d84238b3a7a4e586ed97ae",
            "a7b11c7b6a1942f3b4db4f903558660c",
            "26e065d543cc488bb88fc9763ca860bf",
            "01ff2726e01043eb9201eb23f9e7ed72",
            "4aefba7e24f64481811418ce856421e7",
            "a58d76b94a594bed93a890d6bf65fcd0",
            "006448e80e93483b80c207813902394f",
            "dd7edd98ec5e42eaa7e26a528a1464e8",
            "e0493ea958a5457381a4bcd2ae22d0be",
            "b6395c8bb0124190893a1aaf288e39dc",
            "2dec0709331a42d0af0a02c88ace0ca5",
            "5b9b6361a425463ba0640d71b0250f87",
            "76515ba16fba457f8e0a126eaa0f3b5d",
            "cba0e8ded37041e68263c64383501ca7",
            "51a9c217860143218f39acb9f005b479",
            "70f049c6ce0f4d39b61f6a5e37cd8a0a",
            "6670b0fc66724c5f8652e24354d07dc7",
            "641ee0f19d2948229aacf3bf772a674f",
            "6ffc00fcfaef4f61a9830298943ccb5d",
            "b6d18c9df3044428b86c9f16c58ee594",
            "c0280fa1c57745b295843aaeb03f4903",
            "a0ea0c038bc947268a7eadfb66a92f30",
            "7f959112c11e4ac8b6fbc6cb07ea9801",
            "9714f2d2810a4cb79f50d5af275d3684",
            "c2aca07491ea41e8aacc0fea920481a8",
            "a44412b289c1457dab414bb3a850d9c5",
            "3250d93b5a0b4e1ca8558f14ac31c94e",
            "41b5088376ee4a12b30033f4eb8709e8",
            "c42841ebe3d04c14821ec9faa55cc1e4",
            "8de8ff12622a41b6a3f405e3cd2dcfa3",
            "98821b801a48446fb5b5643aa360cbf6",
            "135187f590e448338232093f89c9bb77",
            "ffe4d3dc541d4de4a2510fa06f4a43c2",
            "de3812f92a794cc492e11deb90d5f2d7",
            "3ece7532ed7f4d128f5eb2539fa54b99",
            "4e67d01510e943b5a8f7c10245a8849b",
            "7aeb4a9728c148328b51c50bf65bd76b",
            "d0a09bb192824c879d9e5938343eabed",
            "e1176873b1984b32a27f72209b904b22",
            "5903dae679244eb9af34d244173f98e3",
            "6edc4e5b0d5f4284b8dfcd901658d31c",
            "14ab0075c295470e987dbcf116d5b998",
            "25fda25d3fe747dc92042bbd1f6b6c88",
            "25257db6167d42abb5d3f5d367ba2618",
            "efff6bcfe8df4031a5e852b8a0394654",
            "d89d8f8bfdde4778965fd1f195c554fc",
            "a984f431adae495384fdbbe89b15a107",
            "b12bc5193d3f4c468f6fa7e3bb840645",
            "1ee75026a7bd4f3e8cddb786818643d3",
            "99f408714bc5476ebc2a7b397b0ac1de",
            "dca6b29409d34c7cbf0b885c4ffb6613",
            "b910761c0b4342aaae262039a4b80741",
            "e9e8e4962f6c4d39b89fe8adc2a08058",
            "a5c4917bb23644b5a69e623e6b443b2a",
            "c3757ff7f6a04e67832eb5f974c0deb8",
            "fada519d304f46e49e60adc9c475d0c0",
            "5699cbe0c9774347a35fd3fb1f713103",
            "654add48f9ef48b19cdcaf47a7c59bfc",
            "6a37175625ba463c959f66a84c47e719",
            "6d97b5dbcaf944bb9fd7c13247281442",
            "e4120b9bc798493eb02e612bdb02d2b6",
            "f47f7264a2e24d489ec9d8646890a6ee",
            "286b8838c85647a4b2d2c3b3b3cbc18d",
            "d3fea673f2a24c1caa24225c394811bd",
            "67ad0b10c58d457a92dcb998817ebf28",
            "9b507210153340788762eadb067450ae",
            "7173cd25ba7a4b8d8434c7fdb9601a73",
            "ffd0efd7346c46499af1cf338d3bdb8e",
            "ca67c89140e14c5aa653697f6fd71ef5",
            "41c3923cf1034a68869b1f68d711db04",
            "bf45c5aa48314e35ac50cc9e6bbb480d",
            "b77c5ab5fc664cde97b425f2e7e02d63",
            "6929ba8c75aa49ba962760d133984864",
            "8ef46dc58b204dafbc9976b005cebfd4",
            "9f65bcfbf96145bc8af0249a91e813ed",
            "fed5b046d9894c3a9e09ece4897e672c",
            "a0b43d6100e041848ffc26d1fcb1b55a",
            "4f988d7935784d0c9965c0960fa2bd67",
            "0354d74ad83c44378e847422cac80c41",
            "d3c40fad8863453f88f60e860857680d",
            "11d6634c6fab4ef6a9047ad885723093",
            "549c4cb371e449a2826f524e4c91569e",
            "1ef544c799424632b470c4672c0f51d8",
            "a9da25bca5584230a7522970058ecd1d",
            "9f23f64b1278428cb6bbb83df86fbbee",
            "bc05609cf5544b0a8212ed2cc5c2a7c6",
            "561772cbb97a4d32b67a336f88fa6274",
            "e145f606eb514df095032a7b1d860c91",
            "0711db7c9e264c05a4f2be9fc09e0b7d"
          ]
        },
        "id": "OTuaeehfEe6K",
        "outputId": "69330bb4-6964-4f91-8159-c06bb7412159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjd4123\u001b[0m (\u001b[33myq171014-columbia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MMLU Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3588dccaff12499f9accd29558988651"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dataset_infos.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "847ccad130d8410d84b3e34b384073fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "all/test-00000-of-00001.parquet:   0%|          | 0.00/3.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7b11c7b6a1942f3b4db4f903558660c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "all/validation-00000-of-00001.parquet:   0%|          | 0.00/408k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76515ba16fba457f8e0a126eaa0f3b5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "all/dev-00000-of-00001.parquet:   0%|          | 0.00/76.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9714f2d2810a4cb79f50d5af275d3684"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "all/auxiliary_train-00000-of-00001.parqu(…):   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ece7532ed7f4d128f5eb2539fa54b99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/14042 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d89d8f8bfdde4778965fd1f195c554fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/1531 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5699cbe0c9774347a35fd3fb1f713103"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating dev split:   0%|          | 0/285 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffd0efd7346c46499af1cf338d3bdb8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating auxiliary_train split:   0%|          | 0/99842 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0354d74ad83c44378e847422cac80c41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU Dataset Loaded. Size: 14042 samples.\n",
            "Global setup complete. Ready for Step 2.\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import os\n",
        "os.environ[\"WANDB_QUIET\"] = \"true\"\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "if 'results_table' not in globals():\n",
        "    results_table = []\n",
        "\n",
        "print(\"Loading MMLU Dataset...\")\n",
        "try:\n",
        "    mmlu_dataset = concatenate_datasets([\n",
        "        load_dataset(\"cais/mmlu\", \"all\", split='test')\n",
        "    ])\n",
        "    print(f\"MMLU Dataset Loaded. Size: {len(mmlu_dataset)} samples.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading MMLU: {e}\")\n",
        "    from datasets import Dataset\n",
        "    mmlu_dataset = Dataset.from_dict({\n",
        "        \"question\": [\"1+1=?\"], \"choices\": [[\"1\", \"2\", \"3\", \"4\"]], \"answer\": [1]\n",
        "    })\n",
        "\n",
        "print(\"Global setup complete. Ready for Step 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8KUm27WEe6K"
      },
      "source": [
        "## **Metrics & Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u7BWzcWfEe6K"
      },
      "outputs": [],
      "source": [
        "def recursive_getattr(obj, attr):\n",
        "    for part in attr.split('.'):\n",
        "        obj = getattr(obj, part)\n",
        "    return obj\n",
        "\n",
        "def recursive_setattr(obj, attr, val):\n",
        "    pre, _, post = attr.rpartition('.')\n",
        "    parent = recursive_getattr(obj, pre) if pre else obj\n",
        "    setattr(parent, post, val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I1VOdbZXEe6K"
      },
      "outputs": [],
      "source": [
        "# --- MMLU Logic ---\n",
        "def format_mmlu_prompt(example):\n",
        "    options = [f\"{label}. {example['choices'][i]}\" for i, label in enumerate(['A', 'B', 'C', 'D'])]\n",
        "    prompt_text = f\"Question: {example['question']}\\nOptions:\\n\" + \"\\n\".join(options) + \"\\nAnswer:\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Output only the single letter (A, B, C, or D) corresponding to the correct answer.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_text}\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "def get_mmlu_predictions(model, dataset, num_samples):\n",
        "    predictions, ground_truths = [], []\n",
        "    choices = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    choice_ids = [tokenizer.encode(c)[0] for c in choices]\n",
        "\n",
        "    for i in tqdm(range(min(num_samples, len(dataset))), desc=\"MMLU Eval\"):\n",
        "        ex = dataset[i]\n",
        "        inputs = tokenizer(format_mmlu_prompt(ex), return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits[0, -1, choice_ids]\n",
        "            pred = choices[torch.argmax(logits).item()]\n",
        "        predictions.append(pred)\n",
        "        ground_truths.append(choices[ex['answer']])\n",
        "    return predictions, ground_truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mFUf0hWKEe6K"
      },
      "outputs": [],
      "source": [
        "# --- Metrics Helpers ---\n",
        "def compute_kld(logits_p, logits_q):\n",
        "    p_probs = F.softmax(logits_p, dim=-1)\n",
        "    q_log_probs = F.log_softmax(logits_q, dim=-1)\n",
        "    return nn.KLDivLoss(reduction='batchmean')(q_log_probs, p_probs).item()\n",
        "\n",
        "def calculate_flip_rate(base_preds, new_preds):\n",
        "    \"\"\"Calculates % of answers that changed from the baseline.\"\"\"\n",
        "    if not base_preds or not new_preds: return 0.0\n",
        "    flips = sum([1 for b, n in zip(base_preds, new_preds) if b != n])\n",
        "    return flips / len(base_preds)\n",
        "\n",
        "def compute_perplexity(model, tokenizer):\n",
        "    \"\"\"Computes perplexity on a subset of WikiText-2\"\"\"\n",
        "    encodings = tokenizer(\"\\n\\n\".join(load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:20]), return_tensors=\"pt\")\n",
        "    max_length = model.config.max_position_embeddings\n",
        "    stride = 512\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "\n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Computing PPL\"):\n",
        "        end_loc = min(begin_loc + max_length, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc\n",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "            nlls.append(outputs.loss)\n",
        "\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len: break\n",
        "\n",
        "    return torch.exp(torch.stack(nlls).mean()).item()\n",
        "\n",
        "def measure_efficiency(model, tokenizer, input_text=\"Hello world\"):\n",
        "    # 1. Cleanup\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # 2. Measure Static Memory (Model Weights Only)\n",
        "    # This shows the pure effect of quantization storage\n",
        "    static_mem_bytes = torch.cuda.memory_allocated()\n",
        "    static_mem_gb = static_mem_bytes / 1024**3\n",
        "\n",
        "    # 3. Run Inference\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model.generate(**input_ids, max_new_tokens=50, min_new_tokens=50)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 4. Measure Peak Memory (Weights + KV Cache + Temp Buffers)\n",
        "    # This shows the \"True Cost\" to run the model\n",
        "    peak_mem_bytes = torch.cuda.max_memory_allocated()\n",
        "    peak_mem_gb = peak_mem_bytes / 1024**3\n",
        "\n",
        "    latency = end_time - start_time\n",
        "\n",
        "    return latency, static_mem_gb, peak_mem_gb\n",
        "\n",
        "def evaluate_full_suite(model, tokenizer, dataset, metric_name):\n",
        "    \"\"\"Runs all metrics and returns them.\"\"\"\n",
        "    print(f\"--- Evaluating: {metric_name} ---\")\n",
        "\n",
        "    # 1. Accuracy\n",
        "    preds, truths = get_mmlu_predictions(model, dataset, EVAL_SAMPLES)\n",
        "    acc = sum([1 for p, g in zip(preds, truths) if p == g]) / len(truths)\n",
        "\n",
        "    # 2. Perplexity\n",
        "    ppl = compute_perplexity(model, tokenizer)\n",
        "\n",
        "    # 3. Efficiency (Unpack 3 values now)\n",
        "    lat, static_mem, peak_mem = measure_efficiency(model, tokenizer)\n",
        "\n",
        "    print(f\"Results -> Acc: {acc:.2%}, PPL: {ppl:.2f}, Latency: {lat:.2f}s, Static Mem: {static_mem:.2f}GB, Peak Mem: {peak_mem:.2f}GB\")\n",
        "\n",
        "    # Return separate memory metrics\n",
        "    return acc, ppl, lat, static_mem, peak_mem, preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZvtjgHhEe6K"
      },
      "source": [
        "## **Advanced Sensitivity Profiling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C_AduRqREe6K"
      },
      "outputs": [],
      "source": [
        "def profile_restoration_sensitivity(model_q, model_ref, calib_input, granularity='layer'):\n",
        "    \"\"\"\n",
        "    Profiles sensitivity by measuring the KLD improvement when restoring\n",
        "    individual parts of the quantized model (model_q) back to FP16 (model_ref).\n",
        "\n",
        "    Returns:\n",
        "        sensitivity_scores: Dict mapping name -> KLD improvement (Higher is more sensitive).\n",
        "    \"\"\"\n",
        "    print(f\"Profiling Restoration Sensitivity (Granularity: {granularity})...\")\n",
        "\n",
        "    # Compute Baseline\n",
        "    model_ref.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_device = next(model_ref.parameters()).device\n",
        "        base_logits = model_ref(calib_input.to(ref_device)).logits.to(device)\n",
        "        current_logits = model_q(calib_input.to(device)).logits\n",
        "        initial_kld = compute_kld(base_logits, current_logits)\n",
        "\n",
        "    print(f\"Initial Quantized KLD: {initial_kld:.6f}\")\n",
        "\n",
        "    sensitivity_scores = {}\n",
        "\n",
        "    def get_module_by_name(module, access_string):\n",
        "        names = access_string.split(sep='.')\n",
        "        return reduce(getattr, names, module)\n",
        "\n",
        "    from functools import reduce\n",
        "\n",
        "    # Block-wise or Layer-wise\n",
        "    if granularity == 'block':\n",
        "        if hasattr(model_q, 'model') and hasattr(model_q.model, 'layers'):\n",
        "            iterable_items = list(enumerate(model_q.model.layers))\n",
        "            prefix = \"model.model.layers\"\n",
        "        else:\n",
        "            raise ValueError(\"Could not detect transformer blocks structure.\")\n",
        "        iterator = tqdm(iterable_items, desc=\"Profiling Blocks\")\n",
        "    elif granularity == 'layer':\n",
        "        # # We limit this to just the linear layers to save time\n",
        "        # iterable_items = [(n, m) for n, m in model_q.named_modules() if isinstance(m, (nn.Linear,  import_bnb_linear_type_if_needed()))]\n",
        "        iterable_items = [(n, m) for n, m in model_q.named_modules()\n",
        "                          if \"mlp\" in n or \"self_attn\" in n]\n",
        "        iterator = tqdm(iterable_items, desc=\"Profiling Layers\")\n",
        "\n",
        "    # Restoration Loop\n",
        "    for name_or_idx, module_q in iterator:\n",
        "        target_name = f\"{prefix}.{name_or_idx}\" if granularity == 'block' else name_or_idx\n",
        "        try:\n",
        "            module_ref = recursive_getattr(model_ref, target_name)\n",
        "            backup_quant_module = recursive_getattr(model_q, target_name)\n",
        "            module_fp16_gpu = copy.deepcopy(module_ref).to(device)\n",
        "            recursive_setattr(model_q, target_name, module_fp16_gpu)\n",
        "\n",
        "            # Measure New KLD\n",
        "            with torch.no_grad():\n",
        "                new_logits = model_q(calib_input.to(device)).logits\n",
        "                new_kld = compute_kld(base_logits, new_logits)\n",
        "\n",
        "            improvement = initial_kld - new_kld\n",
        "            sensitivity_scores[target_name] = improvement\n",
        "            recursive_setattr(model_q, target_name, backup_quant_module)\n",
        "\n",
        "            # Cleanup VRAM\n",
        "            del module_fp16_gpu\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {target_name}: {e}\")\n",
        "\n",
        "    return sensitivity_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU7oMJVWEe6L"
      },
      "source": [
        "## **The \"Surgery\" Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Sk7gsLZuEe6L"
      },
      "outputs": [],
      "source": [
        "def perform_surgery(model, sensitive_names, fp16_model_cpu):\n",
        "    \"\"\"\n",
        "    Replaces the sensitive quantized layers in 'model' (GPU)\n",
        "    with the original FP16 layers from 'fp16_model_cpu' (CPU).\n",
        "\n",
        "    This Generic Version uses deepcopy, so it works for:\n",
        "    - Individual Linear layers (gate_proj, q_proj)\n",
        "    - Entire Blocks (Qwen2MLP, Qwen2Attention)\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    print(f\"Surgery: Replacing {len(sensitive_names)} Sensitive Layers with FP16...\")\n",
        "\n",
        "    for name in sensitive_names:\n",
        "        try:\n",
        "            # 1. Get original FP16 module from CPU backup\n",
        "            #    (This handles Linear, Qwen2MLP, Qwen2Attention, etc.)\n",
        "            original_module = recursive_getattr(fp16_model_cpu, name)\n",
        "\n",
        "            # 2. Create a deep copy and move to GPU\n",
        "            #    We use deepcopy instead of manually instantiating nn.Linear.\n",
        "            #    This preserves the exact class type and configuration.\n",
        "            module_fp16_gpu = copy.deepcopy(original_module).to(model.device)\n",
        "\n",
        "            # 3. Swap into the quantized model\n",
        "            recursive_setattr(model, name, module_fp16_gpu)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping layer {name}: {e}\")\n",
        "\n",
        "    print(f\"Surgery Complete: {count} layers restored.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NempZKuoEe6L"
      },
      "source": [
        "# Model Selection & Baseline Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734,
          "referenced_widgets": [
            "66ff7684a0604f1fa4cbd2fbdb1c1fec",
            "3830730cece64a4aae13da58882d1033",
            "f38c79d3e3b44b70a9e1fff80df774a3",
            "1fb6151e1f6848688b04d05673b0615a",
            "934a027ce9984a8993f7189efe0405d2",
            "7e0a0636663c498aa2f4938df35588da",
            "c381b3a5d23845bca830c2146eb54b23",
            "5436120953874d38a048a1135a1dac1a",
            "0c7112c4f4d544288bbe06368950aef4",
            "0684658ec3cc4fc9a83b20ca3a7b3221",
            "2c8f430702d144f29bdd00a7c5e662c1",
            "141ff3140cfa471592dc1db8824e1304",
            "793c889db67c4aeeb479f2121a1ce74e",
            "dd8edf7cd2974753a1b32c9b5133e5f3",
            "890972ea3e724d95b9c2295b2d5d19ae",
            "75072fbff1554ca2a88c99af77e1040b",
            "c12cfeab568a4b7a8e1af92deb88a194",
            "219ace21263f47e9bc39b535f73f7e1f",
            "fa585e10a2c042ed9fd1fd85ed1aabb4",
            "1212ea5f4fa944759be13331cdd92467",
            "de0333593ce14530a80176b20be2e293",
            "40fb88f57b8a40e4ac5be1f8ccecde7c",
            "ea6dc92fb5cb4fe2ae100324c6cc8093",
            "4898310262254dd4aaf1d9559ec057e7",
            "895d747a24b441a698ee8a19bbfc7572",
            "fa7700409c8f409bbbb4c1caf186732d",
            "77512a5ff2b34d00a183173134219e53",
            "ca6c5eca30be49fe9dbe152fd93ce263",
            "dadb44b2d43740b48cfd7a94874599b4",
            "b87d41d904ca4674844147f881d34c68",
            "7f315d1c3273475e878e1c607dab7e70",
            "536b5088191b47169270c5cf0b6860e0",
            "ff2df24cb1834071880a52866704f2e9",
            "130e4fa08b2f46c8a9f5cb22bea9b1a7",
            "482430794cad480ba586b01f21d08434",
            "507fc21949ea4f1d80e6d4eb681a0456",
            "7a56a1f84974474880617f87e53984bd",
            "d671d6cdd3624010ad428ce18459c299",
            "f465a94d9bc446bd81247c8016eb21a7",
            "dd7809aa4f3043f8a5ba37c41e657adc",
            "5ff8b142e11c4b9eb25e27765c4499f2",
            "7b505c9d27cb496c942d8a8c73b5c6a1",
            "f6a7d6fff1134e69a2ee0d24cb1f7775",
            "bc2c0b7f8bb84f45a1fc4b2f87954cbb",
            "d1ff7f06f96149c1bb4c7c482eaa67f2",
            "19f809a8e80946e7b3a77213911d415f",
            "5b761482a88043e9b294021bffa2ff7c",
            "121cd6254189426a9d66f70046a30a9d",
            "7d55e2e0600d49a78c84a1871f54b0d6",
            "c1531ea4c74b4f1a8811081070b49246",
            "82c533e2d25c42ce9d09946f52af6730",
            "dc7988115c2b40ef92f66b1e4830b5c4",
            "1e108f27569541218b6e9dbf7f85a72e",
            "d58db0f4f7df4641b3c739641c887a5b",
            "1e9421cf27b345c98960cd6e425464c5",
            "dbfbdcb563b84767a1681f645c3662de",
            "cd7db80335ae412b8706035186c01cc1",
            "7ff04cf8ed32424c86df43998e24588c",
            "9dc1e23ab14246719130238ec88c49d7",
            "eaa45fbbdf1149459820c208916c379d",
            "9fadc10d1a37423e9150787310439576",
            "2d32869a0faf4fc29b0a9338a8a4cb1c",
            "616726e4321e49d39a9da3d2d6a4d366",
            "17fcc80397924066b243da14c79c294f",
            "3f5aed3078bb4eb3b1691c7e6226f174",
            "33c0bc95109b4b3193a36755da254faf",
            "bfc5391d05e04022ba844d7c644bf706",
            "5d6939b4136d43e791f0a82e30384340",
            "c37bd619574f47ae991e5d96e58d74d8",
            "0f024d83d2fc4fa292486f94e6280508",
            "2822a85234334819894f20a69531d02a",
            "0f7e789d13584e8fbfdf2f30057fa16c",
            "fcab44a684ce484eb1a45547869474ee",
            "58cf8a2ceb234cde88076bc95aafe992",
            "dd3f4c660af24cd1966af41be9cc8aa3",
            "56f4325a81fb4e51976d75d45b9260b0",
            "e8dde89828864054ae22ac6b9473fb47",
            "fc8cbe47100541df8eee57fe5a0bf108",
            "b995f525b51d4c7f80475b696c4ffefe",
            "55f8120035284f1ba16d021b70698492",
            "74873564dccc4c5eb0b7d0e5a795efb4",
            "d7585352ed244a67bb07e4ba09b86ebf",
            "066491dc2cc94315917d8d01ca286083",
            "f0f15bc3e1a9498cafab3abd71709f05",
            "a05391677fba42179c572d85943bbdf3",
            "4705798486a6408c9b7fab81a46824d5",
            "f67179d544b04b4d9fe88d1d31778505",
            "c1e39377afb946fc80cdf6eff0c17eeb",
            "dc38ac90b4324affbc465ea319181d15",
            "bbe121a538c040419d232ff54e8f096c",
            "f20f32a9ef5a447fb55a1a6daa9d1031",
            "21c46f7e973f466ab81f751111f4dbc8",
            "073eb9d4b5e04bcda00dd9e5842855b0",
            "b286ec64bdb8404ba14e1bae4c1b7c5e",
            "9756623c56944bf2ba946fade8da484d",
            "f50915111cb041eb90c566c71e43058f",
            "b35da9d9f2734ef1b78d229f9a2385bc",
            "26bdb3efa0d74936aeb232aac76657cd",
            "da89a40da50a45218c0477f30c1f7035",
            "645b90aab62b48ae85adda1997dac736",
            "fb34ca04c2e341f589ec5ee338d51195",
            "16dda9fef42b472da867ebe813029fc4",
            "20c244eeedf54d3aace06bf9cad2d7f7",
            "2302a57407df42809f17b59deb471c85",
            "a313dfddd490461383351934964c1b16",
            "1984bf74f88048cabe7e2742fdf6425b",
            "0a3fd4760854458e8c09266d4e3acf52",
            "8189cb3ff91242488a89bdb00755fcc6",
            "afaacc800e264115bc56b5a851523607",
            "0746c3d5d5014b70a987c49a61b23582",
            "9b779f52649b488ba3627a41c702642d",
            "ce7c6b0a96ed41bfb41e29f1f1882f15",
            "c95346755937449493b0818b3d926b34",
            "32b6851669b540ef85c852afae1b7d8e",
            "19eb0046e90142f8983b42fa5196c00c",
            "529be9c0c982400884300e9b92724a10",
            "6ea116c8d37f4354a8df65250d89bf12",
            "632605de16f74b69872882f7c28259fb",
            "c3644b6e452546b1882283769c451763",
            "9fd12f82ff3e4efc9120393ee1c7a19d",
            "54c57fd04aa64d2bafcb7b67dd690e6c",
            "0c52062613e54e7189abea7c3c9d2f6c",
            "acdc2d54d7de4d96adae347e8bbfc851",
            "00d42b973ca1494098159f1717dc6ed5",
            "d3bdd1226c6c4d638ba8f2ef1bcc46f9",
            "21aeb65e4d35455db34dc6e1eb42c74b",
            "9b25caa1d81743248ee7b1868044392b",
            "6e74e11084ad48a9b52f7c56098c3f4e",
            "6b0763a71eb04b748ad8ed99e32a4b75",
            "486671f40af34bdfa250c53566ce48fc",
            "778ff1f35dcf4140968ce341710b170f",
            "cfd4fc7cdbdf4de099dcc602365109ad",
            "30f63adbe5ba442fbbe758d22094bb2f",
            "cdf028375bc34ddb95b80cb56aa07520",
            "37c7cfb28de544dc8ab16363b0b37976",
            "818fa8d40e7b4509897d9ce989ba64e7",
            "cfb7bc375b094b8ba0c172138fd607f8",
            "9a90c497714c4bb8bc13c47ee76987d5",
            "c3db19c38695412f82b7dd0f5feb2596",
            "5bee939af951461d8cfdb760e0ed2bc4",
            "9c5e0d78eddc4ee0969c6acc7df6f635",
            "fe77fd8b8ae349608628cf0d4e0029dd",
            "00bf65ffb82e40a88d8f8ed07a2d8f24",
            "0a79c08025a84ec3a46487ec2efa69ff",
            "6cf86dcf47054064a9cbc284a150e653",
            "d34c30fc1cc24b7e8f33457bc23b9ec8",
            "a4dd1d2701ce43088e54f1452adc7884",
            "6ebb32c4f5af4e5d8ad1585aa7c5a25c",
            "77d95b33c9044456a52f8ae9bff185d2",
            "9ba0990848df44aa8403a939aec9a67f",
            "0773d514af944211a01010361258df45",
            "083a54d4667a48c2ae3565d0d7f25684",
            "da224730af574cec8c57ac0a68f028e1",
            "2f785c6c8f644c99b33c502540d8c3bd"
          ]
        },
        "id": "EFU1i3zrEe6L",
        "outputId": "80149b22-9a52-427a-e7a5-dcba2d45d8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "Selected Model: Qwen/Qwen2.5-1.5B-Instruct\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66ff7684a0604f1fa4cbd2fbdb1c1fec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "141ff3140cfa471592dc1db8824e1304"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea6dc92fb5cb4fe2ae100324c6cc8093"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "130e4fa08b2f46c8a9f5cb22bea9b1a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading FP16 Baseline (This may take a minute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1ff7f06f96149c1bb4c7c482eaa67f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbfbdcb563b84767a1681f645c3662de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfc5391d05e04022ba844d7c644bf706"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating: FP16 Baseline ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MMLU Eval: 100%|██████████| 5000/5000 [03:10<00:00, 26.26it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc8cbe47100541df8eee57fe5a0bf108"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc38ac90b4324affbc465ea319181d15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "645b90aab62b48ae85adda1997dac736"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b779f52649b488ba3627a41c702642d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c52062613e54e7189abea7c3c9d2f6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30f63adbe5ba442fbbe758d22094bb2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a79c08025a84ec3a46487ec2efa69ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing PPL:   0%|          | 0/3 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results -> Acc: 57.04%, PPL: 6.33, Latency: 1.91s, Static Mem: 2.88GB, Peak Mem: 2.89GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/axusra8f' target=\"_blank\">Qwen2.5-1.5B-Instruct-Baseline</a></strong> <br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/axusra8f' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/axusra8f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Qwen2.5-1.5B-Instruct-Baseline</strong> at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/axusra8f' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/axusra8f</a><br> View project at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Loaded & Evaluated.\n"
          ]
        }
      ],
      "source": [
        "# Select model\n",
        "CURRENT_MODEL_ID = MODELS_TO_TEST[0]\n",
        "\n",
        "print(f\"{'='*40}\\nSelected Model: {CURRENT_MODEL_ID}\\n{'='*40}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CURRENT_MODEL_ID)\n",
        "print(\"Loading FP16 Baseline (This may take a minute)...\")\n",
        "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "    CURRENT_MODEL_ID,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Evaluate Baseline\n",
        "base_acc, base_ppl, base_lat, base_static_mem, base_peak_mem, base_preds = evaluate_full_suite(\n",
        "    model_fp16, tokenizer, mmlu_dataset, \"FP16 Baseline\"\n",
        ")\n",
        "\n",
        "# Log Baseline to WandB\n",
        "run = wandb.init(project=WANDB_PROJECT_NAME, name=f\"{CURRENT_MODEL_ID.split('/')[-1]}-Baseline\", reinit=True)\n",
        "wandb.log({\n",
        "    \"Accuracy\": base_acc,\n",
        "    \"Perplexity\": base_ppl,\n",
        "    \"Latency\": base_lat,\n",
        "    \"Static_Memory\": base_static_mem,\n",
        "    \"Peak_Memory\": base_peak_mem,\n",
        "    \"Threshold\": 0,\n",
        "    \"Flip_Rate\": 0.0,\n",
        "    \"Method\": \"Baseline\"\n",
        "})\n",
        "run.finish()\n",
        "\n",
        "# Store in Results Table\n",
        "results_table.append({\n",
        "    \"Model\": CURRENT_MODEL_ID,\n",
        "    \"Method\": \"FP16 Baseline\",\n",
        "    \"Threshold\": 0,\n",
        "    \"Acc\": base_acc,\n",
        "    \"Flip\": 0.0,\n",
        "    \"PPL\": base_ppl,\n",
        "    \"Latency\": base_lat,\n",
        "    \"Static Mem\": base_static_mem,\n",
        "    \"Peak Mem\": base_peak_mem\n",
        "})\n",
        "\n",
        "print(\"Baseline Loaded & Evaluated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_90Y6y2hEe6L",
        "outputId": "b27c8305-6d4d-416d-8234-e8dee73f090d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing Calibration Data...\n",
            "Moving FP16 model to CPU to free up VRAM...\n",
            "VRAM Cleared. Ready for Experiments.\n"
          ]
        }
      ],
      "source": [
        "# Profiling & Offloading\n",
        "print(\"Preparing Calibration Data...\")\n",
        "calib_data = tokenizer(\n",
        "    \"\\n\\n\".join(load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10]),\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids.to(device)\n",
        "\n",
        "granularity_mode = 'layer'\n",
        "\n",
        "# Offload FP16 Model to CPU to save memory\n",
        "print(\"Moving FP16 model to CPU to free up VRAM...\")\n",
        "model_fp16.cpu()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"VRAM Cleared. Ready for Experiments.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3KtTkPVrU_o"
      },
      "source": [
        "# Preliminary Check:\n",
        "Justifying the \"Base\" Precision\n",
        "Variable: Floating Point Type (FP8 vs. FP4/NF4)\n",
        "\n",
        "Purpose: Before running complex KLD experiments, you must decide what your \"base\" low-precision format is. If FP4 destroys the model completely and FP8 is perfect, then KLD is needed for FP4 but not FP8. If both are good, FP4 is better for efficiency.\n",
        "\n",
        "Design:\n",
        "\n",
        "Run: 1.5B Model (FP16 Baseline) vs. 1.5B (FP8) vs. 1.5B (NF4).\n",
        "\n",
        "Metric: Perplexity & MMLU Accuracy.\n",
        "\n",
        "Hypothesis: NF4 offers higher compression but higher degradation than FP8. This justifies using NF4 (or INT4 methods) as the primary candidate for your KLD restoration because it needs the help more than FP8 does.\n",
        "\n",
        "Decision: If verified, fix 4-bit (NF4/INT4) as the standard base for the rest of the experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_vSknhGrb5_"
      },
      "source": [
        "**Refer to the FP8 vs FP4 notebook**\n",
        "\n",
        "**Results: NF4 offers higher compression but higher degradation than FP8. We will use NF4 (or INT4 methods) as the primary candidate for the following experiments because it needs the help more than FP8 does.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S49TWxQ0OLeO"
      },
      "source": [
        "# Experiment 1: Sensitivity Analysis\n",
        "Research Question: How much of the model actually needs to be kept in high precision to recover performance? Is there a point of diminishing returns?\n",
        "\n",
        "Fixed Variables:\n",
        "\n",
        "Model: Qwen2.5-1.5B (Small enough to run fast, big enough to show signal).\n",
        "\n",
        "Method: NF4 (The simplest 4-bit baseline).\n",
        "\n",
        "Independent Variable (Change this):\n",
        "\n",
        "KLD Threshold / % Restored: 0% (Baseline), 1%, 5%, 10%, 20%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUZ8LdxhjgT4"
      },
      "source": [
        "**Refer to the FP8 vs FP4 notebook**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "imX3DJ0JrUsU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyT4R4Orjq1u"
      },
      "source": [
        "# Experiment 2: Algorithm Comparison\n",
        "Research Question: Does KLD guidance work better on top of simple quantization (NF4) or advanced quantization (AWQ/GPTQ)?\n",
        "\n",
        "Fixed Variables:\n",
        "\n",
        "Model: Qwen2.5-1.5B (Consistent with Exp 1).\n",
        "\n",
        "Threshold: Fix this to the \"winner\" from Exp 1 (e.g., if 5% was the sweet spot, use 5% for all).\n",
        "\n",
        "Independent Variable (Change this):\n",
        "\n",
        "Method: NF4 vs. AWQ vs. GPTQ.\n",
        "\n",
        "Why: AWQ and GPTQ already do some optimization. You want to see if your KLD method adds value on top of them, or if it's only useful for naive methods like NF4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzIk75EWEe6L"
      },
      "source": [
        "#### AWQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5fPCHiUjEe6M"
      },
      "outputs": [],
      "source": [
        "from llmcompressor.modifiers.awq import AWQModifier\n",
        "from llmcompressor.modifiers.quantization import GPTQModifier\n",
        "from llmcompressor import oneshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hc0qTTETq3aq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3b576484266d494e95b11bfbc93c85cd",
            "8dac8fbc0533416dbe01002bae1996f6",
            "d173082019384afe804e97cb7ebae64a",
            "cf2b88cb612b4971b5b0e9855ceb2d43",
            "72a1725891fd44dc8b4b010f32af6ff3",
            "d346fb7689c24b4e8495f0f545557c4c",
            "1d9c5685325d43d899395916ee0d08fb",
            "9e6ec8f9ce524ede8563d2db955f664b",
            "c72095992b4f4dbd8d369111fc6a1089",
            "67fcb770e4554e028c284b88f41f42de",
            "f12b9341935d43eb89f039f8742fdbb9"
          ]
        },
        "outputId": "93ed4610-372c-49d4-bca7-c9eba805167e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Experiment: AWQ (Qwen/Qwen2.5-1.5B-Instruct) ---\n",
            "Running AWQ Oneshot Quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|          | 0/128 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b576484266d494e95b11bfbc93c85cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T02:17:59.510724+0000 | reset | INFO - Compression lifecycle reset\n",
            "2025-12-17T02:17:59.520085+0000 | _create_default_logger | INFO - Logging all LLM Compressor modifier-level logs to sparse_logs/17-12-2025_02.17.59.log\n",
            "2025-12-17T02:17:59.521046+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
            "2025-12-17T02:17:59.555096+0000 | on_initialize | INFO - No AWQModifier.mappings provided, inferring from model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resolving mapping 1/4 (0 skipped): : 28it [00:00, 1268.16it/s]\n",
            "Resolving mapping 2/4 (27 skipped): : 28it [00:00, 1547.49it/s]\n",
            "Resolving mapping 3/4 (0 skipped): : 28it [00:00, 1228.78it/s]\n",
            "Resolving mapping 4/4 (0 skipped): : 28it [00:00, 1561.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T02:17:59.656091+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
            "2025-12-17T02:17:59.657016+0000 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `AWQModifier`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Preparing cache: 100%|██████████| 128/128 [00:00<00:00, 177.26it/s]\n",
            "(1/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 56.11it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.30s/it]\n",
            "(1/29): Propagating: 100%|██████████| 128/128 [00:01<00:00, 100.07it/s]\n",
            "(2/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 88.98it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:07<00:00,  2.34s/it]\n",
            "(2/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 307.95it/s]\n",
            "(3/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 126.65it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.32s/it]\n",
            "(3/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 308.12it/s]\n",
            "(4/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 125.09it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.32s/it]\n",
            "(4/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 309.67it/s]\n",
            "(5/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 115.30it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n",
            "(5/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 309.53it/s]\n",
            "(6/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 125.11it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n",
            "(6/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 309.78it/s]\n",
            "(7/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 125.89it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.32s/it]\n",
            "(7/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 302.89it/s]\n",
            "(8/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 127.14it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.28s/it]\n",
            "(8/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 312.37it/s]\n",
            "(9/29): Calibrating: 100%|██████████| 128/128 [00:00<00:00, 128.17it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.29s/it]\n",
            "(9/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 310.81it/s]\n",
            "(10/29): Calibrating: 100%|██████████| 128/128 [00:00<00:00, 130.03it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.29s/it]\n",
            "(10/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 303.37it/s]\n",
            "(11/29): Calibrating: 100%|██████████| 128/128 [00:00<00:00, 128.93it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.29s/it]\n",
            "(11/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 302.27it/s]\n",
            "(12/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 114.58it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n",
            "(12/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 308.52it/s]\n",
            "(13/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 109.78it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:07<00:00,  2.35s/it]\n",
            "(13/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 314.26it/s]\n",
            "(14/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 127.02it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n",
            "(14/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 305.54it/s]\n",
            "(15/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 118.93it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.29s/it]\n",
            "(15/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 301.27it/s]\n",
            "(16/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 127.82it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.30s/it]\n",
            "(16/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 312.58it/s]\n",
            "(17/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 127.90it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.30s/it]\n",
            "(17/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 308.46it/s]\n",
            "(18/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 126.10it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n",
            "(18/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 313.63it/s]\n",
            "(19/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 127.71it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n",
            "(19/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 307.78it/s]\n",
            "(20/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 126.28it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.30s/it]\n",
            "(20/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 311.40it/s]\n",
            "(21/29): Calibrating: 100%|██████████| 128/128 [00:00<00:00, 128.25it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.29s/it]\n",
            "(21/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 312.00it/s]\n",
            "(22/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 123.88it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.30s/it]\n",
            "(22/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 310.15it/s]\n",
            "(23/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 127.18it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n",
            "(23/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 304.96it/s]\n",
            "(24/29): Calibrating: 100%|██████████| 128/128 [00:00<00:00, 128.09it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.29s/it]\n",
            "(24/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 308.44it/s]\n",
            "(25/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 118.51it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.32s/it]\n",
            "(25/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 304.02it/s]\n",
            "(26/29): Calibrating: 100%|██████████| 128/128 [00:00<00:00, 129.72it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n",
            "(26/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 313.09it/s]\n",
            "(27/29): Calibrating: 100%|██████████| 128/128 [00:00<00:00, 128.93it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.30s/it]\n",
            "(27/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 308.23it/s]\n",
            "(28/29): Calibrating: 100%|██████████| 128/128 [00:01<00:00, 124.72it/s]\n",
            "Smoothing: 100%|██████████| 3/3 [00:06<00:00,  2.30s/it]\n",
            "(28/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 309.75it/s]\n",
            "(29/29): Calibrating: 100%|██████████| 128/128 [00:00<00:00, 166.40it/s]\n",
            "Smoothing: 0it [00:00, ?it/s]\n",
            "(29/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 172.67it/s]\n",
            "Smoothing: 0it [00:00, ?it/s]\n",
            "Calibrating weights: 197it [00:02, 73.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T02:22:03.662600+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n",
            "2025-12-17T02:22:03.716883+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Compressing model: 197it [00:03, 51.29it/s]\n",
            "Compressing model: 197it [00:00, 993.49it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiling AWQ Sensitivity...\n",
            "Profiling Restoration Sensitivity (Granularity: block)...\n",
            "Initial Quantized KLD: 142.224121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Blocks: 100%|██████████| 28/28 [00:00<00:00, 30840.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.model.layers.0: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.1: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.2: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.3: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.4: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.5: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.6: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.7: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.8: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.9: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.10: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.11: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.12: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.13: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.14: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.15: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.16: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.17: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.18: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.19: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.20: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.21: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.22: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.23: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.24: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.25: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.26: 'Qwen2Model' object has no attribute 'model'\n",
            "Skipping model.model.layers.27: 'Qwen2Model' object has no attribute 'model'\n",
            "\n",
            "Targeting Threshold: 0% kept in FP16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/l0j47l23' target=\"_blank\">Qwen2.5-1.5B-Instruct-AWQ-0.0</a></strong> <br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/l0j47l23' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/l0j47l23</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating: KLD-AWQ-0% ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MMLU Eval: 100%|██████████| 5000/5000 [15:48<00:00,  5.27it/s]\n",
            "Computing PPL:   0%|          | 0/3 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results -> Acc: 51.90%, PPL: 7.77, Latency: 9.05s, Static Mem: 4.06GB, Peak Mem: 6.24GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Qwen2.5-1.5B-Instruct-AWQ-0.0</strong> at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/l0j47l23' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/l0j47l23</a><br> View project at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Targeting Threshold: 5% kept in FP16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/gmrb6xcd' target=\"_blank\">Qwen2.5-1.5B-Instruct-AWQ-0.05</a></strong> <br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/gmrb6xcd' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/gmrb6xcd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating: KLD-AWQ-5% ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MMLU Eval: 100%|██████████| 5000/5000 [15:48<00:00,  5.27it/s]\n",
            "Computing PPL:   0%|          | 0/3 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results -> Acc: 51.90%, PPL: 7.77, Latency: 8.92s, Static Mem: 4.07GB, Peak Mem: 6.24GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Qwen2.5-1.5B-Instruct-AWQ-0.05</strong> at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/gmrb6xcd' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/gmrb6xcd</a><br> View project at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Targeting Threshold: 10% kept in FP16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/3mjsdm1d' target=\"_blank\">Qwen2.5-1.5B-Instruct-AWQ-0.1</a></strong> <br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/3mjsdm1d' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/3mjsdm1d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating: KLD-AWQ-10% ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MMLU Eval: 100%|██████████| 5000/5000 [15:48<00:00,  5.27it/s]\n",
            "Computing PPL:   0%|          | 0/3 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results -> Acc: 51.90%, PPL: 7.77, Latency: 8.98s, Static Mem: 4.07GB, Peak Mem: 6.25GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Qwen2.5-1.5B-Instruct-AWQ-0.1</strong> at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/3mjsdm1d' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/3mjsdm1d</a><br> View project at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Targeting Threshold: 20% kept in FP16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/50tm8ycs' target=\"_blank\">Qwen2.5-1.5B-Instruct-AWQ-0.2</a></strong> <br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/50tm8ycs' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/50tm8ycs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating: KLD-AWQ-20% ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MMLU Eval: 100%|██████████| 5000/5000 [15:49<00:00,  5.27it/s]\n",
            "Computing PPL:   0%|          | 0/3 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results -> Acc: 51.90%, PPL: 7.77, Latency: 8.93s, Static Mem: 4.06GB, Peak Mem: 6.24GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Qwen2.5-1.5B-Instruct-AWQ-0.2</strong> at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/50tm8ycs' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/50tm8ycs</a><br> View project at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Targeting Threshold: 30% kept in FP16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/ux2b5ht1' target=\"_blank\">Qwen2.5-1.5B-Instruct-AWQ-0.3</a></strong> <br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/ux2b5ht1' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/ux2b5ht1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating: KLD-AWQ-30% ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MMLU Eval: 100%|██████████| 5000/5000 [15:50<00:00,  5.26it/s]\n",
            "Computing PPL:   0%|          | 0/3 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results -> Acc: 51.90%, PPL: 7.77, Latency: 9.09s, Static Mem: 4.06GB, Peak Mem: 6.24GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Qwen2.5-1.5B-Instruct-AWQ-0.3</strong> at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/ux2b5ht1' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/ux2b5ht1</a><br> View project at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# AWQ\n",
        "print(f\"\\n--- Starting Experiment: AWQ ({CURRENT_MODEL_ID}) ---\")\n",
        "\n",
        "print(\"Running AWQ Oneshot Quantization...\")\n",
        "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "calib_data_obj = Dataset.from_dict({\"text\": [text for text in ds[\"text\"] if len(text) > 0][:128]})\n",
        "\n",
        "recipe = [AWQModifier(targets=\"Linear\", scheme=\"W4A16\")]\n",
        "oneshot(\n",
        "    model=CURRENT_MODEL_ID,\n",
        "    dataset=calib_data_obj,\n",
        "    recipe=recipe,\n",
        "    output_dir=\"./awq_temp\",\n",
        "    num_calibration_samples=128,\n",
        "    max_seq_length=512,\n",
        "    save_compressed=True\n",
        ")\n",
        "\n",
        "model_awq = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./awq_temp\", device_map=\"auto\", trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Profiling AWQ Sensitivity...\")\n",
        "awq_sensitivity = profile_restoration_sensitivity(\n",
        "    model_q=model_awq,\n",
        "    model_ref=model_fp16,\n",
        "    calib_input=calib_data,\n",
        "    granularity='block'\n",
        ")\n",
        "\n",
        "sorted_awq = sorted(awq_sensitivity.items(), key=lambda x: x[1], reverse=True)\n",
        "all_layer_names = [n for n, s in sorted_awq]\n",
        "\n",
        "# Experiment loop\n",
        "sorted_thresholds = sorted(SENSITIVITY_THRESHOLDS)\n",
        "current_restored_count = 0\n",
        "\n",
        "for threshold in sorted_thresholds:\n",
        "    print(f\"\\nTargeting Threshold: {threshold:.0%} kept in FP16\")\n",
        "\n",
        "    target_count = int(len(all_layer_names) * threshold)\n",
        "    layers_to_fix_now = all_layer_names[current_restored_count : target_count]\n",
        "\n",
        "    if layers_to_fix_now:\n",
        "        print(f\"Restoring {len(layers_to_fix_now)} additional layers...\")\n",
        "        perform_surgery(model_awq, layers_to_fix_now, model_fp16)\n",
        "        current_restored_count = target_count\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=WANDB_PROJECT_NAME,\n",
        "        name=f\"{CURRENT_MODEL_ID.split('/')[-1]}-AWQ-{threshold}\",\n",
        "        config={\"model\": CURRENT_MODEL_ID, \"method\": \"KLD-AWQ\", \"threshold\": threshold},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    acc, ppl, lat, static_mem, peak_mem, preds = evaluate_full_suite(\n",
        "        model_awq, tokenizer, mmlu_dataset, f\"KLD-AWQ-{threshold:.0%}\"\n",
        "    )\n",
        "\n",
        "    flip = calculate_flip_rate(base_preds, preds)\n",
        "\n",
        "    wandb.log({\n",
        "        \"Accuracy\": acc, \"Perplexity\": ppl, \"Latency\": lat, \"Static_Memory\": static_mem,\n",
        "        \"Peak_Memory\": peak_mem, \"Flip_Rate\": flip, \"Threshold\": threshold\n",
        "    })\n",
        "\n",
        "    results_table.append({\n",
        "        \"Model\": CURRENT_MODEL_ID,\n",
        "        \"Method\": \"KLD-AWQ\",\n",
        "        \"Threshold\": threshold,\n",
        "        \"Acc\": acc,\n",
        "        \"Flip\": flip,\n",
        "        \"PPL\": ppl,\n",
        "        \"Latency\": lat,\n",
        "        \"Static Mem\": static_mem,\n",
        "        \"Peak Mem\": peak_mem\n",
        "    })\n",
        "    run.finish()\n",
        "\n",
        "shutil.rmtree(\"./awq_temp\")\n",
        "del model_awq\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mShpzqiEe6M"
      },
      "source": [
        "#### GPTQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a3ed54310d024410b06e96ca947f5670",
            "7e022f0566724ce0b1e5816eaf701066",
            "2123bb2539f1405987d1d1f6eb4aa5d2",
            "8c1199ee398e4cd7b43118d664a5300b",
            "418ebd063c5a48c2879c5dd98e9dc71b",
            "5364ccd6e4a24beab4cd4170a0e4b650",
            "8cc2f2fc66524ae387d8533a3f99bbdd",
            "784748a5ff904dfeac01e5738165d2d9",
            "bc45ef0877c74d3ea992817d9b74402d",
            "d12fdee8f41c4a4685af49ee49f72b5d",
            "2fc136c1b08b483b8081dbecaa3dbb3a"
          ]
        },
        "id": "VYW3CidLEe6M",
        "outputId": "291587c1-836a-47a8-c76a-3f63ab3c1bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Experiment: GPTQ (Qwen/Qwen2.5-1.5B-Instruct) ---\n",
            "Running GPTQ Optimization...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|          | 0/128 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3ed54310d024410b06e96ca947f5670"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:18:47.673653+0000 | reset | INFO - Compression lifecycle reset\n",
            "2025-12-17T04:18:47.677213+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
            "2025-12-17T04:18:47.716021+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
            "2025-12-17T04:18:47.717778+0000 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `GPTQModifier`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preparing cache: 100%|██████████| 128/128 [00:01<00:00, 118.11it/s]\n",
            "(1/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 45.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:18:52.836161+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:18:53.768407+0000 | compress | METRIC - time 0.93s\n",
            "2025-12-17T04:18:53.770683+0000 | compress | METRIC - error 710.89\n",
            "2025-12-17T04:18:53.772297+0000 | compress | METRIC - GPU 0 | usage: 83.07% | total memory: 24 GB\n",
            "2025-12-17T04:18:53.773710+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:18:53.775049+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:18:54.673974+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:18:54.676151+0000 | compress | METRIC - error 109.75\n",
            "2025-12-17T04:18:54.677841+0000 | compress | METRIC - GPU 0 | usage: 83.07% | total memory: 24 GB\n",
            "2025-12-17T04:18:54.679119+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:18:54.680518+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:18:55.574114+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:18:55.576375+0000 | compress | METRIC - error 12.60\n",
            "2025-12-17T04:18:55.577810+0000 | compress | METRIC - GPU 0 | usage: 83.07% | total memory: 24 GB\n",
            "2025-12-17T04:18:55.579216+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:18:55.580553+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:18:56.514778+0000 | compress | METRIC - time 0.93s\n",
            "2025-12-17T04:18:56.517058+0000 | compress | METRIC - error 82.77\n",
            "2025-12-17T04:18:56.518441+0000 | compress | METRIC - GPU 0 | usage: 83.07% | total memory: 24 GB\n",
            "2025-12-17T04:18:56.519693+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:18:56.520991+0000 | compress_modules | INFO - Quantizing model.layers.0.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:18:57.524952+0000 | compress | METRIC - time 1.00s\n",
            "2025-12-17T04:18:57.527325+0000 | compress | METRIC - error 1237.71\n",
            "2025-12-17T04:18:57.528868+0000 | compress | METRIC - GPU 0 | usage: 83.07% | total memory: 24 GB\n",
            "2025-12-17T04:18:57.530098+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:18:57.531587+0000 | compress_modules | INFO - Quantizing model.layers.0.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:18:58.506744+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:18:58.509028+0000 | compress | METRIC - error 852.88\n",
            "2025-12-17T04:18:58.510548+0000 | compress | METRIC - GPU 0 | usage: 83.07% | total memory: 24 GB\n",
            "2025-12-17T04:18:58.511748+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:18:58.513130+0000 | compress_modules | INFO - Quantizing model.layers.0.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:19:04.110942+0000 | compress | METRIC - time 5.60s\n",
            "2025-12-17T04:19:04.116616+0000 | compress | METRIC - error 83.49\n",
            "2025-12-17T04:19:04.117989+0000 | compress | METRIC - GPU 0 | usage: 85.75% | total memory: 24 GB\n",
            "2025-12-17T04:19:04.119182+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(1/29): Propagating: 100%|██████████| 128/128 [00:01<00:00, 68.99it/s]\n",
            "(2/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:19:08.705177+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:19:09.615605+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:19:09.617936+0000 | compress | METRIC - error 439.97\n",
            "2025-12-17T04:19:09.619492+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:09.620771+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:19:09.622204+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:19:10.514133+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:19:10.516296+0000 | compress | METRIC - error 127.43\n",
            "2025-12-17T04:19:10.517673+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:10.518852+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:19:10.520376+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:19:11.406071+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:19:11.408288+0000 | compress | METRIC - error 29.14\n",
            "2025-12-17T04:19:11.409733+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:11.410965+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:19:11.412202+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:19:12.309357+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:19:12.311682+0000 | compress | METRIC - error 38.53\n",
            "2025-12-17T04:19:12.313101+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:12.314228+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:19:12.315569+0000 | compress_modules | INFO - Quantizing model.layers.1.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:19:13.292271+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:19:13.294408+0000 | compress | METRIC - error 33216.43\n",
            "2025-12-17T04:19:13.296105+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:13.297409+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:19:13.298768+0000 | compress_modules | INFO - Quantizing model.layers.1.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:19:14.286622+0000 | compress | METRIC - time 0.99s\n",
            "2025-12-17T04:19:14.288818+0000 | compress | METRIC - error 27285.75\n",
            "2025-12-17T04:19:14.290109+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:14.291399+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:19:14.292703+0000 | compress_modules | INFO - Quantizing model.layers.1.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:19:19.928974+0000 | compress | METRIC - time 5.64s\n",
            "2025-12-17T04:19:19.935177+0000 | compress | METRIC - error 15377.56\n",
            "2025-12-17T04:19:19.936875+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:19:19.938317+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(2/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 260.82it/s]\n",
            "(3/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:19:22.819693+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:19:23.754272+0000 | compress | METRIC - time 0.93s\n",
            "2025-12-17T04:19:23.756453+0000 | compress | METRIC - error 1234.91\n",
            "2025-12-17T04:19:23.757787+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:23.759068+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:19:23.760550+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:19:24.646802+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:19:24.649140+0000 | compress | METRIC - error 262.90\n",
            "2025-12-17T04:19:24.650739+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:24.652063+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:19:24.653416+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:19:25.531815+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:19:25.533963+0000 | compress | METRIC - error 109.83\n",
            "2025-12-17T04:19:25.535371+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:25.536524+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:19:25.537802+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:19:26.458687+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:19:26.460907+0000 | compress | METRIC - error 19.91\n",
            "2025-12-17T04:19:26.462480+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:26.463810+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:19:26.465282+0000 | compress_modules | INFO - Quantizing model.layers.2.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:19:27.430913+0000 | compress | METRIC - time 0.96s\n",
            "2025-12-17T04:19:27.432964+0000 | compress | METRIC - error 26703.81\n",
            "2025-12-17T04:19:27.434393+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:27.435651+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:19:27.436793+0000 | compress_modules | INFO - Quantizing model.layers.2.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:19:28.429835+0000 | compress | METRIC - time 0.99s\n",
            "2025-12-17T04:19:28.432055+0000 | compress | METRIC - error 17576.89\n",
            "2025-12-17T04:19:28.433397+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:28.434674+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:19:28.436090+0000 | compress_modules | INFO - Quantizing model.layers.2.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:19:34.048812+0000 | compress | METRIC - time 5.61s\n",
            "2025-12-17T04:19:34.054588+0000 | compress | METRIC - error 22127.25\n",
            "2025-12-17T04:19:34.056043+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:19:34.057372+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(3/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 302.15it/s]\n",
            "(4/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:19:36.881270+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:19:37.790139+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:19:37.792532+0000 | compress | METRIC - error 1078.17\n",
            "2025-12-17T04:19:37.793871+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:37.795450+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:19:37.796944+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:19:38.688384+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:19:38.690669+0000 | compress | METRIC - error 228.41\n",
            "2025-12-17T04:19:38.692323+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:38.693613+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:19:38.695229+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:19:39.580079+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:19:39.582071+0000 | compress | METRIC - error 128.97\n",
            "2025-12-17T04:19:39.583525+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:39.584815+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:19:39.586270+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:19:40.522946+0000 | compress | METRIC - time 0.94s\n",
            "2025-12-17T04:19:40.525503+0000 | compress | METRIC - error 32.26\n",
            "2025-12-17T04:19:40.527165+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:40.528510+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:19:40.530022+0000 | compress_modules | INFO - Quantizing model.layers.3.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:19:41.530854+0000 | compress | METRIC - time 1.00s\n",
            "2025-12-17T04:19:41.533215+0000 | compress | METRIC - error 19044.63\n",
            "2025-12-17T04:19:41.534744+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:41.536118+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:19:41.537679+0000 | compress_modules | INFO - Quantizing model.layers.3.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:19:42.534319+0000 | compress | METRIC - time 1.00s\n",
            "2025-12-17T04:19:42.536724+0000 | compress | METRIC - error 10497.36\n",
            "2025-12-17T04:19:42.538361+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:42.539566+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:19:42.540890+0000 | compress_modules | INFO - Quantizing model.layers.3.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:19:48.173856+0000 | compress | METRIC - time 5.63s\n",
            "2025-12-17T04:19:48.179476+0000 | compress | METRIC - error 538.81\n",
            "2025-12-17T04:19:48.181031+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:19:48.182296+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(4/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 299.95it/s]\n",
            "(5/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:19:50.986337+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:19:51.884126+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:19:51.886332+0000 | compress | METRIC - error 1017.68\n",
            "2025-12-17T04:19:51.887808+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:51.889078+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:19:51.890488+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:19:52.819803+0000 | compress | METRIC - time 0.93s\n",
            "2025-12-17T04:19:52.822240+0000 | compress | METRIC - error 194.82\n",
            "2025-12-17T04:19:52.823838+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:52.825113+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:19:52.826543+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:19:53.724286+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:19:53.727243+0000 | compress | METRIC - error 132.52\n",
            "2025-12-17T04:19:53.728840+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:53.730186+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:19:53.731671+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:19:54.640206+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:19:54.642520+0000 | compress | METRIC - error 82.27\n",
            "2025-12-17T04:19:54.644310+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:54.645480+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:19:54.646796+0000 | compress_modules | INFO - Quantizing model.layers.4.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:19:55.633878+0000 | compress | METRIC - time 0.99s\n",
            "2025-12-17T04:19:55.636077+0000 | compress | METRIC - error 10033.61\n",
            "2025-12-17T04:19:55.637381+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:55.638578+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:19:55.639816+0000 | compress_modules | INFO - Quantizing model.layers.4.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:19:56.622325+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:19:56.624493+0000 | compress | METRIC - error 6156.65\n",
            "2025-12-17T04:19:56.625810+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:19:56.627153+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:19:56.628500+0000 | compress_modules | INFO - Quantizing model.layers.4.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:20:02.224509+0000 | compress | METRIC - time 5.59s\n",
            "2025-12-17T04:20:02.230266+0000 | compress | METRIC - error 345.47\n",
            "2025-12-17T04:20:02.231813+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:20:02.233078+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(5/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 300.36it/s]\n",
            "(6/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:20:05.051789+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:20:05.969491+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:20:05.971787+0000 | compress | METRIC - error 1087.30\n",
            "2025-12-17T04:20:05.973378+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:05.975110+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:20:05.976691+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:20:06.877935+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:20:06.879877+0000 | compress | METRIC - error 232.03\n",
            "2025-12-17T04:20:06.881275+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:06.882605+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:20:06.883939+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:20:07.764980+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:20:07.767260+0000 | compress | METRIC - error 191.08\n",
            "2025-12-17T04:20:07.768786+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:07.770306+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:20:07.771884+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:20:08.658695+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:20:08.660800+0000 | compress | METRIC - error 104.06\n",
            "2025-12-17T04:20:08.662302+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:08.663564+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:20:08.664885+0000 | compress_modules | INFO - Quantizing model.layers.5.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:20:09.651377+0000 | compress | METRIC - time 0.99s\n",
            "2025-12-17T04:20:09.653635+0000 | compress | METRIC - error 21314.42\n",
            "2025-12-17T04:20:09.655332+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:09.656686+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:20:09.657971+0000 | compress_modules | INFO - Quantizing model.layers.5.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:20:10.618616+0000 | compress | METRIC - time 0.96s\n",
            "2025-12-17T04:20:10.620864+0000 | compress | METRIC - error 13954.43\n",
            "2025-12-17T04:20:10.622100+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:10.623564+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:20:10.624888+0000 | compress_modules | INFO - Quantizing model.layers.5.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:20:16.223123+0000 | compress | METRIC - time 5.60s\n",
            "2025-12-17T04:20:16.228882+0000 | compress | METRIC - error 434.10\n",
            "2025-12-17T04:20:16.230184+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:20:16.231434+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(6/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 294.33it/s]\n",
            "(7/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:20:19.066857+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:20:19.971927+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:20:19.974184+0000 | compress | METRIC - error 1406.10\n",
            "2025-12-17T04:20:19.975710+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:19.976913+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:20:19.978616+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:20:20.861703+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:20:20.864370+0000 | compress | METRIC - error 293.04\n",
            "2025-12-17T04:20:20.866213+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:20.867594+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:20:20.868911+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:20:21.743194+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:20:21.745403+0000 | compress | METRIC - error 156.82\n",
            "2025-12-17T04:20:21.746849+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:21.748249+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:20:21.749785+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:20:22.659263+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:20:22.661494+0000 | compress | METRIC - error 47.73\n",
            "2025-12-17T04:20:22.662982+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:22.664228+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:20:22.665470+0000 | compress_modules | INFO - Quantizing model.layers.6.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:20:23.624158+0000 | compress | METRIC - time 0.96s\n",
            "2025-12-17T04:20:23.626367+0000 | compress | METRIC - error 6497.12\n",
            "2025-12-17T04:20:23.627885+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:23.629212+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:20:23.630621+0000 | compress_modules | INFO - Quantizing model.layers.6.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:20:24.605844+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:20:24.608111+0000 | compress | METRIC - error 5451.55\n",
            "2025-12-17T04:20:24.609657+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:24.610882+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:20:24.612347+0000 | compress_modules | INFO - Quantizing model.layers.6.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:20:30.171988+0000 | compress | METRIC - time 5.56s\n",
            "2025-12-17T04:20:30.177659+0000 | compress | METRIC - error 653.58\n",
            "2025-12-17T04:20:30.179176+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:20:30.180626+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(7/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 296.91it/s]\n",
            "(8/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:20:33.008147+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:20:33.933029+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:20:33.935454+0000 | compress | METRIC - error 741.22\n",
            "2025-12-17T04:20:33.936978+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:33.938297+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:20:33.939670+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:20:34.818853+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:20:34.821592+0000 | compress | METRIC - error 143.60\n",
            "2025-12-17T04:20:34.823149+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:34.824642+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:20:34.826078+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:20:35.705784+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:20:35.708250+0000 | compress | METRIC - error 114.73\n",
            "2025-12-17T04:20:35.709732+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:35.711207+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:20:35.712545+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:20:36.615091+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:20:36.617533+0000 | compress | METRIC - error 99.01\n",
            "2025-12-17T04:20:36.619018+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:36.620343+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:20:36.621611+0000 | compress_modules | INFO - Quantizing model.layers.7.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:20:37.621089+0000 | compress | METRIC - time 1.00s\n",
            "2025-12-17T04:20:37.623411+0000 | compress | METRIC - error 5167.10\n",
            "2025-12-17T04:20:37.624947+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:37.626346+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:20:37.627899+0000 | compress_modules | INFO - Quantizing model.layers.7.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:20:38.625810+0000 | compress | METRIC - time 1.00s\n",
            "2025-12-17T04:20:38.628031+0000 | compress | METRIC - error 4949.92\n",
            "2025-12-17T04:20:38.629400+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:38.630735+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:20:38.632316+0000 | compress_modules | INFO - Quantizing model.layers.7.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:20:44.182538+0000 | compress | METRIC - time 5.55s\n",
            "2025-12-17T04:20:44.188364+0000 | compress | METRIC - error 500.54\n",
            "2025-12-17T04:20:44.189810+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:20:44.191212+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(8/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 300.98it/s]\n",
            "(9/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:20:47.010587+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:20:47.918996+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:20:47.921247+0000 | compress | METRIC - error 1552.70\n",
            "2025-12-17T04:20:47.922532+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:47.923825+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:20:47.925376+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:20:48.844259+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:20:48.846733+0000 | compress | METRIC - error 273.26\n",
            "2025-12-17T04:20:48.848082+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:48.849475+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:20:48.850950+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:20:49.749467+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:20:49.751836+0000 | compress | METRIC - error 171.47\n",
            "2025-12-17T04:20:49.753159+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:49.754408+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:20:49.756090+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:20:50.689379+0000 | compress | METRIC - time 0.93s\n",
            "2025-12-17T04:20:50.691754+0000 | compress | METRIC - error 117.48\n",
            "2025-12-17T04:20:50.693049+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:50.694380+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:20:50.695810+0000 | compress_modules | INFO - Quantizing model.layers.8.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:20:51.672760+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:20:51.675086+0000 | compress | METRIC - error 5944.90\n",
            "2025-12-17T04:20:51.676791+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:51.678133+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:20:51.679607+0000 | compress_modules | INFO - Quantizing model.layers.8.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:20:52.686301+0000 | compress | METRIC - time 1.01s\n",
            "2025-12-17T04:20:52.688905+0000 | compress | METRIC - error 5466.68\n",
            "2025-12-17T04:20:52.690261+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:20:52.691795+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:20:52.693251+0000 | compress_modules | INFO - Quantizing model.layers.8.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:20:58.275994+0000 | compress | METRIC - time 5.58s\n",
            "2025-12-17T04:20:58.281815+0000 | compress | METRIC - error 468.75\n",
            "2025-12-17T04:20:58.283323+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:20:58.284646+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(9/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 296.07it/s]\n",
            "(10/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:21:01.112362+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:21:02.023793+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:21:02.026289+0000 | compress | METRIC - error 1391.74\n",
            "2025-12-17T04:21:02.027991+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:02.029363+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:21:02.030759+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:21:02.920028+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:21:02.922312+0000 | compress | METRIC - error 284.27\n",
            "2025-12-17T04:21:02.923607+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:02.924926+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:21:02.926573+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:21:03.803178+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:21:03.805651+0000 | compress | METRIC - error 164.44\n",
            "2025-12-17T04:21:03.807125+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:03.808564+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:21:03.810122+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:21:04.701967+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:21:04.704226+0000 | compress | METRIC - error 117.32\n",
            "2025-12-17T04:21:04.705724+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:04.706741+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:21:04.708557+0000 | compress_modules | INFO - Quantizing model.layers.9.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:21:05.695218+0000 | compress | METRIC - time 0.99s\n",
            "2025-12-17T04:21:05.697518+0000 | compress | METRIC - error 5939.94\n",
            "2025-12-17T04:21:05.698921+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:05.700229+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:21:05.702057+0000 | compress_modules | INFO - Quantizing model.layers.9.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:21:06.701285+0000 | compress | METRIC - time 1.00s\n",
            "2025-12-17T04:21:06.703573+0000 | compress | METRIC - error 5751.78\n",
            "2025-12-17T04:21:06.704948+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:06.706226+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:21:06.707725+0000 | compress_modules | INFO - Quantizing model.layers.9.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:21:12.308277+0000 | compress | METRIC - time 5.60s\n",
            "2025-12-17T04:21:12.313951+0000 | compress | METRIC - error 610.46\n",
            "2025-12-17T04:21:12.315525+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:21:12.316795+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(10/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 300.96it/s]\n",
            "(11/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:21:15.130709+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:21:16.039797+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:21:16.042271+0000 | compress | METRIC - error 1394.13\n",
            "2025-12-17T04:21:16.043667+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:16.044954+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:21:16.046291+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:21:16.922584+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:21:16.925133+0000 | compress | METRIC - error 270.61\n",
            "2025-12-17T04:21:16.926775+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:16.927921+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:21:16.929383+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:21:17.801661+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:21:17.803951+0000 | compress | METRIC - error 234.80\n",
            "2025-12-17T04:21:17.805448+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:17.806736+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:21:17.808141+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:21:18.714626+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:21:18.717164+0000 | compress | METRIC - error 165.96\n",
            "2025-12-17T04:21:18.718604+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:18.719861+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:21:18.721402+0000 | compress_modules | INFO - Quantizing model.layers.10.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:21:19.710819+0000 | compress | METRIC - time 0.99s\n",
            "2025-12-17T04:21:19.713225+0000 | compress | METRIC - error 5590.13\n",
            "2025-12-17T04:21:19.714789+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:19.716164+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:21:19.717685+0000 | compress_modules | INFO - Quantizing model.layers.10.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:21:20.693638+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:21:20.695873+0000 | compress | METRIC - error 5267.97\n",
            "2025-12-17T04:21:20.697411+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:20.698609+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:21:20.700110+0000 | compress_modules | INFO - Quantizing model.layers.10.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:21:26.249243+0000 | compress | METRIC - time 5.55s\n",
            "2025-12-17T04:21:26.255072+0000 | compress | METRIC - error 487.04\n",
            "2025-12-17T04:21:26.256309+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:21:26.257467+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(11/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 301.07it/s]\n",
            "(12/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:21:29.072229+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:21:30.012611+0000 | compress | METRIC - time 0.94s\n",
            "2025-12-17T04:21:30.014956+0000 | compress | METRIC - error 1486.99\n",
            "2025-12-17T04:21:30.016562+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:30.018051+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:21:30.019399+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:21:30.901516+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:21:30.903818+0000 | compress | METRIC - error 290.09\n",
            "2025-12-17T04:21:30.905314+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:30.906688+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:21:30.908054+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:21:31.801401+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:21:31.803837+0000 | compress | METRIC - error 207.56\n",
            "2025-12-17T04:21:31.805366+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:31.806578+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:21:31.808004+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:21:32.706919+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:21:32.709191+0000 | compress | METRIC - error 124.66\n",
            "2025-12-17T04:21:32.710656+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:32.711887+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:21:32.713276+0000 | compress_modules | INFO - Quantizing model.layers.11.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:21:33.689384+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:21:33.691797+0000 | compress | METRIC - error 6019.45\n",
            "2025-12-17T04:21:33.693006+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:33.694321+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:21:33.695809+0000 | compress_modules | INFO - Quantizing model.layers.11.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:21:34.705012+0000 | compress | METRIC - time 1.01s\n",
            "2025-12-17T04:21:34.707400+0000 | compress | METRIC - error 5291.23\n",
            "2025-12-17T04:21:34.708941+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:34.710317+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:21:34.711853+0000 | compress_modules | INFO - Quantizing model.layers.11.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:21:40.338568+0000 | compress | METRIC - time 5.63s\n",
            "2025-12-17T04:21:40.344773+0000 | compress | METRIC - error 475.93\n",
            "2025-12-17T04:21:40.346076+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:21:40.347262+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(12/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 295.71it/s]\n",
            "(13/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:21:43.167720+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:21:44.116292+0000 | compress | METRIC - time 0.95s\n",
            "2025-12-17T04:21:44.118665+0000 | compress | METRIC - error 1917.99\n",
            "2025-12-17T04:21:44.120269+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:44.121629+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:21:44.123109+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:21:45.038943+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:21:45.041417+0000 | compress | METRIC - error 411.33\n",
            "2025-12-17T04:21:45.073630+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:45.074847+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:21:45.076356+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:21:45.970327+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:21:45.972631+0000 | compress | METRIC - error 214.35\n",
            "2025-12-17T04:21:45.974028+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:45.975504+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:21:45.976906+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:21:46.878228+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:21:46.880665+0000 | compress | METRIC - error 104.49\n",
            "2025-12-17T04:21:46.882213+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:46.883753+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:21:46.885337+0000 | compress_modules | INFO - Quantizing model.layers.12.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:21:47.838650+0000 | compress | METRIC - time 0.95s\n",
            "2025-12-17T04:21:47.840905+0000 | compress | METRIC - error 5962.25\n",
            "2025-12-17T04:21:47.842394+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:47.843815+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:21:47.845239+0000 | compress_modules | INFO - Quantizing model.layers.12.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:21:48.798854+0000 | compress | METRIC - time 0.95s\n",
            "2025-12-17T04:21:48.801107+0000 | compress | METRIC - error 5417.96\n",
            "2025-12-17T04:21:48.802416+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:48.803673+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:21:48.804946+0000 | compress_modules | INFO - Quantizing model.layers.12.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:21:54.325873+0000 | compress | METRIC - time 5.52s\n",
            "2025-12-17T04:21:54.331736+0000 | compress | METRIC - error 491.91\n",
            "2025-12-17T04:21:54.333203+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:21:54.334503+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(13/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 292.94it/s]\n",
            "(14/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:21:57.172004+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:21:58.088171+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:21:58.090504+0000 | compress | METRIC - error 1357.13\n",
            "2025-12-17T04:21:58.091941+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:58.093270+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:21:58.094692+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:21:58.982956+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:21:58.985233+0000 | compress | METRIC - error 271.90\n",
            "2025-12-17T04:21:58.986673+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:58.987921+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:21:58.989361+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:21:59.867358+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:21:59.869609+0000 | compress | METRIC - error 180.07\n",
            "2025-12-17T04:21:59.870965+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:21:59.872189+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:21:59.873495+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:22:00.766758+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:22:00.769207+0000 | compress | METRIC - error 121.77\n",
            "2025-12-17T04:22:00.770721+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:00.772231+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:22:00.773778+0000 | compress_modules | INFO - Quantizing model.layers.13.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:22:01.749593+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:22:01.751999+0000 | compress | METRIC - error 5389.19\n",
            "2025-12-17T04:22:01.753388+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:01.754801+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:22:01.756081+0000 | compress_modules | INFO - Quantizing model.layers.13.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:22:02.726175+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:22:02.728368+0000 | compress | METRIC - error 5178.69\n",
            "2025-12-17T04:22:02.729550+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:02.730767+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:22:02.732130+0000 | compress_modules | INFO - Quantizing model.layers.13.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:22:08.358739+0000 | compress | METRIC - time 5.63s\n",
            "2025-12-17T04:22:08.364636+0000 | compress | METRIC - error 429.93\n",
            "2025-12-17T04:22:08.366067+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:22:08.367280+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(14/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 297.11it/s]\n",
            "(15/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:22:11.191607+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:22:12.151519+0000 | compress | METRIC - time 0.96s\n",
            "2025-12-17T04:22:12.153916+0000 | compress | METRIC - error 2631.61\n",
            "2025-12-17T04:22:12.155272+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:12.156683+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:22:12.158096+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:22:13.049277+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:22:13.051888+0000 | compress | METRIC - error 394.17\n",
            "2025-12-17T04:22:13.053412+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:13.054867+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:22:13.056175+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:22:13.922846+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:22:13.925206+0000 | compress | METRIC - error 301.66\n",
            "2025-12-17T04:22:13.926587+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:13.927934+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:22:13.929382+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:22:14.825353+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:22:14.827756+0000 | compress | METRIC - error 154.37\n",
            "2025-12-17T04:22:14.829075+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:14.830463+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:22:14.831813+0000 | compress_modules | INFO - Quantizing model.layers.14.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:22:15.811115+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:22:15.813445+0000 | compress | METRIC - error 5831.32\n",
            "2025-12-17T04:22:15.814841+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:15.816034+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:22:15.817679+0000 | compress_modules | INFO - Quantizing model.layers.14.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:22:16.797442+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:22:16.799811+0000 | compress | METRIC - error 5928.17\n",
            "2025-12-17T04:22:16.801055+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:16.802404+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:22:16.803848+0000 | compress_modules | INFO - Quantizing model.layers.14.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:22:22.368391+0000 | compress | METRIC - time 5.56s\n",
            "2025-12-17T04:22:22.374281+0000 | compress | METRIC - error 560.78\n",
            "2025-12-17T04:22:22.375811+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:22:22.376982+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(15/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 295.75it/s]\n",
            "(16/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:22:25.202215+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:22:26.130286+0000 | compress | METRIC - time 0.93s\n",
            "2025-12-17T04:22:26.132630+0000 | compress | METRIC - error 2947.74\n",
            "2025-12-17T04:22:26.134190+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:26.135319+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:22:26.136701+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:22:27.029370+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:22:27.031766+0000 | compress | METRIC - error 329.60\n",
            "2025-12-17T04:22:27.033323+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:27.034686+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:22:27.035956+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:22:27.910858+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:22:27.913205+0000 | compress | METRIC - error 299.78\n",
            "2025-12-17T04:22:27.914513+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:27.915671+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:22:27.917201+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:22:28.803766+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:22:28.806236+0000 | compress | METRIC - error 173.52\n",
            "2025-12-17T04:22:28.807822+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:28.809237+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:22:28.810683+0000 | compress_modules | INFO - Quantizing model.layers.15.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:22:29.783900+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:22:29.786312+0000 | compress | METRIC - error 5900.72\n",
            "2025-12-17T04:22:29.787946+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:29.789380+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:22:29.790757+0000 | compress_modules | INFO - Quantizing model.layers.15.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:22:30.752956+0000 | compress | METRIC - time 0.96s\n",
            "2025-12-17T04:22:30.755547+0000 | compress | METRIC - error 5582.38\n",
            "2025-12-17T04:22:30.757072+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:30.758389+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:22:30.760196+0000 | compress_modules | INFO - Quantizing model.layers.15.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:22:36.327792+0000 | compress | METRIC - time 5.57s\n",
            "2025-12-17T04:22:36.333654+0000 | compress | METRIC - error 593.77\n",
            "2025-12-17T04:22:36.335202+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:22:36.336485+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(16/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 298.34it/s]\n",
            "(17/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:22:39.157952+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:22:40.079479+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:22:40.081859+0000 | compress | METRIC - error 2564.20\n",
            "2025-12-17T04:22:40.083374+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:40.084662+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:22:40.086058+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:22:40.965682+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:22:40.968147+0000 | compress | METRIC - error 480.11\n",
            "2025-12-17T04:22:40.969621+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:40.970944+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:22:40.972483+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:22:41.854457+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:22:41.856734+0000 | compress | METRIC - error 374.84\n",
            "2025-12-17T04:22:41.858070+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:41.859528+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:22:41.860949+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:22:42.760996+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:22:42.763499+0000 | compress | METRIC - error 127.68\n",
            "2025-12-17T04:22:42.765081+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:42.766640+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:22:42.768191+0000 | compress_modules | INFO - Quantizing model.layers.16.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:22:43.739086+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:22:43.741510+0000 | compress | METRIC - error 6661.96\n",
            "2025-12-17T04:22:43.743148+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:43.744181+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:22:43.746110+0000 | compress_modules | INFO - Quantizing model.layers.16.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:22:44.716339+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:22:44.718599+0000 | compress | METRIC - error 6545.52\n",
            "2025-12-17T04:22:44.719989+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:44.721594+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:22:44.723027+0000 | compress_modules | INFO - Quantizing model.layers.16.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:22:50.263038+0000 | compress | METRIC - time 5.54s\n",
            "2025-12-17T04:22:50.268881+0000 | compress | METRIC - error 550.74\n",
            "2025-12-17T04:22:50.270226+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:22:50.271679+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(17/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 299.40it/s]\n",
            "(18/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:22:53.098498+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:22:54.012041+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:22:54.014387+0000 | compress | METRIC - error 2253.02\n",
            "2025-12-17T04:22:54.015822+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:54.016970+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:22:54.018262+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:22:54.936799+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:22:54.939224+0000 | compress | METRIC - error 276.60\n",
            "2025-12-17T04:22:54.940719+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:54.942017+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:22:54.943486+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:22:55.817225+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:22:55.819521+0000 | compress | METRIC - error 484.30\n",
            "2025-12-17T04:22:55.821327+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:55.822591+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:22:55.823882+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:22:56.716511+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:22:56.718864+0000 | compress | METRIC - error 105.75\n",
            "2025-12-17T04:22:56.720435+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:56.721847+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:22:56.723380+0000 | compress_modules | INFO - Quantizing model.layers.17.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:22:57.698595+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:22:57.700744+0000 | compress | METRIC - error 6503.30\n",
            "2025-12-17T04:22:57.702098+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:57.703418+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:22:57.705125+0000 | compress_modules | INFO - Quantizing model.layers.17.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:22:58.695453+0000 | compress | METRIC - time 0.99s\n",
            "2025-12-17T04:22:58.697920+0000 | compress | METRIC - error 6472.71\n",
            "2025-12-17T04:22:58.699311+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:22:58.700762+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:22:58.702335+0000 | compress_modules | INFO - Quantizing model.layers.17.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:23:04.222209+0000 | compress | METRIC - time 5.52s\n",
            "2025-12-17T04:23:04.228037+0000 | compress | METRIC - error 596.62\n",
            "2025-12-17T04:23:04.229603+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:23:04.230928+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(18/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 299.54it/s]\n",
            "(19/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:23:07.048189+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:23:07.973092+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:23:07.975586+0000 | compress | METRIC - error 1817.72\n",
            "2025-12-17T04:23:07.977100+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:07.978033+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:23:07.979689+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:23:08.883052+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:23:08.885431+0000 | compress | METRIC - error 303.91\n",
            "2025-12-17T04:23:08.887169+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:08.888665+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:23:08.890137+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:23:09.765446+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:23:09.767894+0000 | compress | METRIC - error 307.32\n",
            "2025-12-17T04:23:09.769499+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:09.770817+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:23:09.772413+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:23:10.666842+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:23:10.669337+0000 | compress | METRIC - error 162.36\n",
            "2025-12-17T04:23:10.670781+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:10.671946+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:23:10.673222+0000 | compress_modules | INFO - Quantizing model.layers.18.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:23:11.655202+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:23:11.657668+0000 | compress | METRIC - error 6774.26\n",
            "2025-12-17T04:23:11.659010+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:11.660328+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:23:11.661800+0000 | compress_modules | INFO - Quantizing model.layers.18.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:23:12.632547+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:23:12.634791+0000 | compress | METRIC - error 6678.12\n",
            "2025-12-17T04:23:12.635930+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:12.636920+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:23:12.638051+0000 | compress_modules | INFO - Quantizing model.layers.18.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:23:18.176971+0000 | compress | METRIC - time 5.54s\n",
            "2025-12-17T04:23:18.183008+0000 | compress | METRIC - error 803.90\n",
            "2025-12-17T04:23:18.184327+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:23:18.185843+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(19/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 298.47it/s]\n",
            "(20/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:23:21.007283+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:23:21.954974+0000 | compress | METRIC - time 0.95s\n",
            "2025-12-17T04:23:21.957696+0000 | compress | METRIC - error 2435.56\n",
            "2025-12-17T04:23:21.959206+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:21.960604+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:23:21.962017+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:23:22.861289+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:23:22.863843+0000 | compress | METRIC - error 321.94\n",
            "2025-12-17T04:23:22.865543+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:22.866931+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:23:22.868269+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:23:23.749626+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:23:23.752193+0000 | compress | METRIC - error 647.15\n",
            "2025-12-17T04:23:23.753839+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:23.755178+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:23:23.756727+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:23:24.681680+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:23:24.684380+0000 | compress | METRIC - error 246.04\n",
            "2025-12-17T04:23:24.686044+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:24.687514+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:23:24.689062+0000 | compress_modules | INFO - Quantizing model.layers.19.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:23:25.645708+0000 | compress | METRIC - time 0.96s\n",
            "2025-12-17T04:23:25.648006+0000 | compress | METRIC - error 8110.64\n",
            "2025-12-17T04:23:25.649363+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:25.650501+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:23:25.651883+0000 | compress_modules | INFO - Quantizing model.layers.19.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:23:26.664415+0000 | compress | METRIC - time 1.01s\n",
            "2025-12-17T04:23:26.666928+0000 | compress | METRIC - error 8701.21\n",
            "2025-12-17T04:23:26.668342+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:26.669656+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:23:26.670943+0000 | compress_modules | INFO - Quantizing model.layers.19.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:23:32.214209+0000 | compress | METRIC - time 5.54s\n",
            "2025-12-17T04:23:32.220120+0000 | compress | METRIC - error 1329.56\n",
            "2025-12-17T04:23:32.221570+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:23:32.222895+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(20/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 299.67it/s]\n",
            "(21/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:23:35.046177+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:23:35.953991+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:23:35.956531+0000 | compress | METRIC - error 3125.38\n",
            "2025-12-17T04:23:35.957852+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:35.959055+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:23:35.960474+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:23:36.869055+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:23:36.871394+0000 | compress | METRIC - error 392.27\n",
            "2025-12-17T04:23:36.872658+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:36.873889+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:23:36.875325+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:23:37.747273+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:23:37.749762+0000 | compress | METRIC - error 933.49\n",
            "2025-12-17T04:23:37.751158+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:37.752481+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:23:37.754053+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:23:38.643800+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:23:38.646173+0000 | compress | METRIC - error 293.84\n",
            "2025-12-17T04:23:38.647722+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:38.649091+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:23:38.650650+0000 | compress_modules | INFO - Quantizing model.layers.20.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:23:39.624901+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:23:39.627314+0000 | compress | METRIC - error 9020.30\n",
            "2025-12-17T04:23:39.628787+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:39.629589+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:23:39.631300+0000 | compress_modules | INFO - Quantizing model.layers.20.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:23:40.613305+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:23:40.615683+0000 | compress | METRIC - error 9248.30\n",
            "2025-12-17T04:23:40.617233+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:40.618743+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:23:40.620612+0000 | compress_modules | INFO - Quantizing model.layers.20.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:23:46.178240+0000 | compress | METRIC - time 5.56s\n",
            "2025-12-17T04:23:46.184446+0000 | compress | METRIC - error 1135.85\n",
            "2025-12-17T04:23:46.185737+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:23:46.187238+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(21/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 298.14it/s]\n",
            "(22/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:23:48.998834+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:23:49.929968+0000 | compress | METRIC - time 0.93s\n",
            "2025-12-17T04:23:49.932326+0000 | compress | METRIC - error 3046.51\n",
            "2025-12-17T04:23:49.933892+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:49.935212+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:23:49.936555+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:23:50.818215+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:23:50.820794+0000 | compress | METRIC - error 372.67\n",
            "2025-12-17T04:23:50.822344+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:50.823452+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:23:50.825154+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:23:51.702463+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:23:51.704897+0000 | compress | METRIC - error 809.39\n",
            "2025-12-17T04:23:51.706313+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:51.707599+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:23:51.708903+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:23:52.607967+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:23:52.610390+0000 | compress | METRIC - error 195.12\n",
            "2025-12-17T04:23:52.611747+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:52.613070+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:23:52.614608+0000 | compress_modules | INFO - Quantizing model.layers.21.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:23:53.576833+0000 | compress | METRIC - time 0.96s\n",
            "2025-12-17T04:23:53.579175+0000 | compress | METRIC - error 12417.66\n",
            "2025-12-17T04:23:53.580515+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:53.581858+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:23:53.583473+0000 | compress_modules | INFO - Quantizing model.layers.21.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:23:54.561922+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:23:54.564225+0000 | compress | METRIC - error 12460.84\n",
            "2025-12-17T04:23:54.565446+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:23:54.566655+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:23:54.568102+0000 | compress_modules | INFO - Quantizing model.layers.21.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:24:00.133963+0000 | compress | METRIC - time 5.56s\n",
            "2025-12-17T04:24:00.140038+0000 | compress | METRIC - error 1716.69\n",
            "2025-12-17T04:24:00.141485+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:24:00.142727+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(22/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 301.88it/s]\n",
            "(23/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:24:02.952560+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:24:03.853390+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:24:03.855888+0000 | compress | METRIC - error 2884.45\n",
            "2025-12-17T04:24:03.857344+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:03.858621+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:24:03.860059+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:24:04.757940+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:24:04.760373+0000 | compress | METRIC - error 434.48\n",
            "2025-12-17T04:24:04.761753+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:04.762943+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:24:04.764538+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:24:05.628128+0000 | compress | METRIC - time 0.86s\n",
            "2025-12-17T04:24:05.630540+0000 | compress | METRIC - error 972.13\n",
            "2025-12-17T04:24:05.631912+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:05.633224+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:24:05.634820+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:24:06.558539+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:24:06.561129+0000 | compress | METRIC - error 298.74\n",
            "2025-12-17T04:24:06.562531+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:06.563553+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:24:06.565014+0000 | compress_modules | INFO - Quantizing model.layers.22.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:24:07.553722+0000 | compress | METRIC - time 0.99s\n",
            "2025-12-17T04:24:07.556072+0000 | compress | METRIC - error 14459.79\n",
            "2025-12-17T04:24:07.557522+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:07.558890+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:24:07.560327+0000 | compress_modules | INFO - Quantizing model.layers.22.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:24:08.537139+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:24:08.539634+0000 | compress | METRIC - error 14327.71\n",
            "2025-12-17T04:24:08.541293+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:08.542871+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:24:08.544369+0000 | compress_modules | INFO - Quantizing model.layers.22.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:24:14.123711+0000 | compress | METRIC - time 5.58s\n",
            "2025-12-17T04:24:14.129688+0000 | compress | METRIC - error 2337.10\n",
            "2025-12-17T04:24:14.131276+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:24:14.132669+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(23/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 298.99it/s]\n",
            "(24/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:24:16.952689+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:24:17.852100+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:24:17.854590+0000 | compress | METRIC - error 4123.63\n",
            "2025-12-17T04:24:17.856111+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:17.857302+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:24:17.859082+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:24:18.768115+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:24:18.770602+0000 | compress | METRIC - error 484.24\n",
            "2025-12-17T04:24:18.771895+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:18.773175+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:24:18.774547+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:24:19.647113+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:24:19.649594+0000 | compress | METRIC - error 1362.77\n",
            "2025-12-17T04:24:19.651057+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:19.652297+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:24:19.653680+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:24:20.554785+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:24:20.557391+0000 | compress | METRIC - error 530.37\n",
            "2025-12-17T04:24:20.559126+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:20.560696+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:24:20.562205+0000 | compress_modules | INFO - Quantizing model.layers.23.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:24:21.553866+0000 | compress | METRIC - time 0.99s\n",
            "2025-12-17T04:24:21.556449+0000 | compress | METRIC - error 14171.42\n",
            "2025-12-17T04:24:21.557871+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:21.559035+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:24:21.560718+0000 | compress_modules | INFO - Quantizing model.layers.23.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:24:22.543044+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:24:22.545431+0000 | compress | METRIC - error 14781.16\n",
            "2025-12-17T04:24:22.546763+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:22.547792+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:24:22.549367+0000 | compress_modules | INFO - Quantizing model.layers.23.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:24:28.152608+0000 | compress | METRIC - time 5.60s\n",
            "2025-12-17T04:24:28.158845+0000 | compress | METRIC - error 2442.33\n",
            "2025-12-17T04:24:28.160278+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:24:28.161611+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(24/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 298.38it/s]\n",
            "(25/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:24:30.984035+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:24:31.901263+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:24:31.903841+0000 | compress | METRIC - error 3723.69\n",
            "2025-12-17T04:24:31.905680+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:31.907095+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:24:31.908641+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:24:32.812041+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:24:32.813861+0000 | compress | METRIC - error 495.07\n",
            "2025-12-17T04:24:32.815376+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:32.816872+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:24:32.818493+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:24:33.725200+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:24:33.727739+0000 | compress | METRIC - error 2113.94\n",
            "2025-12-17T04:24:33.729202+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:33.730505+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:24:33.731949+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:24:34.621009+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:24:34.623464+0000 | compress | METRIC - error 623.28\n",
            "2025-12-17T04:24:34.624857+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:34.626036+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:24:34.627682+0000 | compress_modules | INFO - Quantizing model.layers.24.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:24:35.593076+0000 | compress | METRIC - time 0.96s\n",
            "2025-12-17T04:24:35.595472+0000 | compress | METRIC - error 13570.86\n",
            "2025-12-17T04:24:35.596798+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:35.597956+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:24:35.599267+0000 | compress_modules | INFO - Quantizing model.layers.24.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:24:36.572867+0000 | compress | METRIC - time 0.97s\n",
            "2025-12-17T04:24:36.575187+0000 | compress | METRIC - error 14584.52\n",
            "2025-12-17T04:24:36.576577+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:36.577932+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:24:36.579240+0000 | compress_modules | INFO - Quantizing model.layers.24.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:24:42.234228+0000 | compress | METRIC - time 5.65s\n",
            "2025-12-17T04:24:42.240147+0000 | compress | METRIC - error 2546.15\n",
            "2025-12-17T04:24:42.241748+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:24:42.243030+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(25/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 300.39it/s]\n",
            "(26/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:24:45.072862+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:24:45.997235+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:24:45.999697+0000 | compress | METRIC - error 4435.08\n",
            "2025-12-17T04:24:46.001301+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:46.002608+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:24:46.004269+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:24:46.895666+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:24:46.898112+0000 | compress | METRIC - error 466.20\n",
            "2025-12-17T04:24:46.899688+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:46.901093+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:24:46.902623+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:24:47.776024+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:24:47.778503+0000 | compress | METRIC - error 2151.11\n",
            "2025-12-17T04:24:47.779970+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:47.781094+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:24:47.782615+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:24:48.663619+0000 | compress | METRIC - time 0.88s\n",
            "2025-12-17T04:24:48.665987+0000 | compress | METRIC - error 681.26\n",
            "2025-12-17T04:24:48.667437+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:48.668809+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:24:48.670580+0000 | compress_modules | INFO - Quantizing model.layers.25.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:24:49.661967+0000 | compress | METRIC - time 0.99s\n",
            "2025-12-17T04:24:49.664473+0000 | compress | METRIC - error 13688.67\n",
            "2025-12-17T04:24:49.665998+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:49.667293+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:24:49.668881+0000 | compress_modules | INFO - Quantizing model.layers.25.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:24:50.667600+0000 | compress | METRIC - time 1.00s\n",
            "2025-12-17T04:24:50.669966+0000 | compress | METRIC - error 15839.68\n",
            "2025-12-17T04:24:50.671495+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:24:50.705165+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:24:50.707036+0000 | compress_modules | INFO - Quantizing model.layers.25.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:24:56.250599+0000 | compress | METRIC - time 5.54s\n",
            "2025-12-17T04:24:56.256729+0000 | compress | METRIC - error 3644.43\n",
            "2025-12-17T04:24:56.258307+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:24:56.259587+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(26/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 292.41it/s]\n",
            "(27/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:24:59.100623+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:24:59.999780+0000 | compress | METRIC - time 0.90s\n",
            "2025-12-17T04:25:00.002276+0000 | compress | METRIC - error 4561.47\n",
            "2025-12-17T04:25:00.003941+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:00.005256+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:25:00.006761+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:25:00.918978+0000 | compress | METRIC - time 0.91s\n",
            "2025-12-17T04:25:00.921607+0000 | compress | METRIC - error 569.13\n",
            "2025-12-17T04:25:00.923537+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:00.924928+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:25:00.926285+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:25:01.797222+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:25:01.799667+0000 | compress | METRIC - error 3396.67\n",
            "2025-12-17T04:25:01.801100+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:01.802660+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:25:01.804163+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:25:02.692333+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:25:02.694582+0000 | compress | METRIC - error 769.11\n",
            "2025-12-17T04:25:02.696160+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:02.697565+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:25:02.699196+0000 | compress_modules | INFO - Quantizing model.layers.26.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:25:03.656866+0000 | compress | METRIC - time 0.96s\n",
            "2025-12-17T04:25:03.659238+0000 | compress | METRIC - error 12534.53\n",
            "2025-12-17T04:25:03.660442+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:03.661864+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:25:03.663282+0000 | compress_modules | INFO - Quantizing model.layers.26.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:25:04.643224+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:25:04.645750+0000 | compress | METRIC - error 15088.70\n",
            "2025-12-17T04:25:04.647059+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:04.648315+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:25:04.649715+0000 | compress_modules | INFO - Quantizing model.layers.26.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:25:10.247122+0000 | compress | METRIC - time 5.60s\n",
            "2025-12-17T04:25:10.252968+0000 | compress | METRIC - error 17895.39\n",
            "2025-12-17T04:25:10.254440+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:25:10.255805+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(27/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 292.31it/s]\n",
            "(28/29): Calibrating: 100%|██████████| 128/128 [00:02<00:00, 55.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:25:13.090210+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.q_proj using 128 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:25:13.985374+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:25:13.987846+0000 | compress | METRIC - error 4358.91\n",
            "2025-12-17T04:25:13.989048+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:13.990407+0000 | compress | METRIC - Compressed module size: 4.77696 MB\n",
            "2025-12-17T04:25:13.991970+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.k_proj using 128 samples\n",
            "2025-12-17T04:25:14.880475+0000 | compress | METRIC - time 0.89s\n",
            "2025-12-17T04:25:14.882808+0000 | compress | METRIC - error 461.29\n",
            "2025-12-17T04:25:14.884176+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:14.885344+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:25:14.886658+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.v_proj using 128 samples\n",
            "2025-12-17T04:25:15.753789+0000 | compress | METRIC - time 0.87s\n",
            "2025-12-17T04:25:15.756180+0000 | compress | METRIC - error 3390.49\n",
            "2025-12-17T04:25:15.757696+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:15.758922+0000 | compress | METRIC - Compressed module size: 0.79616 MB\n",
            "2025-12-17T04:25:15.760278+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.o_proj using 128 samples\n",
            "2025-12-17T04:25:16.683333+0000 | compress | METRIC - time 0.92s\n",
            "2025-12-17T04:25:16.685655+0000 | compress | METRIC - error 2089.92\n",
            "2025-12-17T04:25:16.687138+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:16.688408+0000 | compress | METRIC - Compressed module size: 4.773888 MB\n",
            "2025-12-17T04:25:16.689920+0000 | compress_modules | INFO - Quantizing model.layers.27.mlp.gate_proj using 128 samples\n",
            "2025-12-17T04:25:17.655212+0000 | compress | METRIC - time 0.96s\n",
            "2025-12-17T04:25:17.657637+0000 | compress | METRIC - error 19677.38\n",
            "2025-12-17T04:25:17.659013+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:17.660271+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:25:17.662014+0000 | compress_modules | INFO - Quantizing model.layers.27.mlp.up_proj using 128 samples\n",
            "2025-12-17T04:25:18.641345+0000 | compress | METRIC - time 0.98s\n",
            "2025-12-17T04:25:18.643761+0000 | compress | METRIC - error 20343.05\n",
            "2025-12-17T04:25:18.645048+0000 | compress | METRIC - GPU 0 | usage: 81.13% | total memory: 24 GB\n",
            "2025-12-17T04:25:18.646294+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n",
            "2025-12-17T04:25:18.647536+0000 | compress_modules | INFO - Quantizing model.layers.27.mlp.down_proj using 128 samples\n",
            "2025-12-17T04:25:24.267336+0000 | compress | METRIC - time 5.62s\n",
            "2025-12-17T04:25:24.273139+0000 | compress | METRIC - error 12193.18\n",
            "2025-12-17T04:25:24.274449+0000 | compress | METRIC - GPU 0 | usage: 83.81% | total memory: 24 GB\n",
            "2025-12-17T04:25:24.275606+0000 | compress | METRIC - Compressed module size: 27.84768 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(28/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 288.55it/s]\n",
            "(29/29): Calibrating: 100%|██████████| 128/128 [00:00<00:00, 167.68it/s]\n",
            "(29/29): Propagating: 100%|██████████| 128/128 [00:00<00:00, 173.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17T04:25:26.712211+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n",
            "2025-12-17T04:25:26.776847+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compressing model: 196it [00:03, 51.60it/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Compressing model: 196it [00:00, 891.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiling GPTQ Sensitivity...\n",
            "Profiling Restoration Sensitivity (Granularity: layer)...\n",
            "Initial Quantized KLD: 34.250000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   0%|          | 1/280 [00:00<00:53,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.0.self_attn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 105.38 MiB is free. Process 27680 has 22.05 GiB memory in use. Of the allocated memory 21.32 GiB is allocated by PyTorch, and 477.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProfiling Layers:   1%|          | 2/280 [00:00<00:53,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.0.self_attn.q_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 105.38 MiB is free. Process 27680 has 22.05 GiB memory in use. Of the allocated memory 21.32 GiB is allocated by PyTorch, and 470.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProfiling Layers:   1%|          | 3/280 [00:00<00:52,  5.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.0.self_attn.k_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 105.38 MiB is free. Process 27680 has 22.05 GiB memory in use. Of the allocated memory 21.32 GiB is allocated by PyTorch, and 474.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProfiling Layers:   1%|▏         | 4/280 [00:00<00:52,  5.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.0.self_attn.v_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 105.38 MiB is free. Process 27680 has 22.05 GiB memory in use. Of the allocated memory 21.32 GiB is allocated by PyTorch, and 473.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProfiling Layers:   2%|▏         | 5/280 [00:00<00:52,  5.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.0.self_attn.o_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 125.38 MiB is free. Process 27680 has 22.03 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 447.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   2%|▎         | 7/280 [00:01<00:52,  5.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.0.mlp: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 125.38 MiB is free. Process 27680 has 22.03 GiB memory in use. Of the allocated memory 21.27 GiB is allocated by PyTorch, and 507.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.0.mlp.gate_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 45.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.42 GiB is allocated by PyTorch, and 427.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   3%|▎         | 9/280 [00:01<00:52,  5.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.0.mlp.up_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 45.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.42 GiB is allocated by PyTorch, and 427.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.0.mlp.down_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 45.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.42 GiB is allocated by PyTorch, and 427.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   4%|▍         | 11/280 [00:02<00:50,  5.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.0.mlp.act_fn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 45.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.40 GiB is allocated by PyTorch, and 453.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.1.self_attn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 43.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 444.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   5%|▍         | 13/280 [00:02<00:49,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.1.self_attn.q_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 43.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 439.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.1.self_attn.k_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 43.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 443.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   5%|▌         | 15/280 [00:02<00:49,  5.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.1.self_attn.v_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 43.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 443.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.1.self_attn.o_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 43.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 439.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   6%|▌         | 17/280 [00:03<00:49,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.1.mlp: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 43.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 498.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.1.mlp.gate_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 43.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.38 GiB is allocated by PyTorch, and 471.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   7%|▋         | 19/280 [00:03<00:49,  5.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.1.mlp.up_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 43.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.38 GiB is allocated by PyTorch, and 471.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.1.mlp.down_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 43.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.38 GiB is allocated by PyTorch, and 471.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   8%|▊         | 21/280 [00:03<00:47,  5.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.1.mlp.act_fn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 43.38 MiB is free. Process 27680 has 22.11 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 497.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.2.self_attn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 41.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 488.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   8%|▊         | 23/280 [00:04<00:47,  5.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.2.self_attn.q_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 41.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 483.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.2.self_attn.k_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 41.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 488.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:   9%|▉         | 25/280 [00:04<00:45,  5.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.2.self_attn.v_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 41.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 489.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.2.self_attn.o_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 41.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 484.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  10%|▉         | 27/280 [00:05<00:46,  5.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.2.mlp: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 41.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.32 GiB is allocated by PyTorch, and 542.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.2.mlp.gate_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 41.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.34 GiB is allocated by PyTorch, and 516.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  10%|█         | 29/280 [00:05<00:45,  5.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.2.mlp.up_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 41.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.34 GiB is allocated by PyTorch, and 516.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.2.mlp.down_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 41.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.34 GiB is allocated by PyTorch, and 516.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  11%|█         | 31/280 [00:05<00:44,  5.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.2.mlp.act_fn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 41.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.32 GiB is allocated by PyTorch, and 542.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.3.self_attn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 39.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 534.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  12%|█▏        | 33/280 [00:06<00:43,  5.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.3.self_attn.q_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 39.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 529.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.3.self_attn.k_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 39.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 533.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  12%|█▎        | 35/280 [00:06<00:42,  5.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.3.self_attn.v_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 39.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 533.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.3.self_attn.o_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 39.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 530.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  13%|█▎        | 37/280 [00:06<00:42,  5.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.3.mlp: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 39.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.40 GiB is allocated by PyTorch, and 456.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.3.mlp.gate_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 39.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.30 GiB is allocated by PyTorch, and 562.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  14%|█▍        | 39/280 [00:07<00:42,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.3.mlp.up_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 39.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.30 GiB is allocated by PyTorch, and 561.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.3.mlp.down_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 39.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.30 GiB is allocated by PyTorch, and 562.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  15%|█▍        | 41/280 [00:07<00:41,  5.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.3.mlp.act_fn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 39.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.40 GiB is allocated by PyTorch, and 456.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.4.self_attn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 37.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 447.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  15%|█▌        | 43/280 [00:07<00:40,  5.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.4.self_attn.q_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 37.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.42 GiB is allocated by PyTorch, and 442.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.4.self_attn.k_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 37.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 446.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  16%|█▌        | 45/280 [00:08<00:40,  5.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.4.self_attn.v_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 37.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.41 GiB is allocated by PyTorch, and 446.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.4.self_attn.o_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 37.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.42 GiB is allocated by PyTorch, and 442.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  17%|█▋        | 47/280 [00:08<00:40,  5.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.4.mlp: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 37.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 500.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.4.mlp.gate_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 37.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.39 GiB is allocated by PyTorch, and 474.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  18%|█▊        | 49/280 [00:08<00:39,  5.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.4.mlp.up_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 37.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.39 GiB is allocated by PyTorch, and 474.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.4.mlp.down_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 37.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.39 GiB is allocated by PyTorch, and 474.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  18%|█▊        | 51/280 [00:09<00:38,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.4.mlp.act_fn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 37.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 501.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.5.self_attn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 35.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 492.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  19%|█▉        | 53/280 [00:09<00:37,  6.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.5.self_attn.q_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 35.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 488.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.5.self_attn.k_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 35.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 492.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  20%|█▉        | 55/280 [00:09<00:36,  6.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.5.self_attn.v_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 35.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.37 GiB is allocated by PyTorch, and 492.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.5.self_attn.o_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 35.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.38 GiB is allocated by PyTorch, and 487.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  20%|██        | 57/280 [00:10<00:36,  6.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.5.mlp: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 33.38 MiB is free. Process 27680 has 22.12 GiB memory in use. Of the allocated memory 21.33 GiB is allocated by PyTorch, and 534.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.5.mlp.gate_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 535.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  21%|██        | 59/280 [00:10<00:35,  6.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.5.mlp.up_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 7.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 533.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.5.mlp.down_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 7.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 533.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  22%|██▏       | 61/280 [00:10<00:34,  6.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.5.mlp.act_fn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 7.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.45 GiB is allocated by PyTorch, and 441.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.6.self_attn: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.34 GiB is allocated by PyTorch, and 551.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  22%|██▎       | 63/280 [00:11<00:33,  6.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.6.self_attn.q_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 546.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.6.self_attn.k_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.34 GiB is allocated by PyTorch, and 551.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  23%|██▎       | 65/280 [00:11<00:33,  6.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.6.self_attn.v_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.46 GiB is allocated by PyTorch, and 432.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.6.self_attn.o_proj: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.35 GiB is allocated by PyTorch, and 546.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.6.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.49 GiB is allocated by PyTorch, and 397.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  27%|██▋       | 76/280 [00:11<00:07, 25.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.6.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.49 GiB is allocated by PyTorch, and 397.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.6.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.49 GiB is allocated by PyTorch, and 397.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.6.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.49 GiB is allocated by PyTorch, and 397.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.6.mlp.act_fn: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.49 GiB is allocated by PyTorch, and 397.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.7.self_attn: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 387.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.7.self_attn.q_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 382.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.7.self_attn.k_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 388.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.7.self_attn.v_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 388.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.7.self_attn.o_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 384.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.7.mlp: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 378.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.7.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.49 GiB is allocated by PyTorch, and 399.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.7.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.49 GiB is allocated by PyTorch, and 399.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.7.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.49 GiB is allocated by PyTorch, and 399.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.7.mlp.act_fn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 378.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.8.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 375.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.8.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.8.self_attn.k_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 376.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.8.self_attn.v_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 376.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  52%|█████▏    | 146/280 [00:11<00:00, 171.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.8.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 372.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.8.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 389.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.8.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 389.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.8.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 389.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.8.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.50 GiB is allocated by PyTorch, and 389.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.8.mlp.act_fn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 376.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.9.self_attn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.9.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.9.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.9.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.9.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.9.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.9.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.9.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.9.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.9.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.10.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.10.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.10.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.10.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.10.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.10.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.10.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.10.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.10.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.10.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.11.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.11.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.11.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.11.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.11.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.11.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.11.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.11.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.11.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.11.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.12.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.12.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.12.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.12.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.12.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.12.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.12.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.12.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.12.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.12.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.13.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.13.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.13.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.13.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.13.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.13.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.13.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.13.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.13.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.13.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.14.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.14.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.14.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.14.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.14.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.14.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.14.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.14.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.14.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.14.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.15.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.15.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.15.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.15.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.15.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.15.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.15.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.15.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.15.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.15.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.16.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.16.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.16.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.16.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers:  82%|████████▏ | 230/280 [00:12<00:00, 285.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.16.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.16.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.16.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.16.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.16.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.16.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.17.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.17.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.17.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.17.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.17.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.17.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.17.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.17.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.17.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.17.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.18.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.18.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.18.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.18.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.18.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.18.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.18.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.18.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.18.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.18.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.19.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.19.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.19.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.19.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.19.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.19.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.19.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.19.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.19.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.19.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.20.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.20.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.20.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.20.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.20.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.20.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.20.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.20.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.20.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.20.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.21.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.21.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.21.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.21.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.21.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.21.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.21.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.21.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.21.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.21.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.22.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.22.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.22.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.22.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.22.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.22.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.22.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.22.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.22.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.22.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.23.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.23.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.23.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.23.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.23.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.23.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.23.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.23.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.23.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.23.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.24.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.24.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.24.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.24.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.24.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Profiling Layers: 100%|██████████| 280/280 [00:12<00:00, 22.97it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping model.layers.24.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.24.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.24.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.24.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.24.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.25.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.25.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.25.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.25.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.25.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.25.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.25.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.25.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.25.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.25.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.26.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.26.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.26.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.26.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.26.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.26.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.26.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.26.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.26.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.26.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.27.self_attn: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.27.self_attn.q_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.27.self_attn.k_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.27.self_attn.v_proj: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.27.self_attn.o_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.27.mlp: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.27.mlp.gate_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.27.mlp.up_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.27.mlp.down_proj: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.51 GiB is allocated by PyTorch, and 379.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Skipping model.layers.27.mlp.act_fn: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 1.38 MiB is free. Process 27680 has 22.15 GiB memory in use. Of the allocated memory 21.52 GiB is allocated by PyTorch, and 374.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Targeting Threshold: 0% kept in FP16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Qwen2.5-1.5B-Instruct-GPTQ-0.0</strong> at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/ybn52f6b' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/ybn52f6b</a><br> View project at: <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/575duxne' target=\"_blank\">Qwen2.5-1.5B-Instruct-GPTQ-0.0</a></strong> <br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/575duxne' target=\"_blank\">https://wandb.ai/yq171014-columbia-university/KLD_Quantization_Exp2/runs/575duxne</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating: KLD-GPTQ-0% ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MMLU Eval:  14%|█▍        | 698/5000 [01:28<09:05,  7.88it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2707764829.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     acc, ppl, lat, mem, preds = evaluate_full_suite(\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mmodel_gptq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmlu_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"KLD-GPTQ-{threshold:.0%}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     )\n",
            "\u001b[0;32m/tmp/ipython-input-3849498488.py\u001b[0m in \u001b[0;36mevaluate_full_suite\u001b[0;34m(model, tokenizer, dataset, metric_name)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# 1. Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mmlu_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVAL_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4158930788.py\u001b[0m in \u001b[0;36mget_mmlu_predictions\u001b[0;34m(model, dataset, num_samples)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_mmlu_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoice_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 449\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                         \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;31m# Restore original forward methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Fully Connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# GPTQ\n",
        "\n",
        "print(f\"\\n--- Starting Experiment: GPTQ ({CURRENT_MODEL_ID}) ---\")\n",
        "\n",
        "print(\"Running GPTQ Optimization...\")\n",
        "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "calib_data_obj = Dataset.from_dict({\"text\": [text for text in ds[\"text\"] if len(text) > 0][:128]})\n",
        "\n",
        "recipe = [\n",
        "    GPTQModifier(\n",
        "        targets=\"Linear\",\n",
        "        scheme=\"W4A16\",\n",
        "        ignore=[\"lm_head\"],\n",
        "        dampening_frac=0.01\n",
        "    )\n",
        "]\n",
        "\n",
        "oneshot(\n",
        "    model=CURRENT_MODEL_ID,\n",
        "    dataset=calib_data_obj,\n",
        "    recipe=recipe,\n",
        "    output_dir=\"./gptq_temp\",\n",
        "    num_calibration_samples=128,\n",
        "    max_seq_length=512,\n",
        "    save_compressed=True\n",
        ")\n",
        "\n",
        "model_gptq = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./gptq_temp\", device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(\"Profiling GPTQ Sensitivity...\")\n",
        "gptq_sensitivity = profile_restoration_sensitivity(\n",
        "    model_q=model_gptq,\n",
        "    model_ref=model_fp16,\n",
        "    calib_input=calib_data,\n",
        "    granularity= 'layer'\n",
        ")\n",
        "\n",
        "sorted_gptq = sorted(gptq_sensitivity.items(), key=lambda x: x[1], reverse=True)\n",
        "all_layer_names = [n for n, s in sorted_gptq]\n",
        "\n",
        "sorted_thresholds = sorted(SENSITIVITY_THRESHOLDS)\n",
        "current_restored_count = 0\n",
        "\n",
        "for threshold in sorted_thresholds:\n",
        "    print(f\"\\nTargeting Threshold: {threshold:.0%} kept in FP16\")\n",
        "\n",
        "    target_count = int(len(all_layer_names) * threshold)\n",
        "    layers_to_fix_now = all_layer_names[current_restored_count : target_count]\n",
        "\n",
        "    if layers_to_fix_now:\n",
        "        print(f\"Restoring {len(layers_to_fix_now)} additional layers...\")\n",
        "        perform_surgery(model_gptq, layers_to_fix_now, model_fp16)\n",
        "        current_restored_count = target_count\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=WANDB_PROJECT_NAME,\n",
        "        name=f\"{CURRENT_MODEL_ID.split('/')[-1]}-GPTQ-{threshold}\",\n",
        "        config={\"model\": CURRENT_MODEL_ID, \"method\": \"KLD-GPTQ\", \"threshold\": threshold},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    acc, ppl, lat, mem, preds = evaluate_full_suite(\n",
        "        model_gptq, tokenizer, mmlu_dataset, f\"KLD-GPTQ-{threshold:.0%}\"\n",
        "    )\n",
        "\n",
        "    flip = calculate_flip_rate(base_preds, preds)\n",
        "\n",
        "    wandb.log({\n",
        "        \"Accuracy\": acc, \"Perplexity\": ppl, \"Latency\": lat, \"Static_Memory\": static_mem,\n",
        "        \"Peak_Memory\": peak_mem, \"Flip_Rate\": flip, \"Threshold\": threshold\n",
        "    })\n",
        "\n",
        "    results_table.append({\n",
        "        \"Model\": CURRENT_MODEL_ID,\n",
        "        \"Method\": \"KLD-GPTQ\",\n",
        "        \"Threshold\": threshold,\n",
        "        \"Acc\": acc,\n",
        "        \"Flip\": flip,\n",
        "        \"PPL\": ppl,\n",
        "        \"Latency\": lat,\n",
        "        \"Static Mem\": static_mem,\n",
        "        \"Peak Mem\": peak_mem\n",
        "    })\n",
        "    run.finish()\n",
        "\n",
        "shutil.rmtree(\"./gptq_temp\")\n",
        "del model_gptq\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfmTms15Ee6M"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeswA4JSEe6M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Create DataFrame\n",
        "df = pd.DataFrame(results_table)\n",
        "\n",
        "# Display the data\n",
        "print(df)\n",
        "\n",
        "# 2. Set up the figure with 2 subplots (Static vs Peak)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# --- Plot A: Static Memory (Model Size) vs Flip Rate ---\n",
        "sns.scatterplot(\n",
        "    data=df, x='Static Mem', y='Flip', hue='Method', style='Method',\n",
        "    s=200, palette='viridis', ax=axes[0]\n",
        ")\n",
        "axes[0].set_title(\"Compression Efficiency: Static Memory vs Flip Rate\")\n",
        "axes[0].set_xlabel(\"Static Memory (GB) - [Weights Only]\")\n",
        "axes[0].set_ylabel(\"Flip Rate (Lower is Better)\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add labels for Plot A\n",
        "for i in range(df.shape[0]):\n",
        "    row = df.iloc[i]\n",
        "    # Check if 'Static_Mem' exists to avoid errors\n",
        "    if 'Static_Mem' in row:\n",
        "        axes[0].text(row.Static_Mem + 0.01, row.Flip + 0.001, f\"{row.Threshold:.0%}\", fontsize=9)\n",
        "\n",
        "# --- Plot B: Peak Memory (Runtime Cost) vs Flip Rate ---\n",
        "sns.scatterplot(\n",
        "    data=df, x='Peak Mem', y='Flip', hue='Method', style='Method',\n",
        "    s=200, palette='magma', ax=axes[1]\n",
        ")\n",
        "axes[1].set_title(\"Runtime Efficiency: Peak Memory vs Flip Rate\")\n",
        "axes[1].set_xlabel(\"Peak Memory (GB) - [Weights + Activations + Overhead]\")\n",
        "axes[1].set_ylabel(\"Flip Rate (Lower is Better)\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Add labels for Plot B\n",
        "for i in range(df.shape[0]):\n",
        "    row = df.iloc[i]\n",
        "    if 'Peak_Mem' in row:\n",
        "        axes[1].text(row.Peak_Mem + 0.05, row.Flip + 0.001, f\"{row.Threshold:.0%}\", fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"efficiency_frontier_comparison_Experiment2.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44g9_WijEe6M"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure DataFrame is ready\n",
        "df = pd.DataFrame(results_table)\n",
        "\n",
        "# Set up the figure with 3 subplots side-by-side\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
        "\n",
        "# --- Plot 1: Perplexity (Lower is Better) ---\n",
        "sns.lineplot(\n",
        "    data=df, x='Threshold', y='PPL', hue='Method', style='Method',\n",
        "    markers=True, markersize=10, linewidth=2.5, ax=axes[0], palette='viridis'\n",
        ")\n",
        "axes[0].set_title(\"Perplexity vs Restoration (Lower is Better)\")\n",
        "axes[0].set_xlabel(\"% Layers Restored to BF16\")\n",
        "axes[0].set_ylabel(\"Perplexity\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].invert_yaxis() # Optional: If you want 'better' (lower) to be higher up, remove if standard view preferred.\n",
        "\n",
        "# --- Plot 2: Accuracy (Higher is Better) ---\n",
        "sns.lineplot(\n",
        "    data=df, x='Threshold', y='Acc', hue='Method', style='Method',\n",
        "    markers=True, markersize=10, linewidth=2.5, ax=axes[1], palette='magma'\n",
        ")\n",
        "axes[1].set_title(\"MMLU Accuracy vs Restoration (Higher is Better)\")\n",
        "axes[1].set_xlabel(\"% Layers Restored to BF16\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# --- Plot 3: Latency (Lower is Better) ---\n",
        "sns.lineplot(\n",
        "    data=df, x='Threshold', y='Latency', hue='Method', style='Method',\n",
        "    markers=True, markersize=10, linewidth=2.5, ax=axes[2], palette='coolwarm'\n",
        ")\n",
        "axes[2].set_title(\"Inference Latency vs Restoration\")\n",
        "axes[2].set_xlabel(\"% Layers Restored to BF16\")\n",
        "axes[2].set_ylabel(\"Latency (Seconds)\")\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"metrics_comparison_Experiment2.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnF7og1QEe6M"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHQ_woJSEe6M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil # Import shutil for copying files\n",
        "\n",
        "# Define the directory to save files in Google Drive\n",
        "save_dir = '/content/drive/MyDrive/Columbia-LLMSeminar/SLLM project/Jiayi/exp2'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save results_table as CSV\n",
        "df_results = pd.DataFrame(results_table)\n",
        "df_results.to_csv(os.path.join(save_dir, 'exp2_results.csv'), index=False)\n",
        "print(f\"Results table saved to {os.path.join(save_dir, 'exp2_results.csv')}\") # Corrected print statement\n",
        "\n",
        "# Copy figures to Google Drive\n",
        "figures_to_copy = [\n",
        "    'efficiency_frontier_comparison_Experiment2.png',\n",
        "    'metrics_comparison_Experiment2.png'\n",
        "]\n",
        "\n",
        "for fig_name in figures_to_copy:\n",
        "    fig_path = os.path.join('/content', fig_name)\n",
        "    if os.path.exists(fig_path):\n",
        "        # Use shutil.copy2 to copy the file, then os.remove to delete the original\n",
        "        shutil.copy2(fig_path, os.path.join(save_dir, fig_name))\n",
        "        # os.remove(fig_path) # Remove original after copying\n",
        "        print(f\"Copied {fig_name} to {os.path.join(save_dir, fig_name)} and removed original.\")\n",
        "    else:\n",
        "        print(f\"Figure {fig_name} not found in current directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz51AE_4kNIt"
      },
      "source": [
        "# Experiment 3: Model Size\n",
        "Research Question: Does this technique generalize to larger models? (Larger models are usually more robust to quantization; do they need less restoration?)\n",
        "\n",
        "Fixed Variables:\n",
        "\n",
        "Method: The \"Winner\" from Exp 2 (likely NF4 for speed or AWQ for accuracy).\n",
        "\n",
        "Threshold: Fix to the \"sweet spot\" (e.g., 5%).\n",
        "\n",
        "Independent Variable (Change this):\n",
        "\n",
        "Model Size: 1.5B vs. 7B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rMVJqUJkjib"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3588dccaff12499f9accd29558988651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00aa65e3489e4543bc38ceccbb19a120",
              "IPY_MODEL_44bd6f0ff8944824bcf2b540bad6d40a",
              "IPY_MODEL_d65a08b0a057495db6532fdb122f8558"
            ],
            "layout": "IPY_MODEL_fd41eab1035941658cbe38a1d93d8ffb"
          }
        },
        "00aa65e3489e4543bc38ceccbb19a120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53445d7cc2404c398a4bf12d07323e99",
            "placeholder": "​",
            "style": "IPY_MODEL_8c295fcbe6004a8eb0a47e7eb840a7fd",
            "value": "README.md: "
          }
        },
        "44bd6f0ff8944824bcf2b540bad6d40a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95560e5e95a84a81b62cca7baea22016",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d76c8a693204c94a7971c121e2cc6e6",
            "value": 1
          }
        },
        "d65a08b0a057495db6532fdb122f8558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7e47e2d13e342be937e88185e7cffdc",
            "placeholder": "​",
            "style": "IPY_MODEL_d2145c5aab8146f394c6c9a6f49c6979",
            "value": " 53.2k/? [00:00&lt;00:00, 4.99MB/s]"
          }
        },
        "fd41eab1035941658cbe38a1d93d8ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53445d7cc2404c398a4bf12d07323e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c295fcbe6004a8eb0a47e7eb840a7fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95560e5e95a84a81b62cca7baea22016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9d76c8a693204c94a7971c121e2cc6e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7e47e2d13e342be937e88185e7cffdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2145c5aab8146f394c6c9a6f49c6979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "847ccad130d8410d84b3e34b384073fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94d7feb31bbb4b9eb17a4c540f17eded",
              "IPY_MODEL_7a396006415c4e4888dceca9f3c9bbc1",
              "IPY_MODEL_1f3c1068a8984fae96830b56ef135402"
            ],
            "layout": "IPY_MODEL_b92e52a7d73a4cc9aea69e193dd70a47"
          }
        },
        "94d7feb31bbb4b9eb17a4c540f17eded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c15fc046ce84687ac3ecc1439a8f481",
            "placeholder": "​",
            "style": "IPY_MODEL_73452d30f9344d1cbeddf0af2c2e0af9",
            "value": "dataset_infos.json: "
          }
        },
        "7a396006415c4e4888dceca9f3c9bbc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f31c227f53e45e2880b98612fd316b2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1643f11fcdfd4b6599471dcac17ef9b2",
            "value": 1
          }
        },
        "1f3c1068a8984fae96830b56ef135402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e12fc680353442ea32c8b7b5b2246b5",
            "placeholder": "​",
            "style": "IPY_MODEL_8dea8d8377d84238b3a7a4e586ed97ae",
            "value": " 138k/? [00:00&lt;00:00, 16.5MB/s]"
          }
        },
        "b92e52a7d73a4cc9aea69e193dd70a47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c15fc046ce84687ac3ecc1439a8f481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73452d30f9344d1cbeddf0af2c2e0af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f31c227f53e45e2880b98612fd316b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1643f11fcdfd4b6599471dcac17ef9b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e12fc680353442ea32c8b7b5b2246b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dea8d8377d84238b3a7a4e586ed97ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7b11c7b6a1942f3b4db4f903558660c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26e065d543cc488bb88fc9763ca860bf",
              "IPY_MODEL_01ff2726e01043eb9201eb23f9e7ed72",
              "IPY_MODEL_4aefba7e24f64481811418ce856421e7"
            ],
            "layout": "IPY_MODEL_a58d76b94a594bed93a890d6bf65fcd0"
          }
        },
        "26e065d543cc488bb88fc9763ca860bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_006448e80e93483b80c207813902394f",
            "placeholder": "​",
            "style": "IPY_MODEL_dd7edd98ec5e42eaa7e26a528a1464e8",
            "value": "all/test-00000-of-00001.parquet: 100%"
          }
        },
        "01ff2726e01043eb9201eb23f9e7ed72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0493ea958a5457381a4bcd2ae22d0be",
            "max": 3504718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6395c8bb0124190893a1aaf288e39dc",
            "value": 3504718
          }
        },
        "4aefba7e24f64481811418ce856421e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dec0709331a42d0af0a02c88ace0ca5",
            "placeholder": "​",
            "style": "IPY_MODEL_5b9b6361a425463ba0640d71b0250f87",
            "value": " 3.50M/3.50M [00:02&lt;00:00, 1.69MB/s]"
          }
        },
        "a58d76b94a594bed93a890d6bf65fcd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "006448e80e93483b80c207813902394f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd7edd98ec5e42eaa7e26a528a1464e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0493ea958a5457381a4bcd2ae22d0be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6395c8bb0124190893a1aaf288e39dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2dec0709331a42d0af0a02c88ace0ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9b6361a425463ba0640d71b0250f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76515ba16fba457f8e0a126eaa0f3b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cba0e8ded37041e68263c64383501ca7",
              "IPY_MODEL_51a9c217860143218f39acb9f005b479",
              "IPY_MODEL_70f049c6ce0f4d39b61f6a5e37cd8a0a"
            ],
            "layout": "IPY_MODEL_6670b0fc66724c5f8652e24354d07dc7"
          }
        },
        "cba0e8ded37041e68263c64383501ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_641ee0f19d2948229aacf3bf772a674f",
            "placeholder": "​",
            "style": "IPY_MODEL_6ffc00fcfaef4f61a9830298943ccb5d",
            "value": "all/validation-00000-of-00001.parquet: 100%"
          }
        },
        "51a9c217860143218f39acb9f005b479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6d18c9df3044428b86c9f16c58ee594",
            "max": 408449,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0280fa1c57745b295843aaeb03f4903",
            "value": 408449
          }
        },
        "70f049c6ce0f4d39b61f6a5e37cd8a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0ea0c038bc947268a7eadfb66a92f30",
            "placeholder": "​",
            "style": "IPY_MODEL_7f959112c11e4ac8b6fbc6cb07ea9801",
            "value": " 408k/408k [00:00&lt;00:00, 604kB/s]"
          }
        },
        "6670b0fc66724c5f8652e24354d07dc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641ee0f19d2948229aacf3bf772a674f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ffc00fcfaef4f61a9830298943ccb5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6d18c9df3044428b86c9f16c58ee594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0280fa1c57745b295843aaeb03f4903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0ea0c038bc947268a7eadfb66a92f30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f959112c11e4ac8b6fbc6cb07ea9801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9714f2d2810a4cb79f50d5af275d3684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2aca07491ea41e8aacc0fea920481a8",
              "IPY_MODEL_a44412b289c1457dab414bb3a850d9c5",
              "IPY_MODEL_3250d93b5a0b4e1ca8558f14ac31c94e"
            ],
            "layout": "IPY_MODEL_41b5088376ee4a12b30033f4eb8709e8"
          }
        },
        "c2aca07491ea41e8aacc0fea920481a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c42841ebe3d04c14821ec9faa55cc1e4",
            "placeholder": "​",
            "style": "IPY_MODEL_8de8ff12622a41b6a3f405e3cd2dcfa3",
            "value": "all/dev-00000-of-00001.parquet: 100%"
          }
        },
        "a44412b289c1457dab414bb3a850d9c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98821b801a48446fb5b5643aa360cbf6",
            "max": 76504,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_135187f590e448338232093f89c9bb77",
            "value": 76504
          }
        },
        "3250d93b5a0b4e1ca8558f14ac31c94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffe4d3dc541d4de4a2510fa06f4a43c2",
            "placeholder": "​",
            "style": "IPY_MODEL_de3812f92a794cc492e11deb90d5f2d7",
            "value": " 76.5k/76.5k [00:00&lt;00:00, 143kB/s]"
          }
        },
        "41b5088376ee4a12b30033f4eb8709e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c42841ebe3d04c14821ec9faa55cc1e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de8ff12622a41b6a3f405e3cd2dcfa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98821b801a48446fb5b5643aa360cbf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "135187f590e448338232093f89c9bb77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ffe4d3dc541d4de4a2510fa06f4a43c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de3812f92a794cc492e11deb90d5f2d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ece7532ed7f4d128f5eb2539fa54b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e67d01510e943b5a8f7c10245a8849b",
              "IPY_MODEL_7aeb4a9728c148328b51c50bf65bd76b",
              "IPY_MODEL_d0a09bb192824c879d9e5938343eabed"
            ],
            "layout": "IPY_MODEL_e1176873b1984b32a27f72209b904b22"
          }
        },
        "4e67d01510e943b5a8f7c10245a8849b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5903dae679244eb9af34d244173f98e3",
            "placeholder": "​",
            "style": "IPY_MODEL_6edc4e5b0d5f4284b8dfcd901658d31c",
            "value": "all/auxiliary_train-00000-of-00001.parqu(…): 100%"
          }
        },
        "7aeb4a9728c148328b51c50bf65bd76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14ab0075c295470e987dbcf116d5b998",
            "max": 47513731,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25fda25d3fe747dc92042bbd1f6b6c88",
            "value": 47513731
          }
        },
        "d0a09bb192824c879d9e5938343eabed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25257db6167d42abb5d3f5d367ba2618",
            "placeholder": "​",
            "style": "IPY_MODEL_efff6bcfe8df4031a5e852b8a0394654",
            "value": " 47.5M/47.5M [00:02&lt;00:00, 32.4MB/s]"
          }
        },
        "e1176873b1984b32a27f72209b904b22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5903dae679244eb9af34d244173f98e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6edc4e5b0d5f4284b8dfcd901658d31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14ab0075c295470e987dbcf116d5b998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25fda25d3fe747dc92042bbd1f6b6c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25257db6167d42abb5d3f5d367ba2618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efff6bcfe8df4031a5e852b8a0394654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d89d8f8bfdde4778965fd1f195c554fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a984f431adae495384fdbbe89b15a107",
              "IPY_MODEL_b12bc5193d3f4c468f6fa7e3bb840645",
              "IPY_MODEL_1ee75026a7bd4f3e8cddb786818643d3"
            ],
            "layout": "IPY_MODEL_99f408714bc5476ebc2a7b397b0ac1de"
          }
        },
        "a984f431adae495384fdbbe89b15a107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dca6b29409d34c7cbf0b885c4ffb6613",
            "placeholder": "​",
            "style": "IPY_MODEL_b910761c0b4342aaae262039a4b80741",
            "value": "Generating test split: 100%"
          }
        },
        "b12bc5193d3f4c468f6fa7e3bb840645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9e8e4962f6c4d39b89fe8adc2a08058",
            "max": 14042,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5c4917bb23644b5a69e623e6b443b2a",
            "value": 14042
          }
        },
        "1ee75026a7bd4f3e8cddb786818643d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3757ff7f6a04e67832eb5f974c0deb8",
            "placeholder": "​",
            "style": "IPY_MODEL_fada519d304f46e49e60adc9c475d0c0",
            "value": " 14042/14042 [00:00&lt;00:00, 207705.74 examples/s]"
          }
        },
        "99f408714bc5476ebc2a7b397b0ac1de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dca6b29409d34c7cbf0b885c4ffb6613": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b910761c0b4342aaae262039a4b80741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9e8e4962f6c4d39b89fe8adc2a08058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c4917bb23644b5a69e623e6b443b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3757ff7f6a04e67832eb5f974c0deb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fada519d304f46e49e60adc9c475d0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5699cbe0c9774347a35fd3fb1f713103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_654add48f9ef48b19cdcaf47a7c59bfc",
              "IPY_MODEL_6a37175625ba463c959f66a84c47e719",
              "IPY_MODEL_6d97b5dbcaf944bb9fd7c13247281442"
            ],
            "layout": "IPY_MODEL_e4120b9bc798493eb02e612bdb02d2b6"
          }
        },
        "654add48f9ef48b19cdcaf47a7c59bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f47f7264a2e24d489ec9d8646890a6ee",
            "placeholder": "​",
            "style": "IPY_MODEL_286b8838c85647a4b2d2c3b3b3cbc18d",
            "value": "Generating validation split: 100%"
          }
        },
        "6a37175625ba463c959f66a84c47e719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3fea673f2a24c1caa24225c394811bd",
            "max": 1531,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67ad0b10c58d457a92dcb998817ebf28",
            "value": 1531
          }
        },
        "6d97b5dbcaf944bb9fd7c13247281442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b507210153340788762eadb067450ae",
            "placeholder": "​",
            "style": "IPY_MODEL_7173cd25ba7a4b8d8434c7fdb9601a73",
            "value": " 1531/1531 [00:00&lt;00:00, 86638.60 examples/s]"
          }
        },
        "e4120b9bc798493eb02e612bdb02d2b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f47f7264a2e24d489ec9d8646890a6ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "286b8838c85647a4b2d2c3b3b3cbc18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3fea673f2a24c1caa24225c394811bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67ad0b10c58d457a92dcb998817ebf28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b507210153340788762eadb067450ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7173cd25ba7a4b8d8434c7fdb9601a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffd0efd7346c46499af1cf338d3bdb8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca67c89140e14c5aa653697f6fd71ef5",
              "IPY_MODEL_41c3923cf1034a68869b1f68d711db04",
              "IPY_MODEL_bf45c5aa48314e35ac50cc9e6bbb480d"
            ],
            "layout": "IPY_MODEL_b77c5ab5fc664cde97b425f2e7e02d63"
          }
        },
        "ca67c89140e14c5aa653697f6fd71ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6929ba8c75aa49ba962760d133984864",
            "placeholder": "​",
            "style": "IPY_MODEL_8ef46dc58b204dafbc9976b005cebfd4",
            "value": "Generating dev split: 100%"
          }
        },
        "41c3923cf1034a68869b1f68d711db04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f65bcfbf96145bc8af0249a91e813ed",
            "max": 285,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fed5b046d9894c3a9e09ece4897e672c",
            "value": 285
          }
        },
        "bf45c5aa48314e35ac50cc9e6bbb480d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0b43d6100e041848ffc26d1fcb1b55a",
            "placeholder": "​",
            "style": "IPY_MODEL_4f988d7935784d0c9965c0960fa2bd67",
            "value": " 285/285 [00:00&lt;00:00, 25684.93 examples/s]"
          }
        },
        "b77c5ab5fc664cde97b425f2e7e02d63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6929ba8c75aa49ba962760d133984864": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef46dc58b204dafbc9976b005cebfd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f65bcfbf96145bc8af0249a91e813ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fed5b046d9894c3a9e09ece4897e672c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0b43d6100e041848ffc26d1fcb1b55a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f988d7935784d0c9965c0960fa2bd67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0354d74ad83c44378e847422cac80c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3c40fad8863453f88f60e860857680d",
              "IPY_MODEL_11d6634c6fab4ef6a9047ad885723093",
              "IPY_MODEL_549c4cb371e449a2826f524e4c91569e"
            ],
            "layout": "IPY_MODEL_1ef544c799424632b470c4672c0f51d8"
          }
        },
        "d3c40fad8863453f88f60e860857680d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9da25bca5584230a7522970058ecd1d",
            "placeholder": "​",
            "style": "IPY_MODEL_9f23f64b1278428cb6bbb83df86fbbee",
            "value": "Generating auxiliary_train split: 100%"
          }
        },
        "11d6634c6fab4ef6a9047ad885723093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc05609cf5544b0a8212ed2cc5c2a7c6",
            "max": 99842,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_561772cbb97a4d32b67a336f88fa6274",
            "value": 99842
          }
        },
        "549c4cb371e449a2826f524e4c91569e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e145f606eb514df095032a7b1d860c91",
            "placeholder": "​",
            "style": "IPY_MODEL_0711db7c9e264c05a4f2be9fc09e0b7d",
            "value": " 99842/99842 [00:00&lt;00:00, 284457.52 examples/s]"
          }
        },
        "1ef544c799424632b470c4672c0f51d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9da25bca5584230a7522970058ecd1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f23f64b1278428cb6bbb83df86fbbee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc05609cf5544b0a8212ed2cc5c2a7c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "561772cbb97a4d32b67a336f88fa6274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e145f606eb514df095032a7b1d860c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0711db7c9e264c05a4f2be9fc09e0b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66ff7684a0604f1fa4cbd2fbdb1c1fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3830730cece64a4aae13da58882d1033",
              "IPY_MODEL_f38c79d3e3b44b70a9e1fff80df774a3",
              "IPY_MODEL_1fb6151e1f6848688b04d05673b0615a"
            ],
            "layout": "IPY_MODEL_934a027ce9984a8993f7189efe0405d2"
          }
        },
        "3830730cece64a4aae13da58882d1033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e0a0636663c498aa2f4938df35588da",
            "placeholder": "​",
            "style": "IPY_MODEL_c381b3a5d23845bca830c2146eb54b23",
            "value": "tokenizer_config.json: "
          }
        },
        "f38c79d3e3b44b70a9e1fff80df774a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5436120953874d38a048a1135a1dac1a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c7112c4f4d544288bbe06368950aef4",
            "value": 1
          }
        },
        "1fb6151e1f6848688b04d05673b0615a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0684658ec3cc4fc9a83b20ca3a7b3221",
            "placeholder": "​",
            "style": "IPY_MODEL_2c8f430702d144f29bdd00a7c5e662c1",
            "value": " 7.30k/? [00:00&lt;00:00, 874kB/s]"
          }
        },
        "934a027ce9984a8993f7189efe0405d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e0a0636663c498aa2f4938df35588da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c381b3a5d23845bca830c2146eb54b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5436120953874d38a048a1135a1dac1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0c7112c4f4d544288bbe06368950aef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0684658ec3cc4fc9a83b20ca3a7b3221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c8f430702d144f29bdd00a7c5e662c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "141ff3140cfa471592dc1db8824e1304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_793c889db67c4aeeb479f2121a1ce74e",
              "IPY_MODEL_dd8edf7cd2974753a1b32c9b5133e5f3",
              "IPY_MODEL_890972ea3e724d95b9c2295b2d5d19ae"
            ],
            "layout": "IPY_MODEL_75072fbff1554ca2a88c99af77e1040b"
          }
        },
        "793c889db67c4aeeb479f2121a1ce74e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c12cfeab568a4b7a8e1af92deb88a194",
            "placeholder": "​",
            "style": "IPY_MODEL_219ace21263f47e9bc39b535f73f7e1f",
            "value": "vocab.json: "
          }
        },
        "dd8edf7cd2974753a1b32c9b5133e5f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa585e10a2c042ed9fd1fd85ed1aabb4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1212ea5f4fa944759be13331cdd92467",
            "value": 1
          }
        },
        "890972ea3e724d95b9c2295b2d5d19ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de0333593ce14530a80176b20be2e293",
            "placeholder": "​",
            "style": "IPY_MODEL_40fb88f57b8a40e4ac5be1f8ccecde7c",
            "value": " 2.78M/? [00:00&lt;00:00, 96.2MB/s]"
          }
        },
        "75072fbff1554ca2a88c99af77e1040b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c12cfeab568a4b7a8e1af92deb88a194": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "219ace21263f47e9bc39b535f73f7e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa585e10a2c042ed9fd1fd85ed1aabb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1212ea5f4fa944759be13331cdd92467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de0333593ce14530a80176b20be2e293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40fb88f57b8a40e4ac5be1f8ccecde7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea6dc92fb5cb4fe2ae100324c6cc8093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4898310262254dd4aaf1d9559ec057e7",
              "IPY_MODEL_895d747a24b441a698ee8a19bbfc7572",
              "IPY_MODEL_fa7700409c8f409bbbb4c1caf186732d"
            ],
            "layout": "IPY_MODEL_77512a5ff2b34d00a183173134219e53"
          }
        },
        "4898310262254dd4aaf1d9559ec057e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca6c5eca30be49fe9dbe152fd93ce263",
            "placeholder": "​",
            "style": "IPY_MODEL_dadb44b2d43740b48cfd7a94874599b4",
            "value": "merges.txt: "
          }
        },
        "895d747a24b441a698ee8a19bbfc7572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b87d41d904ca4674844147f881d34c68",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f315d1c3273475e878e1c607dab7e70",
            "value": 1
          }
        },
        "fa7700409c8f409bbbb4c1caf186732d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_536b5088191b47169270c5cf0b6860e0",
            "placeholder": "​",
            "style": "IPY_MODEL_ff2df24cb1834071880a52866704f2e9",
            "value": " 1.67M/? [00:00&lt;00:00, 79.0MB/s]"
          }
        },
        "77512a5ff2b34d00a183173134219e53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6c5eca30be49fe9dbe152fd93ce263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dadb44b2d43740b48cfd7a94874599b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b87d41d904ca4674844147f881d34c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7f315d1c3273475e878e1c607dab7e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "536b5088191b47169270c5cf0b6860e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff2df24cb1834071880a52866704f2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "130e4fa08b2f46c8a9f5cb22bea9b1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_482430794cad480ba586b01f21d08434",
              "IPY_MODEL_507fc21949ea4f1d80e6d4eb681a0456",
              "IPY_MODEL_7a56a1f84974474880617f87e53984bd"
            ],
            "layout": "IPY_MODEL_d671d6cdd3624010ad428ce18459c299"
          }
        },
        "482430794cad480ba586b01f21d08434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f465a94d9bc446bd81247c8016eb21a7",
            "placeholder": "​",
            "style": "IPY_MODEL_dd7809aa4f3043f8a5ba37c41e657adc",
            "value": "tokenizer.json: "
          }
        },
        "507fc21949ea4f1d80e6d4eb681a0456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ff8b142e11c4b9eb25e27765c4499f2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b505c9d27cb496c942d8a8c73b5c6a1",
            "value": 1
          }
        },
        "7a56a1f84974474880617f87e53984bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a7d6fff1134e69a2ee0d24cb1f7775",
            "placeholder": "​",
            "style": "IPY_MODEL_bc2c0b7f8bb84f45a1fc4b2f87954cbb",
            "value": " 7.03M/? [00:00&lt;00:00, 150MB/s]"
          }
        },
        "d671d6cdd3624010ad428ce18459c299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f465a94d9bc446bd81247c8016eb21a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd7809aa4f3043f8a5ba37c41e657adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ff8b142e11c4b9eb25e27765c4499f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7b505c9d27cb496c942d8a8c73b5c6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6a7d6fff1134e69a2ee0d24cb1f7775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc2c0b7f8bb84f45a1fc4b2f87954cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1ff7f06f96149c1bb4c7c482eaa67f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19f809a8e80946e7b3a77213911d415f",
              "IPY_MODEL_5b761482a88043e9b294021bffa2ff7c",
              "IPY_MODEL_121cd6254189426a9d66f70046a30a9d"
            ],
            "layout": "IPY_MODEL_7d55e2e0600d49a78c84a1871f54b0d6"
          }
        },
        "19f809a8e80946e7b3a77213911d415f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1531ea4c74b4f1a8811081070b49246",
            "placeholder": "​",
            "style": "IPY_MODEL_82c533e2d25c42ce9d09946f52af6730",
            "value": "config.json: 100%"
          }
        },
        "5b761482a88043e9b294021bffa2ff7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc7988115c2b40ef92f66b1e4830b5c4",
            "max": 660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e108f27569541218b6e9dbf7f85a72e",
            "value": 660
          }
        },
        "121cd6254189426a9d66f70046a30a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d58db0f4f7df4641b3c739641c887a5b",
            "placeholder": "​",
            "style": "IPY_MODEL_1e9421cf27b345c98960cd6e425464c5",
            "value": " 660/660 [00:00&lt;00:00, 92.7kB/s]"
          }
        },
        "7d55e2e0600d49a78c84a1871f54b0d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1531ea4c74b4f1a8811081070b49246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82c533e2d25c42ce9d09946f52af6730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc7988115c2b40ef92f66b1e4830b5c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e108f27569541218b6e9dbf7f85a72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d58db0f4f7df4641b3c739641c887a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e9421cf27b345c98960cd6e425464c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbfbdcb563b84767a1681f645c3662de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd7db80335ae412b8706035186c01cc1",
              "IPY_MODEL_7ff04cf8ed32424c86df43998e24588c",
              "IPY_MODEL_9dc1e23ab14246719130238ec88c49d7"
            ],
            "layout": "IPY_MODEL_eaa45fbbdf1149459820c208916c379d"
          }
        },
        "cd7db80335ae412b8706035186c01cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fadc10d1a37423e9150787310439576",
            "placeholder": "​",
            "style": "IPY_MODEL_2d32869a0faf4fc29b0a9338a8a4cb1c",
            "value": "model.safetensors: 100%"
          }
        },
        "7ff04cf8ed32424c86df43998e24588c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_616726e4321e49d39a9da3d2d6a4d366",
            "max": 3087467144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17fcc80397924066b243da14c79c294f",
            "value": 3087467144
          }
        },
        "9dc1e23ab14246719130238ec88c49d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f5aed3078bb4eb3b1691c7e6226f174",
            "placeholder": "​",
            "style": "IPY_MODEL_33c0bc95109b4b3193a36755da254faf",
            "value": " 3.09G/3.09G [00:09&lt;00:00, 508MB/s]"
          }
        },
        "eaa45fbbdf1149459820c208916c379d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fadc10d1a37423e9150787310439576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d32869a0faf4fc29b0a9338a8a4cb1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "616726e4321e49d39a9da3d2d6a4d366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17fcc80397924066b243da14c79c294f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f5aed3078bb4eb3b1691c7e6226f174": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33c0bc95109b4b3193a36755da254faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfc5391d05e04022ba844d7c644bf706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d6939b4136d43e791f0a82e30384340",
              "IPY_MODEL_c37bd619574f47ae991e5d96e58d74d8",
              "IPY_MODEL_0f024d83d2fc4fa292486f94e6280508"
            ],
            "layout": "IPY_MODEL_2822a85234334819894f20a69531d02a"
          }
        },
        "5d6939b4136d43e791f0a82e30384340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f7e789d13584e8fbfdf2f30057fa16c",
            "placeholder": "​",
            "style": "IPY_MODEL_fcab44a684ce484eb1a45547869474ee",
            "value": "generation_config.json: 100%"
          }
        },
        "c37bd619574f47ae991e5d96e58d74d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58cf8a2ceb234cde88076bc95aafe992",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd3f4c660af24cd1966af41be9cc8aa3",
            "value": 242
          }
        },
        "0f024d83d2fc4fa292486f94e6280508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56f4325a81fb4e51976d75d45b9260b0",
            "placeholder": "​",
            "style": "IPY_MODEL_e8dde89828864054ae22ac6b9473fb47",
            "value": " 242/242 [00:00&lt;00:00, 31.2kB/s]"
          }
        },
        "2822a85234334819894f20a69531d02a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f7e789d13584e8fbfdf2f30057fa16c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcab44a684ce484eb1a45547869474ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58cf8a2ceb234cde88076bc95aafe992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd3f4c660af24cd1966af41be9cc8aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56f4325a81fb4e51976d75d45b9260b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8dde89828864054ae22ac6b9473fb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc8cbe47100541df8eee57fe5a0bf108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b995f525b51d4c7f80475b696c4ffefe",
              "IPY_MODEL_55f8120035284f1ba16d021b70698492",
              "IPY_MODEL_74873564dccc4c5eb0b7d0e5a795efb4"
            ],
            "layout": "IPY_MODEL_d7585352ed244a67bb07e4ba09b86ebf"
          }
        },
        "b995f525b51d4c7f80475b696c4ffefe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_066491dc2cc94315917d8d01ca286083",
            "placeholder": "​",
            "style": "IPY_MODEL_f0f15bc3e1a9498cafab3abd71709f05",
            "value": "README.md: "
          }
        },
        "55f8120035284f1ba16d021b70698492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a05391677fba42179c572d85943bbdf3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4705798486a6408c9b7fab81a46824d5",
            "value": 1
          }
        },
        "74873564dccc4c5eb0b7d0e5a795efb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f67179d544b04b4d9fe88d1d31778505",
            "placeholder": "​",
            "style": "IPY_MODEL_c1e39377afb946fc80cdf6eff0c17eeb",
            "value": " 10.5k/? [00:00&lt;00:00, 1.26MB/s]"
          }
        },
        "d7585352ed244a67bb07e4ba09b86ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "066491dc2cc94315917d8d01ca286083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0f15bc3e1a9498cafab3abd71709f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a05391677fba42179c572d85943bbdf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4705798486a6408c9b7fab81a46824d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f67179d544b04b4d9fe88d1d31778505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1e39377afb946fc80cdf6eff0c17eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc38ac90b4324affbc465ea319181d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbe121a538c040419d232ff54e8f096c",
              "IPY_MODEL_f20f32a9ef5a447fb55a1a6daa9d1031",
              "IPY_MODEL_21c46f7e973f466ab81f751111f4dbc8"
            ],
            "layout": "IPY_MODEL_073eb9d4b5e04bcda00dd9e5842855b0"
          }
        },
        "bbe121a538c040419d232ff54e8f096c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b286ec64bdb8404ba14e1bae4c1b7c5e",
            "placeholder": "​",
            "style": "IPY_MODEL_9756623c56944bf2ba946fade8da484d",
            "value": "wikitext-2-raw-v1/test-00000-of-00001.pa(…): 100%"
          }
        },
        "f20f32a9ef5a447fb55a1a6daa9d1031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f50915111cb041eb90c566c71e43058f",
            "max": 732610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b35da9d9f2734ef1b78d229f9a2385bc",
            "value": 732610
          }
        },
        "21c46f7e973f466ab81f751111f4dbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26bdb3efa0d74936aeb232aac76657cd",
            "placeholder": "​",
            "style": "IPY_MODEL_da89a40da50a45218c0477f30c1f7035",
            "value": " 733k/733k [00:01&lt;00:00, 35.8kB/s]"
          }
        },
        "073eb9d4b5e04bcda00dd9e5842855b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b286ec64bdb8404ba14e1bae4c1b7c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9756623c56944bf2ba946fade8da484d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f50915111cb041eb90c566c71e43058f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b35da9d9f2734ef1b78d229f9a2385bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26bdb3efa0d74936aeb232aac76657cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da89a40da50a45218c0477f30c1f7035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "645b90aab62b48ae85adda1997dac736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb34ca04c2e341f589ec5ee338d51195",
              "IPY_MODEL_16dda9fef42b472da867ebe813029fc4",
              "IPY_MODEL_20c244eeedf54d3aace06bf9cad2d7f7"
            ],
            "layout": "IPY_MODEL_2302a57407df42809f17b59deb471c85"
          }
        },
        "fb34ca04c2e341f589ec5ee338d51195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a313dfddd490461383351934964c1b16",
            "placeholder": "​",
            "style": "IPY_MODEL_1984bf74f88048cabe7e2742fdf6425b",
            "value": "wikitext-2-raw-v1/train-00000-of-00001.p(…): 100%"
          }
        },
        "16dda9fef42b472da867ebe813029fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a3fd4760854458e8c09266d4e3acf52",
            "max": 6357543,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8189cb3ff91242488a89bdb00755fcc6",
            "value": 6357543
          }
        },
        "20c244eeedf54d3aace06bf9cad2d7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afaacc800e264115bc56b5a851523607",
            "placeholder": "​",
            "style": "IPY_MODEL_0746c3d5d5014b70a987c49a61b23582",
            "value": " 6.36M/6.36M [00:01&lt;00:00, 5.92MB/s]"
          }
        },
        "2302a57407df42809f17b59deb471c85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a313dfddd490461383351934964c1b16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1984bf74f88048cabe7e2742fdf6425b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a3fd4760854458e8c09266d4e3acf52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8189cb3ff91242488a89bdb00755fcc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afaacc800e264115bc56b5a851523607": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0746c3d5d5014b70a987c49a61b23582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b779f52649b488ba3627a41c702642d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce7c6b0a96ed41bfb41e29f1f1882f15",
              "IPY_MODEL_c95346755937449493b0818b3d926b34",
              "IPY_MODEL_32b6851669b540ef85c852afae1b7d8e"
            ],
            "layout": "IPY_MODEL_19eb0046e90142f8983b42fa5196c00c"
          }
        },
        "ce7c6b0a96ed41bfb41e29f1f1882f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_529be9c0c982400884300e9b92724a10",
            "placeholder": "​",
            "style": "IPY_MODEL_6ea116c8d37f4354a8df65250d89bf12",
            "value": "wikitext-2-raw-v1/validation-00000-of-00(…): 100%"
          }
        },
        "c95346755937449493b0818b3d926b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_632605de16f74b69872882f7c28259fb",
            "max": 657209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3644b6e452546b1882283769c451763",
            "value": 657209
          }
        },
        "32b6851669b540ef85c852afae1b7d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fd12f82ff3e4efc9120393ee1c7a19d",
            "placeholder": "​",
            "style": "IPY_MODEL_54c57fd04aa64d2bafcb7b67dd690e6c",
            "value": " 657k/657k [00:00&lt;00:00, 997kB/s]"
          }
        },
        "19eb0046e90142f8983b42fa5196c00c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "529be9c0c982400884300e9b92724a10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ea116c8d37f4354a8df65250d89bf12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "632605de16f74b69872882f7c28259fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3644b6e452546b1882283769c451763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fd12f82ff3e4efc9120393ee1c7a19d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c57fd04aa64d2bafcb7b67dd690e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c52062613e54e7189abea7c3c9d2f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acdc2d54d7de4d96adae347e8bbfc851",
              "IPY_MODEL_00d42b973ca1494098159f1717dc6ed5",
              "IPY_MODEL_d3bdd1226c6c4d638ba8f2ef1bcc46f9"
            ],
            "layout": "IPY_MODEL_21aeb65e4d35455db34dc6e1eb42c74b"
          }
        },
        "acdc2d54d7de4d96adae347e8bbfc851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b25caa1d81743248ee7b1868044392b",
            "placeholder": "​",
            "style": "IPY_MODEL_6e74e11084ad48a9b52f7c56098c3f4e",
            "value": "Generating test split: 100%"
          }
        },
        "00d42b973ca1494098159f1717dc6ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b0763a71eb04b748ad8ed99e32a4b75",
            "max": 4358,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_486671f40af34bdfa250c53566ce48fc",
            "value": 4358
          }
        },
        "d3bdd1226c6c4d638ba8f2ef1bcc46f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_778ff1f35dcf4140968ce341710b170f",
            "placeholder": "​",
            "style": "IPY_MODEL_cfd4fc7cdbdf4de099dcc602365109ad",
            "value": " 4358/4358 [00:00&lt;00:00, 241409.15 examples/s]"
          }
        },
        "21aeb65e4d35455db34dc6e1eb42c74b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b25caa1d81743248ee7b1868044392b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e74e11084ad48a9b52f7c56098c3f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b0763a71eb04b748ad8ed99e32a4b75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "486671f40af34bdfa250c53566ce48fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "778ff1f35dcf4140968ce341710b170f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfd4fc7cdbdf4de099dcc602365109ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30f63adbe5ba442fbbe758d22094bb2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdf028375bc34ddb95b80cb56aa07520",
              "IPY_MODEL_37c7cfb28de544dc8ab16363b0b37976",
              "IPY_MODEL_818fa8d40e7b4509897d9ce989ba64e7"
            ],
            "layout": "IPY_MODEL_cfb7bc375b094b8ba0c172138fd607f8"
          }
        },
        "cdf028375bc34ddb95b80cb56aa07520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a90c497714c4bb8bc13c47ee76987d5",
            "placeholder": "​",
            "style": "IPY_MODEL_c3db19c38695412f82b7dd0f5feb2596",
            "value": "Generating train split: 100%"
          }
        },
        "37c7cfb28de544dc8ab16363b0b37976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bee939af951461d8cfdb760e0ed2bc4",
            "max": 36718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c5e0d78eddc4ee0969c6acc7df6f635",
            "value": 36718
          }
        },
        "818fa8d40e7b4509897d9ce989ba64e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe77fd8b8ae349608628cf0d4e0029dd",
            "placeholder": "​",
            "style": "IPY_MODEL_00bf65ffb82e40a88d8f8ed07a2d8f24",
            "value": " 36718/36718 [00:00&lt;00:00, 728907.32 examples/s]"
          }
        },
        "cfb7bc375b094b8ba0c172138fd607f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a90c497714c4bb8bc13c47ee76987d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3db19c38695412f82b7dd0f5feb2596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bee939af951461d8cfdb760e0ed2bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c5e0d78eddc4ee0969c6acc7df6f635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe77fd8b8ae349608628cf0d4e0029dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00bf65ffb82e40a88d8f8ed07a2d8f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a79c08025a84ec3a46487ec2efa69ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cf86dcf47054064a9cbc284a150e653",
              "IPY_MODEL_d34c30fc1cc24b7e8f33457bc23b9ec8",
              "IPY_MODEL_a4dd1d2701ce43088e54f1452adc7884"
            ],
            "layout": "IPY_MODEL_6ebb32c4f5af4e5d8ad1585aa7c5a25c"
          }
        },
        "6cf86dcf47054064a9cbc284a150e653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77d95b33c9044456a52f8ae9bff185d2",
            "placeholder": "​",
            "style": "IPY_MODEL_9ba0990848df44aa8403a939aec9a67f",
            "value": "Generating validation split: 100%"
          }
        },
        "d34c30fc1cc24b7e8f33457bc23b9ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0773d514af944211a01010361258df45",
            "max": 3760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_083a54d4667a48c2ae3565d0d7f25684",
            "value": 3760
          }
        },
        "a4dd1d2701ce43088e54f1452adc7884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da224730af574cec8c57ac0a68f028e1",
            "placeholder": "​",
            "style": "IPY_MODEL_2f785c6c8f644c99b33c502540d8c3bd",
            "value": " 3760/3760 [00:00&lt;00:00, 247121.97 examples/s]"
          }
        },
        "6ebb32c4f5af4e5d8ad1585aa7c5a25c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d95b33c9044456a52f8ae9bff185d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba0990848df44aa8403a939aec9a67f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0773d514af944211a01010361258df45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "083a54d4667a48c2ae3565d0d7f25684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da224730af574cec8c57ac0a68f028e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f785c6c8f644c99b33c502540d8c3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b576484266d494e95b11bfbc93c85cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8dac8fbc0533416dbe01002bae1996f6",
              "IPY_MODEL_d173082019384afe804e97cb7ebae64a",
              "IPY_MODEL_cf2b88cb612b4971b5b0e9855ceb2d43"
            ],
            "layout": "IPY_MODEL_72a1725891fd44dc8b4b010f32af6ff3"
          }
        },
        "8dac8fbc0533416dbe01002bae1996f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d346fb7689c24b4e8495f0f545557c4c",
            "placeholder": "​",
            "style": "IPY_MODEL_1d9c5685325d43d899395916ee0d08fb",
            "value": "Tokenizing: 100%"
          }
        },
        "d173082019384afe804e97cb7ebae64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e6ec8f9ce524ede8563d2db955f664b",
            "max": 128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c72095992b4f4dbd8d369111fc6a1089",
            "value": 128
          }
        },
        "cf2b88cb612b4971b5b0e9855ceb2d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67fcb770e4554e028c284b88f41f42de",
            "placeholder": "​",
            "style": "IPY_MODEL_f12b9341935d43eb89f039f8742fdbb9",
            "value": " 128/128 [00:00&lt;00:00, 1122.03 examples/s]"
          }
        },
        "72a1725891fd44dc8b4b010f32af6ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d346fb7689c24b4e8495f0f545557c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d9c5685325d43d899395916ee0d08fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e6ec8f9ce524ede8563d2db955f664b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c72095992b4f4dbd8d369111fc6a1089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67fcb770e4554e028c284b88f41f42de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f12b9341935d43eb89f039f8742fdbb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3ed54310d024410b06e96ca947f5670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e022f0566724ce0b1e5816eaf701066",
              "IPY_MODEL_2123bb2539f1405987d1d1f6eb4aa5d2",
              "IPY_MODEL_8c1199ee398e4cd7b43118d664a5300b"
            ],
            "layout": "IPY_MODEL_418ebd063c5a48c2879c5dd98e9dc71b"
          }
        },
        "7e022f0566724ce0b1e5816eaf701066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5364ccd6e4a24beab4cd4170a0e4b650",
            "placeholder": "​",
            "style": "IPY_MODEL_8cc2f2fc66524ae387d8533a3f99bbdd",
            "value": "Tokenizing: 100%"
          }
        },
        "2123bb2539f1405987d1d1f6eb4aa5d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_784748a5ff904dfeac01e5738165d2d9",
            "max": 128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc45ef0877c74d3ea992817d9b74402d",
            "value": 128
          }
        },
        "8c1199ee398e4cd7b43118d664a5300b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d12fdee8f41c4a4685af49ee49f72b5d",
            "placeholder": "​",
            "style": "IPY_MODEL_2fc136c1b08b483b8081dbecaa3dbb3a",
            "value": " 128/128 [00:00&lt;00:00, 1039.72 examples/s]"
          }
        },
        "418ebd063c5a48c2879c5dd98e9dc71b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5364ccd6e4a24beab4cd4170a0e4b650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cc2f2fc66524ae387d8533a3f99bbdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "784748a5ff904dfeac01e5738165d2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc45ef0877c74d3ea992817d9b74402d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d12fdee8f41c4a4685af49ee49f72b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fc136c1b08b483b8081dbecaa3dbb3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}