{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Environment Setup\n",
        "# !pip uninstall -y torch torchvision torchaudio\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip install -U transformers accelerate bitsandbytes datasets scipy matplotlib pandas tqdm wandb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from tqdm.notebook import tqdm\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import gc\n",
        "import wandb\n",
        "import time\n",
        "\n",
        "# Verify Hardware\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "try:\n",
        "    _ = torch.float8_e4m3fn\n",
        "    print(\"✅ Hardware Ready: Native FP8 Supported.\")\n",
        "except AttributeError:\n",
        "    raise RuntimeError(\"❌ Error: FP8 not supported. Use L4 or A100 GPU.\")\n",
        "\n",
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "p-Jginkoi68D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8tJpydOUJBM"
      },
      "outputs": [],
      "source": [
        "# Set for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import set_seed\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "set_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZfycqbNSRtx"
      },
      "source": [
        "## **Configuration & Experiment Controls**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-604U8KmSktc"
      },
      "outputs": [],
      "source": [
        "# --- Experiment Settings ---\n",
        "# Running a sweep across model sizes\n",
        "MODELS_TO_TEST = [\n",
        "    # \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    # \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    # \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    # \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    \"Qwen/Qwen2.5-14B-Instruct\",\n",
        "]\n",
        "SENSITIVITY_THRESHOLDS = [0.0, 0.2]\n",
        "\n",
        "CALIBRATION_SAMPLES = 128\n",
        "EVAL_SAMPLES = 50\n",
        "WANDB_PROJECT_NAME = \"KLD_Modelsize_test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8DT_16slQDF"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import os\n",
        "os.environ[\"WANDB_QUIET\"] = \"true\"\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "if 'results_table' not in globals():\n",
        "    results_table = []\n",
        "\n",
        "print(\"Loading MMLU Dataset...\")\n",
        "try:\n",
        "    mmlu_dataset = concatenate_datasets([\n",
        "        load_dataset(\"cais/mmlu\", \"all\", split='test')\n",
        "    ])\n",
        "    print(f\"MMLU Dataset Loaded. Size: {len(mmlu_dataset)} samples.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading MMLU: {e}\")\n",
        "    from datasets import Dataset\n",
        "    mmlu_dataset = Dataset.from_dict({\n",
        "        \"question\": [\"1+1=?\"], \"choices\": [[\"1\", \"2\", \"3\", \"4\"]], \"answer\": [1]\n",
        "    })\n",
        "\n",
        "print(\"Global setup complete. Ready for Step 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJwJlfxzTLc8"
      },
      "source": [
        "## **Metrics & Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recursive_getattr(obj, attr):\n",
        "    for part in attr.split('.'):\n",
        "        obj = getattr(obj, part)\n",
        "    return obj\n",
        "\n",
        "def recursive_setattr(obj, attr, val):\n",
        "    pre, _, post = attr.rpartition('.')\n",
        "    parent = recursive_getattr(obj, pre) if pre else obj\n",
        "    setattr(parent, post, val)"
      ],
      "metadata": {
        "id": "kblNy4pDduu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtOjgOt3TSXL"
      },
      "outputs": [],
      "source": [
        "# --- Metrics Helpers ---\n",
        "def compute_kld(logits_p, logits_q):\n",
        "    p_probs = F.softmax(logits_p, dim=-1)\n",
        "    q_log_probs = F.log_softmax(logits_q, dim=-1)\n",
        "    return nn.KLDivLoss(reduction='batchmean')(q_log_probs, p_probs).item()\n",
        "\n",
        "def calculate_flip_rate(base_preds, new_preds):\n",
        "    \"\"\"Calculates % of answers that changed from the baseline.\"\"\"\n",
        "    if not base_preds or not new_preds: return 0.0\n",
        "    flips = sum([1 for b, n in zip(base_preds, new_preds) if b != n])\n",
        "    return flips / len(base_preds)\n",
        "\n",
        "def compute_perplexity(model, tokenizer):\n",
        "    \"\"\"Computes perplexity on a subset of WikiText-2\"\"\"\n",
        "    encodings = tokenizer(\"\\n\\n\".join(load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:20]), return_tensors=\"pt\")\n",
        "    max_length = model.config.max_position_embeddings\n",
        "    stride = 512\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "\n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Computing PPL\"):\n",
        "        end_loc = min(begin_loc + max_length, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc\n",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "            nlls.append(outputs.loss)\n",
        "\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len: break\n",
        "\n",
        "    return torch.exp(torch.stack(nlls).mean()).item()\n",
        "\n",
        "def measure_efficiency(model, tokenizer, input_text=\"Hello world\"):\n",
        "    # 1. Cleanup\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # 2. Measure Static Memory (Model Weights Only)\n",
        "    # This shows the pure effect of quantization storage\n",
        "    static_mem_bytes = torch.cuda.memory_allocated()\n",
        "    static_mem_gb = static_mem_bytes / 1024**3\n",
        "\n",
        "    # 3. Run Inference\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model.generate(**input_ids, max_new_tokens=50, min_new_tokens=50)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 4. Measure Peak Memory (Weights + KV Cache + Temp Buffers)\n",
        "    # This shows the \"True Cost\" to run the model\n",
        "    peak_mem_bytes = torch.cuda.max_memory_allocated()\n",
        "    peak_mem_gb = peak_mem_bytes / 1024**3\n",
        "\n",
        "    latency = end_time - start_time\n",
        "\n",
        "    return latency, static_mem_gb, peak_mem_gb\n",
        "\n",
        "def evaluate_full_suite(model, tokenizer, dataset, metric_name):\n",
        "    \"\"\"Runs all metrics and returns them.\"\"\"\n",
        "    print(f\"--- Evaluating: {metric_name} ---\")\n",
        "\n",
        "    # 1. Accuracy\n",
        "    preds, truths = get_mmlu_predictions(model, dataset, EVAL_SAMPLES)\n",
        "    acc = sum([1 for p, g in zip(preds, truths) if p == g]) / len(truths)\n",
        "\n",
        "    # 2. Perplexity\n",
        "    ppl = compute_perplexity(model, tokenizer)\n",
        "\n",
        "    # 3. Efficiency (Unpack 3 values now)\n",
        "    lat, static_mem, peak_mem = measure_efficiency(model, tokenizer)\n",
        "\n",
        "    print(f\"Results -> Acc: {acc:.2%}, PPL: {ppl:.2f}, Latency: {lat:.2f}s, Static Mem: {static_mem:.2f}GB, Peak Mem: {peak_mem:.2f}GB\")\n",
        "\n",
        "    # Return separate memory metrics\n",
        "    return acc, ppl, lat, static_mem, peak_mem, preds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MMLU Logic ---\n",
        "def format_mmlu_prompt(example):\n",
        "    options = [f\"{label}. {example['choices'][i]}\" for i, label in enumerate(['A', 'B', 'C', 'D'])]\n",
        "    prompt_text = f\"Question: {example['question']}\\nOptions:\\n\" + \"\\n\".join(options) + \"\\nAnswer:\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Output only the single letter (A, B, C, or D) corresponding to the correct answer.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_text}\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "def get_mmlu_predictions(model, dataset, num_samples):\n",
        "    predictions, ground_truths = [], []\n",
        "    choices = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    choice_ids = [tokenizer.encode(c)[0] for c in choices]\n",
        "\n",
        "    for i in tqdm(range(min(num_samples, len(dataset))), desc=\"MMLU Eval\"):\n",
        "        ex = dataset[i]\n",
        "        inputs = tokenizer(format_mmlu_prompt(ex), return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits[0, -1, choice_ids]\n",
        "            pred = choices[torch.argmax(logits).item()]\n",
        "        predictions.append(pred)\n",
        "        ground_truths.append(choices[ex['answer']])\n",
        "    return predictions, ground_truths"
      ],
      "metadata": {
        "id": "_0TUVUuad1r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2lOxztdUMfx"
      },
      "source": [
        "## **Advanced Sensitivity Profiling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DZSQNF9UMGr"
      },
      "outputs": [],
      "source": [
        "def profile_restoration_sensitivity(model_q, model_ref, base_logits, calib_input, granularity='layer'):\n",
        "    \"\"\"\n",
        "    Profiles sensitivity by measuring the KLD improvement when restoring\n",
        "    individual parts of the quantized model (model_q) back to FP16 (model_ref).\n",
        "\n",
        "    Returns:\n",
        "        sensitivity_scores: Dict mapping name -> KLD improvement (Higher is more sensitive).\n",
        "    \"\"\"\n",
        "    print(f\"Profiling Restoration Sensitivity (Granularity: {granularity})...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        current_logits = model_q(calib_input.to(device)).logits\n",
        "        initial_kld = compute_kld(base_logits, current_logits)\n",
        "\n",
        "    print(f\"Initial Quantized KLD: {initial_kld:.6f}\")\n",
        "\n",
        "    sensitivity_scores = {}\n",
        "\n",
        "    def get_module_by_name(module, access_string):\n",
        "        names = access_string.split(sep='.')\n",
        "        return reduce(getattr, names, module)\n",
        "\n",
        "    from functools import reduce\n",
        "\n",
        "    # Block-wise or Layer-wise\n",
        "    if granularity == 'block':\n",
        "        if hasattr(model_q, 'model') and hasattr(model_q.model, 'layers'):\n",
        "            iterable_items = list(enumerate(model_q.model.layers))\n",
        "            prefix = \"model.model.layers\"\n",
        "        else:\n",
        "            raise ValueError(\"Could not detect transformer blocks structure.\")\n",
        "        iterator = tqdm(iterable_items, desc=\"Profiling Blocks\")\n",
        "    elif granularity == 'layer':\n",
        "        iterable_items = []\n",
        "        for n, m in model_q.named_modules():\n",
        "            # Check if it is a Linear layer (or 4bit Linear)\n",
        "            # And make sure it's part of the attention or mlp block\n",
        "            if isinstance(m, (nn.Linear, bnb.nn.Linear4bit)) and (\"mlp\" in n or \"self_attn\" in n):\n",
        "                iterable_items.append((n, m))\n",
        "        iterator = tqdm(iterable_items, desc=\"Profiling Layers\")\n",
        "\n",
        "    # Restoration Loop\n",
        "    for name_or_idx, module_q in iterator:\n",
        "        target_name = f\"{prefix}.{name_or_idx}\" if granularity == 'block' else name_or_idx\n",
        "        try:\n",
        "            module_ref = recursive_getattr(model_ref, target_name)\n",
        "            backup_quant_module = recursive_getattr(model_q, target_name)\n",
        "            module_fp16_gpu = copy.deepcopy(module_ref).to(device)\n",
        "            recursive_setattr(model_q, target_name, module_fp16_gpu)\n",
        "\n",
        "            # Measure New KLD\n",
        "            with torch.no_grad():\n",
        "                new_logits = model_q(calib_input.to(device)).logits\n",
        "                new_kld = compute_kld(base_logits, new_logits)\n",
        "\n",
        "            improvement = initial_kld - new_kld\n",
        "            sensitivity_scores[target_name] = improvement\n",
        "            recursive_setattr(model_q, target_name, backup_quant_module)\n",
        "\n",
        "            # Cleanup VRAM\n",
        "            del module_fp16_gpu\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {target_name}: {e}\")\n",
        "\n",
        "    return sensitivity_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3uDgaguUZpM"
      },
      "source": [
        "## **The \"Surgery\" Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_surgery(model, sensitive_names, fp16_model_cpu):\n",
        "    \"\"\"\n",
        "    Replaces the sensitive quantized layers in 'model' (GPU)\n",
        "    with the original FP16 layers from 'fp16_model_cpu' (CPU).\n",
        "\n",
        "    This Generic Version uses deepcopy, so it works for:\n",
        "    - Individual Linear layers (gate_proj, q_proj)\n",
        "    - Entire Blocks (Qwen2MLP, Qwen2Attention)\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    print(f\"Surgery: Replacing {len(sensitive_names)} Sensitive Layers with FP16...\")\n",
        "\n",
        "    for name in sensitive_names:\n",
        "        try:\n",
        "            # 1. Get original FP16 module from CPU backup\n",
        "            #    (This handles Linear, Qwen2MLP, Qwen2Attention, etc.)\n",
        "            original_module = recursive_getattr(fp16_model_cpu, name)\n",
        "\n",
        "            # 2. Create a deep copy and move to GPU\n",
        "            #    We use deepcopy instead of manually instantiating nn.Linear.\n",
        "            #    This preserves the exact class type and configuration.\n",
        "            module_fp16_gpu = copy.deepcopy(original_module).to(model.device)\n",
        "\n",
        "            # 3. Swap into the quantized model\n",
        "            recursive_setattr(model, name, module_fp16_gpu)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping layer {name}: {e}\")\n",
        "\n",
        "    print(f\"Surgery Complete: {count} layers restored.\")"
      ],
      "metadata": {
        "id": "3EBZbOuV2aJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "Hoz6RsHtvsMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment B: FP8 (Native) - Restoration Method\n",
        "\n",
        "# 1. Define FP8 Wrapper (The \"Quantized Layer\")\n",
        "class FP8LinearStorage(nn.Module):\n",
        "    def __init__(self, original_linear):\n",
        "        super().__init__()\n",
        "        self.in_features = original_linear.in_features\n",
        "        self.out_features = original_linear.out_features\n",
        "        # Store compressed FP8 weights\n",
        "        self.weight = nn.Parameter(original_linear.weight.data.to(torch.float8_e4m3fn), requires_grad=False)\n",
        "        # Keep bias in BF16/FP32 for stability\n",
        "        if original_linear.bias is not None:\n",
        "            self.bias = nn.Parameter(original_linear.bias.data, requires_grad=False)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # On-the-fly dequantize to BF16 for computation\n",
        "        return F.linear(x, self.weight.to(x.dtype), self.bias)\n",
        "\n",
        "# 2. Helper: Convert BF16 Model to Global FP8\n",
        "def convert_to_fp8_global(model, safe_layers={\"lm_head\"}):\n",
        "    print(\"Converting model to Global FP8 (Storage)...\")\n",
        "    modules_to_wrap = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if any(s in name for s in safe_layers):\n",
        "                continue\n",
        "            modules_to_wrap.append((name, module))\n",
        "\n",
        "    for name, module in tqdm(modules_to_wrap, desc=\"Wrapping Layers\"):\n",
        "        parent_name = name.rsplit('.', 1)[0] if '.' in name else ''\n",
        "        child_name = name.split('.')[-1]\n",
        "        parent = model.get_submodule(parent_name) if parent_name else model\n",
        "        setattr(parent, child_name, FP8LinearStorage(module))\n",
        "        del module\n",
        "    torch.cuda.empty_cache()\n",
        "    return model\n",
        "\n",
        "# 3. Helper: Profile Restoration Sensitivity (Specialized for FP8 Wrapper)\n",
        "def profile_fp8_restoration_sensitivity(model_q, model_ref, ref_logits, calib_input):\n",
        "    print(\"Profiling FP8 Restoration Sensitivity...\")\n",
        "    sensitivity = {}\n",
        "\n",
        "    # Get Baseline Error (Global FP8 vs Reference)\n",
        "    with torch.no_grad():\n",
        "        base_logits = model_q(calib_input.to(device)).logits\n",
        "        base_kld = compute_kld(ref_logits, base_logits)\n",
        "\n",
        "    # Identify all wrapper layers\n",
        "    fp8_layers = [(n, m) for n, m in model_q.named_modules() if isinstance(m, FP8LinearStorage)]\n",
        "\n",
        "    for name, module in tqdm(fp8_layers, desc=\"Profiling Layers\"):\n",
        "        # A. Swap to BF16 (Restoration)\n",
        "        orig_layer = recursive_getattr(model_ref, name)\n",
        "        # We temporarily replace the wrapper with the real Linear layer\n",
        "        # (Simulating \"What if we fix this layer?\")\n",
        "\n",
        "        # Create temp BF16 layer\n",
        "        temp_linear = nn.Linear(orig_layer.in_features, orig_layer.out_features, bias=(orig_layer.bias is not None)).to(device, dtype=torch.float16)\n",
        "        temp_linear.weight.data = orig_layer.weight.data.to(device)\n",
        "        if orig_layer.bias is not None:\n",
        "            temp_linear.bias.data = orig_layer.bias.data.to(device)\n",
        "\n",
        "        # Swap IN\n",
        "        recursive_setattr(model_q, name, temp_linear)\n",
        "\n",
        "        # B. Measure New KLD\n",
        "        with torch.no_grad():\n",
        "          cur_logits = model_q(calib_input.to(device)).logits\n",
        "          cur_kld = compute_kld(ref_logits, cur_logits)\n",
        "\n",
        "        # C. Score = How much did KLD drop? (Higher drop = More Sensitive)\n",
        "        sensitivity[name] = base_kld - cur_kld\n",
        "\n",
        "        # D. Swap BACK to FP8 Wrapper\n",
        "        recursive_setattr(model_q, name, module)\n",
        "        del temp_linear\n",
        "\n",
        "    return sensitivity"
      ],
      "metadata": {
        "id": "kX44VVD9_91s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import copy\n",
        "import torch\n",
        "\n",
        "# --- Main Experiment Loop ---\n",
        "\n",
        "for model_id in MODELS_TO_TEST:\n",
        "    print(f\"\\n{'='*60}\\nProcessing Model: {model_id}\\n{'='*60}\")\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # PHASE 1: Baseline Setup (FP16)\n",
        "    # ------------------------------------------------------------------\n",
        "    print(f\"[{model_id}] Loading FP16 Baseline...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    # Load Baseline Model\n",
        "    model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Prepare Calibration Data (re-generate per model to ensure tokenization matches)\n",
        "    calib_data = tokenizer(\n",
        "        \"\\n\\n\".join(load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10]),\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    base_logits = model_fp16(calib_data.to(device)).logits.to(device)\n",
        "\n",
        "    # Evaluate Baseline\n",
        "    base_acc, base_ppl, base_lat, base_static_mem, base_peak_mem, base_preds = evaluate_full_suite(\n",
        "        model_fp16, tokenizer, mmlu_dataset, \"FP16 Baseline\"\n",
        "    )\n",
        "\n",
        "    # Log Baseline\n",
        "    wandb.init(project=WANDB_PROJECT_NAME, name=f\"{model_id.split('/')[-1]}-Baseline\", reinit=True)\n",
        "    wandb.log({\n",
        "        \"Accuracy\": base_acc, \"Perplexity\": base_ppl, \"Latency\": base_lat,\n",
        "        \"Static_Memory\": base_static_mem, \"Peak_Memory\": base_peak_mem,\n",
        "        \"Threshold\": 0, \"Flip_Rate\": 0.0, \"Method\": \"Baseline\", \"Model\": model_id\n",
        "    })\n",
        "\n",
        "    results_table.append({\n",
        "        \"Model\": model_id, \"Method\": \"FP16 Baseline\", \"Threshold\": 0,\n",
        "        \"Acc\": base_acc, \"Flip\": 0.0, \"PPL\": base_ppl,\n",
        "        \"Latency\": base_lat, \"Static Mem\": base_static_mem, \"Peak Mem\": base_peak_mem\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    # Offload Baseline to CPU (Keep it for Surgery/Reference)\n",
        "    print(f\"[{model_id}] Moving FP16 Baseline to CPU...\")\n",
        "    model_fp16 = model_fp16.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # PHASE 2: Quantization Experiments (NF4 & FP8)\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    methods = [\"NF4\", \"FP8\"]\n",
        "\n",
        "    for method in methods:\n",
        "        print(f\"\\n--- Starting Method: {method} on {model_id} ---\")\n",
        "\n",
        "        # A. Model Loading / Creation\n",
        "        if method == \"NF4\":\n",
        "            print(f\"[{model_id}] Loading NF4 Model...\")\n",
        "            model_q = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                quantization_config=BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float16\n",
        "                ),\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.float16,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "            # Profile NF4\n",
        "            sensitivity_map = profile_restoration_sensitivity(\n",
        "                model_q=model_q,\n",
        "                model_ref=model_fp16,\n",
        "                base_logits=base_logits, # Passed CPU model, function handles device move\n",
        "                calib_input=calib_data,\n",
        "                granularity='layer'\n",
        "            )\n",
        "\n",
        "        elif method == \"FP8\":\n",
        "            print(f\"[{model_id}] Creating FP8 Model from Baseline...\")\n",
        "            # Deepcopy FP16 (CPU) -> FP8 (GPU)\n",
        "            model_q = copy.deepcopy(model_fp16).to(device)\n",
        "            model_q = convert_to_fp8_global(model_q)\n",
        "\n",
        "            # Profile FP8\n",
        "            sensitivity_map = profile_fp8_restoration_sensitivity(\n",
        "                model_q=model_q,\n",
        "                model_ref=model_fp16,\n",
        "                ref_logits=base_logits,\n",
        "                calib_input=calib_data,\n",
        "            )\n",
        "\n",
        "        # Sort layers by sensitivity\n",
        "        sorted_layers = sorted(sensitivity_map.items(), key=lambda x: x[1], reverse=True)\n",
        "        all_layer_names = [n for n, s in sorted_layers]\n",
        "\n",
        "        # B. Restoration Loop\n",
        "        sorted_thresholds = sorted(SENSITIVITY_THRESHOLDS)\n",
        "        current_restored_count = 0\n",
        "\n",
        "        for threshold in sorted_thresholds:\n",
        "            print(f\"\\n[{model_id} - {method}] Target Threshold: {threshold:.0%}\")\n",
        "\n",
        "            # Identify new layers to restore\n",
        "            target_count = int(len(all_layer_names) * threshold)\n",
        "            layers_to_fix_now = all_layer_names[current_restored_count : target_count]\n",
        "\n",
        "            # Perform Surgery\n",
        "            if layers_to_fix_now:\n",
        "                perform_surgery(model_q, layers_to_fix_now, model_fp16)\n",
        "                current_restored_count = target_count\n",
        "\n",
        "            # Evaluate\n",
        "            run_name = f\"{model_id.split('/')[-1]}-{method}-{threshold}\"\n",
        "            acc, ppl, lat, static_mem, peak_mem, preds = evaluate_full_suite(\n",
        "                model_q, tokenizer, mmlu_dataset, run_name\n",
        "            )\n",
        "\n",
        "            flip = calculate_flip_rate(base_preds, preds)\n",
        "\n",
        "            # Log\n",
        "            wandb.init(\n",
        "                project=WANDB_PROJECT_NAME,\n",
        "                name=run_name,\n",
        "                config={\"model\": model_id, \"threshold\": threshold, \"method\": f\"KLD-{method}\"},\n",
        "                reinit=True\n",
        "            )\n",
        "\n",
        "            wandb.log({\n",
        "                \"Accuracy\": acc, \"Perplexity\": ppl, \"Latency\": lat,\n",
        "                \"Static_Memory\": static_mem, \"Peak_Memory\": peak_mem,\n",
        "                \"Flip_Rate\": flip, \"Threshold\": threshold, \"Method\": f\"KLD-{method}\",\n",
        "                \"Model\": model_id\n",
        "            })\n",
        "\n",
        "            results_table.append({\n",
        "                \"Model\": model_id,\n",
        "                \"Method\": f\"KLD-{method}\",\n",
        "                \"Threshold\": threshold,\n",
        "                \"Acc\": acc, \"Flip\": flip, \"PPL\": ppl,\n",
        "                \"Latency\": lat, \"Static Mem\": static_mem, \"Peak Mem\": peak_mem\n",
        "            })\n",
        "            wandb.finish()\n",
        "\n",
        "        # C. Cleanup Quantized Model\n",
        "        print(f\"[{model_id}] Cleaning up {method} model...\")\n",
        "        del model_q\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # PHASE 3: Cleanup Baseline\n",
        "    # ------------------------------------------------------------------\n",
        "    print(f\"[{model_id}] Finished. Cleaning up Baseline...\")\n",
        "    del model_fp16\n",
        "    del tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nAll Experiments Complete.\")\n"
      ],
      "metadata": {
        "id": "qt30nkPBhyAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization\n"
      ],
      "metadata": {
        "id": "cGkfBI2cvwsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Create DataFrame\n",
        "df = pd.DataFrame(results_table)\n",
        "\n",
        "# Display the data\n",
        "print(df)\n",
        "\n",
        "# 2. Set up the figure with 2 subplots (Static vs Peak)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# --- Plot A: Static Memory (Model Size) vs Flip Rate ---\n",
        "sns.scatterplot(\n",
        "    data=df, x='Static Mem', y='Flip', hue='Method', style='Method',\n",
        "    s=200, palette='viridis', ax=axes[0]\n",
        ")\n",
        "axes[0].set_title(\"Compression Efficiency: Static Memory vs Flip Rate\")\n",
        "axes[0].set_xlabel(\"Static Memory (GB) - [Weights Only]\")\n",
        "axes[0].set_ylabel(\"Flip Rate (Lower is Better)\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add labels for Plot A\n",
        "for i in range(df.shape[0]):\n",
        "    row = df.iloc[i]\n",
        "    # Check if 'Static_Mem' exists to avoid errors\n",
        "    if 'Static_Mem' in row:\n",
        "        axes[0].text(row.Static_Mem + 0.01, row.Flip + 0.001, f\"{row.Threshold:.0%}\", fontsize=9)\n",
        "\n",
        "# --- Plot B: Peak Memory (Runtime Cost) vs Flip Rate ---\n",
        "sns.scatterplot(\n",
        "    data=df, x='Peak Mem', y='Flip', hue='Method', style='Method',\n",
        "    s=200, palette='magma', ax=axes[1]\n",
        ")\n",
        "axes[1].set_title(\"Runtime Efficiency: Peak Memory vs Flip Rate\")\n",
        "axes[1].set_xlabel(\"Peak Memory (GB) - [Weights + Activations + Overhead]\")\n",
        "axes[1].set_ylabel(\"Flip Rate (Lower is Better)\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Add labels for Plot B\n",
        "for i in range(df.shape[0]):\n",
        "    row = df.iloc[i]\n",
        "    if 'Peak_Mem' in row:\n",
        "        axes[1].text(row.Peak_Mem + 0.05, row.Flip + 0.001, f\"{row.Threshold:.0%}\", fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"efficiency_frontier_comparison.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IOzxweilOeL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure DataFrame is ready\n",
        "df = pd.DataFrame(results_table)\n",
        "\n",
        "# Set up the figure with 3 subplots side-by-side\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
        "\n",
        "# --- Plot 1: Perplexity (Lower is Better) ---\n",
        "sns.lineplot(\n",
        "    data=df, x='Threshold', y='PPL', hue='Method', style='Method',\n",
        "    markers=True, markersize=10, linewidth=2.5, ax=axes[0], palette='viridis'\n",
        ")\n",
        "axes[0].set_title(\"Perplexity vs Restoration (Lower is Better)\")\n",
        "axes[0].set_xlabel(\"% Layers Restored to BF16\")\n",
        "axes[0].set_ylabel(\"Perplexity\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].invert_yaxis() # Optional: If you want 'better' (lower) to be higher up, remove if standard view preferred.\n",
        "\n",
        "# --- Plot 2: Accuracy (Higher is Better) ---\n",
        "sns.lineplot(\n",
        "    data=df, x='Threshold', y='Acc', hue='Method', style='Method',\n",
        "    markers=True, markersize=10, linewidth=2.5, ax=axes[1], palette='magma'\n",
        ")\n",
        "axes[1].set_title(\"MMLU Accuracy vs Restoration (Higher is Better)\")\n",
        "axes[1].set_xlabel(\"% Layers Restored to BF16\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# --- Plot 3: Latency (Lower is Better) ---\n",
        "sns.lineplot(\n",
        "    data=df, x='Threshold', y='Latency', hue='Method', style='Method',\n",
        "    markers=True, markersize=10, linewidth=2.5, ax=axes[2], palette='coolwarm'\n",
        ")\n",
        "axes[2].set_title(\"Inference Latency vs Restoration\")\n",
        "axes[2].set_xlabel(\"% Layers Restored to BF16\")\n",
        "axes[2].set_ylabel(\"Latency (Seconds)\")\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"metrics_comparison.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lDr6fZMQCd6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1031ed5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38604c67"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil # Import shutil for copying files\n",
        "\n",
        "# Define the directory to save files in Google Drive\n",
        "save_dir = '/content/drive/MyDrive/Columbia-LLMSeminar/SLLM project/Mena/exp0_1'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save results_table as CSV\n",
        "df_results = pd.DataFrame(results_table)\n",
        "df_results.to_csv(os.path.join(save_dir, 'exp0_1_results.csv'), index=False)\n",
        "print(f\"Results table saved to {os.path.join(save_dir, 'exp0_1_results.csv')}\") # Corrected print statement\n",
        "\n",
        "# Copy figures to Google Drive\n",
        "figures_to_copy = [\n",
        "    'efficiency_frontier_comparison.png',\n",
        "    'metrics_comparison.png'\n",
        "]\n",
        "\n",
        "for fig_name in figures_to_copy:\n",
        "    fig_path = os.path.join('/content', fig_name)\n",
        "    if os.path.exists(fig_path):\n",
        "        # Use shutil.copy2 to copy the file, then os.remove to delete the original\n",
        "        shutil.copy2(fig_path, os.path.join(save_dir, fig_name))\n",
        "        # os.remove(fig_path) # Remove original after copying\n",
        "        print(f\"Copied {fig_name} to {os.path.join(save_dir, fig_name)} and removed original.\")\n",
        "    else:\n",
        "        print(f\"Figure {fig_name} not found in current directory.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkZZEKhzTM0S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}